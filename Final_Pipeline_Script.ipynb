{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8124606-a4b4-4802-84e1-316f9c142c24",
   "metadata": {},
   "source": [
    "## Pipeline for Data Frame Creation and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a1a72-0448-43bd-99c5-d0461b33c375",
   "metadata": {},
   "source": [
    "### 1. Load Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9bdb223-4324-46d2-8980-165719ae3501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import io\n",
    "import zipfile\n",
    "import shutil\n",
    "import base64\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa5bc7-98d5-4c03-9670-3c8264dd0551",
   "metadata": {},
   "source": [
    "### 2. Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50d4cd-b312-4723-8e98-1678b447ebd5",
   "metadata": {},
   "source": [
    "#### ISO-NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126e01b-1d58-4bfa-8c2a-236404141be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading WW_DALMP_ISO_20220101.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220102.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220103.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220104.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220105.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220106.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220107.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220108.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220109.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220110.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220111.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220112.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220113.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220114.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220115.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220116.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220117.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220118.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220119.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220120.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220121.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220122.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220123.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220124.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220125.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220126.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220127.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220128.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220129.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220130.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220131.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220201.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220202.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220203.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220204.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220205.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220206.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220207.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220208.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220209.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220210.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220211.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220212.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220213.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220214.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220215.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220216.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220217.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220218.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220219.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220220.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220221.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220222.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220223.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220224.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220225.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220226.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220227.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220228.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220301.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220302.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220303.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220304.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220305.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220306.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220307.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220308.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220309.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220310.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220311.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220312.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220313.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220314.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220315.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220316.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220317.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220318.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220319.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220320.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220321.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220322.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220323.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220324.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220325.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220326.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220327.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220328.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220329.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220330.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220331.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220401.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220402.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220403.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220404.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220405.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220406.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220407.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220408.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220409.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220410.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220411.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220412.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220413.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220414.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220415.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220416.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220417.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220418.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220419.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220420.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220421.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220422.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220423.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220424.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220425.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220426.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220427.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220428.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220429.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220430.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220501.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220502.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220503.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220504.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220505.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220506.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220507.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220508.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220509.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220510.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220511.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220512.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220513.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220514.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220515.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220516.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220517.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220518.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220519.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220520.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220521.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220522.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220523.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220524.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220525.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220526.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220527.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220528.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220529.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220530.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220531.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220601.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220602.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220603.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220604.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220605.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220606.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220607.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220608.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220609.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220610.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220611.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220612.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220613.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220614.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220615.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220616.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220617.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220618.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220619.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220620.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220621.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220622.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220623.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220624.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220625.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220626.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220627.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220628.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220629.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220630.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220701.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220702.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220703.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220704.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220705.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220706.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220707.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220708.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220709.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220710.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220711.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220712.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220713.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220714.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220715.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220716.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220717.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220718.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220719.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220720.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220721.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220722.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220723.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220724.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220725.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220726.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220727.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220728.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220729.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220730.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220731.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220801.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220802.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220803.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220804.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220805.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220806.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220807.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220808.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220809.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220810.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220811.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220812.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220813.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220814.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220815.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220816.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220817.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220818.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220819.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220820.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220821.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220822.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220823.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220824.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220825.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220826.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220827.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220828.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220829.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220830.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220831.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220901.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220902.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220903.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220904.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220905.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220906.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220907.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220908.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220909.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220910.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220911.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220912.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220913.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220914.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220915.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220916.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220917.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220918.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220919.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220920.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220921.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220922.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220923.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220924.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220925.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220926.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220927.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220928.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220929.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20220930.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221001.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221002.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221003.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221004.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221005.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221006.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221007.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221008.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221009.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221010.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221011.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221012.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221013.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221014.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221015.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221016.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221017.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221018.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221019.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221020.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221021.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221022.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221023.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221024.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221025.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221026.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221027.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221028.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221029.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221030.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221031.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221101.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221102.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221103.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221104.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221105.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221106.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221107.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221108.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221109.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221110.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221111.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221112.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221113.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221114.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221115.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221116.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221117.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221118.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221119.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221120.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221121.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221122.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221123.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221124.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221125.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221126.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221127.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221128.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221129.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221130.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221201.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221202.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221203.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221204.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221205.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221206.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221207.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221208.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221209.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221210.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221211.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221212.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221213.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221214.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221215.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221216.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221217.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221218.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221219.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221220.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221221.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221222.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221223.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221224.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221225.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221226.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221227.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221228.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221229.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221230.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20221231.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230101.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230102.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230103.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230104.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230105.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230106.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230107.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230108.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230109.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230110.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230111.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230112.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230113.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230114.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230115.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230116.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230117.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230118.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230119.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230120.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230121.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230122.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230123.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230124.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230125.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230126.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230127.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230128.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230129.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230130.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230131.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230201.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230202.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230203.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230204.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230205.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230206.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230207.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230208.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230209.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230210.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230211.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230212.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230213.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230214.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230215.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230216.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230217.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230218.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230219.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230220.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230221.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230222.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230223.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230224.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230225.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230226.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230227.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230228.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230301.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230302.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230303.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230304.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230305.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230306.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230307.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230308.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230309.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230310.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230311.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230312.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230313.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230314.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230315.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230316.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230317.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230318.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230319.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230320.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230321.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230322.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230323.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230324.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230325.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230326.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230327.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230328.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230329.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230330.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230331.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230401.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230402.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230403.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230404.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230405.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230406.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230407.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230408.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230409.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230410.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230411.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230412.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230413.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230414.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230415.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230416.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230417.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230418.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230419.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230420.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230421.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230422.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230423.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230424.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230425.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230426.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230427.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230428.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230429.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230430.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230501.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230502.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230503.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230504.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230505.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230506.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230507.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230508.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230509.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230510.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230511.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230512.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230513.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230514.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230515.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230516.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230517.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230518.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230519.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230520.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230521.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230522.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230523.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230524.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230525.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230526.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230527.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230528.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230529.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230530.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230531.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230601.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230602.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230603.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230604.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230605.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230606.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230607.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230608.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230609.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230610.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230611.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230612.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230613.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230614.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230615.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230616.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230617.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230618.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230619.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230620.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230621.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230622.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230623.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230624.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230625.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230626.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230627.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230628.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230629.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230630.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230701.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230702.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230703.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230704.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230705.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230706.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230707.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230708.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230709.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230710.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230711.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230712.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230713.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230714.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230715.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230716.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230717.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230718.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230719.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230720.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230721.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230722.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230723.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230724.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230725.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230726.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230727.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230728.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230729.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230730.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230731.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230801.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230802.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230803.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230804.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230805.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230806.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230807.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230808.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230809.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230810.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230811.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230812.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230813.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230814.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230815.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230816.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230817.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230818.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230819.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230820.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230821.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230822.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230823.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230824.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230825.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230826.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230827.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230828.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230829.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230830.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230831.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230901.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230902.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230903.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230904.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230905.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230906.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230907.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230908.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230909.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230910.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230911.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230912.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230913.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230914.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230915.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230916.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230917.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230918.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230919.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230920.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230921.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230922.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230923.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230924.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230925.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230926.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230927.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230928.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230929.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20230930.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231001.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231002.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231003.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231004.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231005.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231006.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231007.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231008.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231009.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231010.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231011.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231012.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231013.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231014.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231015.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231016.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231017.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231018.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231019.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231020.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231021.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231022.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231023.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231024.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231025.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231026.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231027.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231028.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231029.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231030.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231031.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231101.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231102.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231103.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231104.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231105.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231106.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231107.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231108.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231109.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231110.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231111.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231112.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231113.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231114.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231115.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231116.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231117.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231118.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231119.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231120.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231121.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231122.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231123.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231124.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231125.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231126.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231127.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231128.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231129.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231130.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231201.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231202.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231203.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231204.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231205.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231206.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231207.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231208.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231209.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231210.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231211.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231212.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231213.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231214.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231215.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231216.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231217.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231218.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231219.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231220.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231221.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231222.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231223.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231224.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231225.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231226.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231227.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231228.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231229.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231230.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20231231.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240101.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240102.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240103.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240104.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240105.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240106.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240107.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240108.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240109.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240110.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240111.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240112.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240113.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240114.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240115.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240116.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240117.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240118.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240119.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240120.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240121.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240122.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240123.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240124.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240125.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240126.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240127.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240128.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240129.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240130.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240131.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240201.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240202.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240203.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240204.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240205.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240206.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240207.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240208.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240209.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240210.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240211.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240212.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240213.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240214.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240215.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240216.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240217.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240218.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240219.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240220.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240221.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240222.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240223.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240224.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240225.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240226.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240227.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240228.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240229.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240301.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240302.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240303.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240304.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240305.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240306.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240307.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240308.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240309.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240310.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240311.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240312.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240313.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240314.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240315.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240316.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240317.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240318.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240319.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240320.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240321.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240322.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240323.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240324.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240325.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240326.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240327.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240328.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240329.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240330.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240331.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240401.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240402.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240403.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240404.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240405.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240406.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240407.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240408.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240409.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240410.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240411.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240412.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240413.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240414.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240415.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240416.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240417.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240418.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240419.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240420.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240421.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240422.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240423.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240424.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240425.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240426.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240427.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240428.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240429.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240430.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240501.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240502.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240503.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240504.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240505.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240506.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240507.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240508.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240509.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240510.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240511.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240512.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240513.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240514.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240515.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240516.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240517.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240518.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240519.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240520.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240521.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240522.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240523.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240524.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240525.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240526.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240527.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240528.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240529.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240530.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240531.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240601.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240602.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240603.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240604.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240605.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240606.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240607.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240608.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240609.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240610.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240611.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240612.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240613.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240614.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240615.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240616.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240617.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240618.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240619.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240620.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240621.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240622.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240623.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240624.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240625.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240626.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240627.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240628.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240629.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240630.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240701.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240702.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240703.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240704.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240705.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240706.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240707.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240708.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240709.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240710.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240711.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240712.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240713.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240714.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240715.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240716.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240717.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240718.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240719.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240720.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240721.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240722.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240723.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240724.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240725.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240726.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240727.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240728.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240729.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240730.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240731.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240801.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240802.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240803.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240804.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240805.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240806.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240807.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240808.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240809.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240810.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240811.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240812.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240813.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240814.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240815.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240816.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240817.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240818.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240819.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240820.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240821.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240822.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240823.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240824.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240825.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240826.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240827.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240828.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240829.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240830.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240831.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240901.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240902.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240903.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240904.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240905.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240906.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240907.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240908.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240909.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240910.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240911.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240912.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240913.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240914.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240915.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240916.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240917.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240918.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240919.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240920.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240921.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240922.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240923.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240924.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240925.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240926.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240927.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240928.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240929.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20240930.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241001.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241002.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241003.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241004.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241005.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241006.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241007.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241008.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241009.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241010.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241011.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241012.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241013.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241014.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241015.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241016.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241017.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241018.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241019.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241020.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241021.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241022.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241023.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241024.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241025.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241026.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241027.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241028.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241029.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241030.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241031.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241101.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241102.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241103.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241104.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241105.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241106.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241107.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241108.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241109.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241110.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241111.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241112.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241113.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241114.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241115.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241116.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241117.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241118.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241119.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241120.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241121.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241122.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241123.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241124.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241125.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241126.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241127.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241128.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241129.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241130.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241201.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241202.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241203.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241204.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241205.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241206.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241207.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241208.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241209.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241210.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241211.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241212.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241213.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241214.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241215.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241216.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241217.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241218.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241219.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241220.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241221.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241222.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241223.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241224.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241225.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241226.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241227.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241228.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241229.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241230.csv...\n",
      "📥 Downloading WW_DALMP_ISO_20241231.csv...\n",
      "✅ Saved isone_lmp_2022Q1.csv with 2604808 records.\n",
      "✅ Saved isone_lmp_2022Q2.csv with 2637744 records.\n",
      "✅ Saved isone_lmp_2022Q3.csv with 2665488 records.\n",
      "✅ Saved isone_lmp_2022Q4.csv with 2686144 records.\n",
      "✅ Saved isone_lmp_2023Q1.csv with 2626231 records.\n",
      "✅ Saved isone_lmp_2023Q2.csv with 2654616 records.\n",
      "✅ Saved isone_lmp_2023Q3.csv with 2674248 records.\n",
      "✅ Saved isone_lmp_2023Q4.csv with 2686144 records.\n",
      "✅ Saved isone_lmp_2024Q1.csv with 2654097 records.\n",
      "✅ Saved isone_lmp_2024Q2.csv with 2652984 records.\n",
      "✅ Saved isone_lmp_2024Q3.csv with 2669472 records.\n",
      "✅ Saved isone_lmp_2024Q4.csv with 2679325 records.\n"
     ]
    }
   ],
   "source": [
    "def download_isone_consolidated_lmp_by_quarter(start_date: str, end_date: str, save_path: str = 'iso_data/isone_lmp_data') -> None:\n",
    "    \"\"\"\n",
    "    Downloads ISO-NE Day-Ahead LMP CSVs for each day in the date range,\n",
    "    cleans and combines the data, and saves one CSV per quarter.\n",
    "\n",
    "    No individual daily files are written to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    all_data = []\n",
    "\n",
    "    # Parse date range\n",
    "    start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    current_dt = start_dt\n",
    "    while current_dt <= end_dt:\n",
    "        date_str = current_dt.strftime('%Y%m%d')\n",
    "        filename = f\"WW_DALMP_ISO_{date_str}.csv\"\n",
    "        url = f\"https://www.iso-ne.com/static-transform/csv/histRpts/da-lmp/{filename}\"\n",
    "\n",
    "        print(f\"📥 Downloading {filename}...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Read directly from the response content\n",
    "            df = pd.read_csv(io.StringIO(response.text), skiprows=4, header=0)\n",
    "            df = df.iloc[1:].reset_index(drop=True)  # Remove type row\n",
    "\n",
    "            df['Fetched_Date'] = current_dt.strftime('%Y-%m-%d')\n",
    "            all_data.append(df)\n",
    "\n",
    "        except requests.exceptions.HTTPError:\n",
    "            print(f\"❌ File not found for {date_str} — skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {date_str}: {e}\")\n",
    "\n",
    "        current_dt += timedelta(days=1)\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"🚫 No data was downloaded.\")\n",
    "        return\n",
    "\n",
    "    # Combine all data\n",
    "    full_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Parse the actual 'Date' column\n",
    "    full_df['Date'] = pd.to_datetime(full_df['Date'], errors='coerce')\n",
    "    full_df = full_df.dropna(subset=['Date'])\n",
    "\n",
    "    # Assign each row to a calendar quarter\n",
    "    full_df['Quarter'] = full_df['Date'].dt.to_period('Q')\n",
    "\n",
    "    # Save one CSV per quarter\n",
    "    for quarter, group in full_df.groupby('Quarter'):\n",
    "        quarter_str = str(quarter).replace('/', '')  # e.g., '2024Q1'\n",
    "        output_filename = f\"isone_lmp_{quarter_str}.csv\"\n",
    "        output_path = os.path.join(save_path, output_filename)\n",
    "\n",
    "        group.drop(columns='Quarter').to_csv(output_path, index=False)\n",
    "        print(f\"✅ Saved {output_filename} with {len(group)} records.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_isone_consolidated_lmp_by_quarter(\"2022-01-01\", \"2024-12-31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6eba8-e46e-49be-a3ab-ec34f610f792",
   "metadata": {},
   "source": [
    "#### NYISO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c15edae1-ad85-4d77-bb17-6aaa4c39e7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⬇️ Downloading 20220101damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20220101damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20220201damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20220201damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20220301damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20220301damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20220401damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20220401damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20220501damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20220501damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20220601damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20220601damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20220701damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20220701damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20220801damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20220801damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20220901damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20220901damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20221001damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20221001damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20221101damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20221101damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20221201damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20221201damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20230101damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20230101damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20230201damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20230201damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20230301damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20230301damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20230401damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20230401damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20230501damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20230501damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20230601damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20230601damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20230701damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20230701damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20230801damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20230801damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20230901damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20230901damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20231001damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20231001damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20231101damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20231101damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20231201damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20231201damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20240101damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20240101damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20240201damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20240201damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20240301damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20240301damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20240401damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20240401damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20240501damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20240501damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20240601damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20240601damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20240701damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20240701damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20240801damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20240801damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20240901damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20240901damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20241001damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20241001damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20241101damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20241101damlbmp_zone_csv.zip\n",
      "⬇️ Downloading 20241201damlbmp_zone_csv.zip...\n",
      "✅ Extracted 20241201damlbmp_zone_csv.zip\n",
      "📂 Found 1096 daily CSVs to process.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_zonal_lmp_data/nyiso_zonal_lmp_2022Q1.csv with 32385 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_zonal_lmp_data/nyiso_zonal_lmp_2022Q2.csv with 32760 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_zonal_lmp_data/nyiso_zonal_lmp_2022Q3.csv with 33120 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_zonal_lmp_data/nyiso_zonal_lmp_2022Q4.csv with 33135 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_zonal_lmp_data/nyiso_zonal_lmp_2023Q1.csv with 32385 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_zonal_lmp_data/nyiso_zonal_lmp_2023Q2.csv with 32760 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_zonal_lmp_data/nyiso_zonal_lmp_2023Q3.csv with 33120 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_zonal_lmp_data/nyiso_zonal_lmp_2023Q4.csv with 33135 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_zonal_lmp_data/nyiso_zonal_lmp_2024Q1.csv with 32745 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_zonal_lmp_data/nyiso_zonal_lmp_2024Q2.csv with 32760 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_zonal_lmp_data/nyiso_zonal_lmp_2024Q3.csv with 33120 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_zonal_lmp_data/nyiso_zonal_lmp_2024Q4.csv with 33135 rows.\n",
      "🧹 Removing extracted folder: iso_data/nyiso_data/nyiso_zonal_lmp_data/extracted\n",
      "✅ Cleanup complete.\n",
      "⬇️ Downloading 20220101damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20220101damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20220201damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20220201damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20220301damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20220301damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20220401damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20220401damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20220501damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20220501damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20220601damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20220601damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20220701damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20220701damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20220801damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20220801damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20220901damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20220901damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20221001damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20221001damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20221101damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20221101damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20221201damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20221201damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20230101damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20230101damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20230201damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20230201damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20230301damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20230301damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20230401damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20230401damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20230501damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20230501damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20230601damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20230601damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20230701damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20230701damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20230801damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20230801damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20230901damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20230901damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20231001damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20231001damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20231101damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20231101damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20231201damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20231201damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20240101damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20240101damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20240201damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20240201damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20240301damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20240301damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20240401damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20240401damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20240501damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20240501damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20240601damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20240601damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20240701damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20240701damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20240801damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20240801damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20240901damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20240901damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20241001damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20241001damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20241101damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20241101damlbmp_gen_csv.zip\n",
      "⬇️ Downloading 20241201damlbmp_gen_csv.zip...\n",
      "✅ Extracted 20241201damlbmp_gen_csv.zip\n",
      "📂 Found 1096 daily CSVs to process.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_gen_lmp_data/nyiso_lmp_2022Q1.csv with 1242120 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_gen_lmp_data/nyiso_lmp_2022Q2.csv with 1258800 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_gen_lmp_data/nyiso_lmp_2022Q3.csv with 1274088 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_gen_lmp_data/nyiso_lmp_2022Q4.csv with 1291856 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_gen_lmp_data/nyiso_lmp_2023Q1.csv with 1274698 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_gen_lmp_data/nyiso_lmp_2023Q2.csv with 1522608 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_gen_lmp_data/nyiso_lmp_2023Q3.csv with 1606632 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_gen_lmp_data/nyiso_lmp_2023Q4.csv with 1617924 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_gen_lmp_data/nyiso_lmp_2024Q1.csv with 1608007 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_gen_lmp_data/nyiso_lmp_2024Q2.csv with 1614864 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_gen_lmp_data/nyiso_lmp_2024Q3.csv with 1641864 rows.\n",
      "✅ Saved iso_data/nyiso_data/nyiso_gen_lmp_data/nyiso_lmp_2024Q4.csv with 1643496 rows.\n",
      "🧹 Removing extracted folder: iso_data/nyiso_data/nyiso_gen_lmp_data/extracted\n",
      "✅ Cleanup complete.\n",
      "✅ Combined and saved iso_data/nyiso_data/nyiso_combined_quarters/nyiso_combined_2022Q1.csv\n",
      "✅ Combined and saved iso_data/nyiso_data/nyiso_combined_quarters/nyiso_combined_2022Q2.csv\n",
      "✅ Combined and saved iso_data/nyiso_data/nyiso_combined_quarters/nyiso_combined_2022Q3.csv\n",
      "✅ Combined and saved iso_data/nyiso_data/nyiso_combined_quarters/nyiso_combined_2022Q4.csv\n",
      "✅ Combined and saved iso_data/nyiso_data/nyiso_combined_quarters/nyiso_combined_2023Q1.csv\n",
      "✅ Combined and saved iso_data/nyiso_data/nyiso_combined_quarters/nyiso_combined_2023Q2.csv\n",
      "✅ Combined and saved iso_data/nyiso_data/nyiso_combined_quarters/nyiso_combined_2023Q3.csv\n",
      "✅ Combined and saved iso_data/nyiso_data/nyiso_combined_quarters/nyiso_combined_2023Q4.csv\n",
      "✅ Combined and saved iso_data/nyiso_data/nyiso_combined_quarters/nyiso_combined_2024Q1.csv\n",
      "✅ Combined and saved iso_data/nyiso_data/nyiso_combined_quarters/nyiso_combined_2024Q2.csv\n",
      "✅ Combined and saved iso_data/nyiso_data/nyiso_combined_quarters/nyiso_combined_2024Q3.csv\n",
      "✅ Combined and saved iso_data/nyiso_data/nyiso_combined_quarters/nyiso_combined_2024Q4.csv\n"
     ]
    }
   ],
   "source": [
    "# NYISO Zonal Data\n",
    "def download_nyiso_zone_lmp_monthly_by_quarter(start_year: int, end_year: int, save_path='iso_data/nyiso_data/nyiso_zonal_lmp_data'):\n",
    "    \"\"\"\n",
    "    Downloads NYISO monthly ZONAL LBMP ZIPs, extracts daily CSVs, aggregates by quarter,\n",
    "    saves quarterly CSVs, and deletes the extracted folder afterward.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    temp_extract_path = os.path.join(save_path, 'extracted')\n",
    "    os.makedirs(temp_extract_path, exist_ok=True)\n",
    "    all_data = []\n",
    "\n",
    "    # Generate list of months\n",
    "    months = pd.date_range(start=f'{start_year}-01-01', end=f'{end_year}-12-31', freq='MS')\n",
    "\n",
    "    for dt in months:\n",
    "        yyyymmdd = dt.strftime('%Y%m01')\n",
    "        zip_filename = f\"{yyyymmdd}damlbmp_zone_csv.zip\"  # ⬅️ _zone_ here!\n",
    "        url = f\"https://mis.nyiso.com/public/csv/damlbmp/{zip_filename}\"\n",
    "\n",
    "        print(f\"⬇️ Downloading {zip_filename}...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"❌ File not found for {dt.strftime('%B %Y')} — skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Extract ZIP in memory\n",
    "            with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "                z.extractall(temp_extract_path)\n",
    "            print(f\"✅ Extracted {zip_filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {zip_filename}: {e}\")\n",
    "\n",
    "    # Collect all extracted CSVs\n",
    "    all_files = [os.path.join(temp_extract_path, f) for f in os.listdir(temp_extract_path) if f.endswith('.csv')]\n",
    "    print(f\"📂 Found {len(all_files)} daily CSVs to process.\")\n",
    "\n",
    "    for file in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {file}: {e}\")\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No data was processed.\")\n",
    "        return\n",
    "\n",
    "    # Combine\n",
    "    full_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Detect timestamp column\n",
    "    timestamp_col = next((col for col in full_df.columns if 'Time' in col or 'time' in col.lower()), None)\n",
    "    if not timestamp_col:\n",
    "        raise ValueError(\"Couldn't find a timestamp column in the combined CSVs.\")\n",
    "\n",
    "    full_df['Date'] = pd.to_datetime(full_df[timestamp_col], errors='coerce')\n",
    "    full_df = full_df.dropna(subset=['Date'])\n",
    "\n",
    "    # Assign calendar quarter\n",
    "    full_df['Quarter'] = full_df['Date'].dt.to_period('Q')\n",
    "\n",
    "    # Save one CSV per quarter\n",
    "    for quarter, group in full_df.groupby('Quarter'):\n",
    "        quarter_str = str(quarter).replace('/', '')\n",
    "        output_path = os.path.join(save_path, f'nyiso_zonal_lmp_{quarter_str}.csv')\n",
    "        group.drop(columns='Quarter').to_csv(output_path, index=False)\n",
    "        print(f\"✅ Saved {output_path} with {len(group)} rows.\")\n",
    "\n",
    "    # ✅ Clean up extracted folder\n",
    "    print(f\"🧹 Removing extracted folder: {temp_extract_path}\")\n",
    "    shutil.rmtree(temp_extract_path)\n",
    "    print(\"✅ Cleanup complete.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    download_nyiso_zone_lmp_monthly_by_quarter(start_year=2022, end_year=2024)\n",
    "\n",
    "# NYISO Generator Data\n",
    "def download_nyiso_gen_lmp_monthly_by_quarter(start_year: int, end_year: int, save_path='iso_data/nyiso_data/nyiso_gen_lmp_data'):\n",
    "    \"\"\"\n",
    "    Downloads NYISO monthly generator LBMP ZIPs, extracts daily CSVs, aggregates by quarter,\n",
    "    saves quarterly CSVs, and deletes the extracted folder afterward.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    temp_extract_path = os.path.join(save_path, 'extracted')\n",
    "    os.makedirs(temp_extract_path, exist_ok=True)\n",
    "    all_data = []\n",
    "\n",
    "    # Generate list of months\n",
    "    months = pd.date_range(start=f'{start_year}-01-01', end=f'{end_year}-12-31', freq='MS')\n",
    "\n",
    "    for dt in months:\n",
    "        yyyymmdd = dt.strftime('%Y%m01')\n",
    "        zip_filename = f\"{yyyymmdd}damlbmp_gen_csv.zip\"\n",
    "        url = f\"https://mis.nyiso.com/public/csv/damlbmp/{zip_filename}\"\n",
    "\n",
    "        print(f\"⬇️ Downloading {zip_filename}...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"❌ File not found for {dt.strftime('%B %Y')} — skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Extract ZIP in memory\n",
    "            with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "                z.extractall(temp_extract_path)\n",
    "            print(f\"✅ Extracted {zip_filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing {zip_filename}: {e}\")\n",
    "\n",
    "    # Collect all extracted CSVs\n",
    "    all_files = [os.path.join(temp_extract_path, f) for f in os.listdir(temp_extract_path) if f.endswith('.csv')]\n",
    "    print(f\"📂 Found {len(all_files)} daily CSVs to process.\")\n",
    "\n",
    "    for file in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Skipping {file}: {e}\")\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No data was processed.\")\n",
    "        return\n",
    "\n",
    "    # Combine\n",
    "    full_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Detect timestamp column\n",
    "    timestamp_col = next((col for col in full_df.columns if 'Time' in col or 'time' in col.lower()), None)\n",
    "    if not timestamp_col:\n",
    "        raise ValueError(\"Couldn't find a timestamp column in the combined CSVs.\")\n",
    "\n",
    "    full_df['Date'] = pd.to_datetime(full_df[timestamp_col], errors='coerce')\n",
    "    full_df = full_df.dropna(subset=['Date'])\n",
    "\n",
    "    # Assign calendar quarter\n",
    "    full_df['Quarter'] = full_df['Date'].dt.to_period('Q')\n",
    "\n",
    "    # Save one CSV per quarter\n",
    "    for quarter, group in full_df.groupby('Quarter'):\n",
    "        quarter_str = str(quarter).replace('/', '')\n",
    "        output_path = os.path.join(save_path, f'nyiso_lmp_{quarter_str}.csv')\n",
    "        group.drop(columns='Quarter').to_csv(output_path, index=False)\n",
    "        print(f\"✅ Saved {output_path} with {len(group)} rows.\")\n",
    "\n",
    "    # ✅ Clean up extracted folder\n",
    "    print(f\"🧹 Removing extracted folder: {temp_extract_path}\")\n",
    "    shutil.rmtree(temp_extract_path)\n",
    "    print(\"✅ Cleanup complete.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    download_nyiso_gen_lmp_monthly_by_quarter(start_year=2022, end_year=2024)\n",
    "\n",
    "# combine zonal and generator data\n",
    "\n",
    "# Paths to your folders\n",
    "nodal_dir = 'iso_data/nyiso_data/nyiso_gen_lmp_data'\n",
    "zonal_dir = 'iso_data/nyiso_data/nyiso_zonal_lmp_data'\n",
    "output_dir = 'iso_data/nyiso_data/nyiso_combined_quarters'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all quarter identifiers from filenames (e.g., '2022Q1')\n",
    "nodal_quarters = [f.replace('nyiso_lmp_', '').replace('.csv', '') for f in os.listdir(nodal_dir) if f.endswith('.csv')]\n",
    "zonal_quarters = [f.replace('nyiso_zonal_lmp_', '').replace('.csv', '') for f in os.listdir(zonal_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Get common quarters that exist in both folders\n",
    "all_quarters = sorted(set(nodal_quarters) & set(zonal_quarters))\n",
    "\n",
    "# Process and merge each quarter\n",
    "for quarter in all_quarters:\n",
    "    nodal_file = os.path.join(nodal_dir, f'nyiso_lmp_{quarter}.csv')\n",
    "    zonal_file = os.path.join(zonal_dir, f'nyiso_zonal_lmp_{quarter}.csv')\n",
    "    \n",
    "    # Read files and tag node_type\n",
    "    nodal_df = pd.read_csv(nodal_file)\n",
    "    nodal_df['node_type'] = 'nodal'\n",
    "    \n",
    "    zonal_df = pd.read_csv(zonal_file)\n",
    "    zonal_df['node_type'] = 'zonal'\n",
    "    \n",
    "    # Combine and save\n",
    "    combined_df = pd.concat([nodal_df, zonal_df], ignore_index=True)\n",
    "    output_path = os.path.join(output_dir, f'nyiso_combined_{quarter}.csv')\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"✅ Combined and saved {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52640f50-77a2-48a2-9ba3-1c815d482d69",
   "metadata": {},
   "source": [
    "#### PJM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a258c-90cd-4256-9522-19e8eac165c4",
   "metadata": {},
   "source": [
    "#### MISO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c9491b1-6919-4359-adec-cd8f6cd9529b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📦 Processing 2025 Q1\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Jan-Mar_DA_LMPs.zip\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Jan-Mar_DA_LMP.zip\n",
      "    Extracting: DA.csv\n",
      "✅ Saved to iso_data/miso_data/2025_Q1.csv\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Jan_Mar_DA_LMPs.zip\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Jan_Mar_DA_LMP.zip\n",
      "\n",
      "📦 Processing 2025 Q2\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Apr-Jun_DA_LMPs.zip\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Apr-Jun_DA_LMP.zip\n",
      "⚠️ No valid file found for 2025 Q2\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025-Apr-Jun_DA_LMPs.zip\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025-Apr-Jun_DA_LMP.zip\n",
      "⚠️ No valid file found for 2025 Q2\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Apr_Jun_DA_LMPs.zip\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Apr_Jun_DA_LMP.zip\n",
      "⚠️ No valid file found for 2025 Q2\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025-Apr_Jun_DA_LMPs.zip\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025-Apr_Jun_DA_LMP.zip\n",
      "⚠️ No valid file found for 2025 Q2\n",
      "\n",
      "📦 Processing 2025 Q3\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Jul-Sep_DA_LMPs.zip\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Jul-Sep_DA_LMP.zip\n",
      "⚠️ No valid file found for 2025 Q3\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025-Jul-Sep_DA_LMPs.zip\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025-Jul-Sep_DA_LMP.zip\n",
      "⚠️ No valid file found for 2025 Q3\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Jul_Sep_DA_LMPs.zip\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Jul_Sep_DA_LMP.zip\n",
      "⚠️ No valid file found for 2025 Q3\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025-Jul_Sep_DA_LMPs.zip\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025-Jul_Sep_DA_LMP.zip\n",
      "⚠️ No valid file found for 2025 Q3\n",
      "\n",
      "📦 Processing 2025 Q4\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Oct-Dec_DA_LMPs.zip\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Oct-Dec_DA_LMP.zip\n",
      "⚠️ No valid file found for 2025 Q4\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025-Oct-Dec_DA_LMPs.zip\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025-Oct-Dec_DA_LMP.zip\n",
      "⚠️ No valid file found for 2025 Q4\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Oct_Dec_DA_LMPs.zip\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025_Oct_Dec_DA_LMP.zip\n",
      "⚠️ No valid file found for 2025 Q4\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025-Oct_Dec_DA_LMPs.zip\n",
      "  Trying: https://docs.misoenergy.org/marketreports/2025-Oct_Dec_DA_LMP.zip\n",
      "⚠️ No valid file found for 2025 Q4\n",
      "\n",
      "📦 Processing annual archive for 2022: https://docs.misoenergy.org/marketreports/202201_DA_LMPs_zip.zip\n",
      "  📦 Found nested ZIP: 2022_Apr-Jun_DA_LMPs.zip\n",
      "    📄 Extracting CSV: 2022_Apr-Jun_DA_LMPs.csv\n",
      "✅ Saved quarterly CSV: iso_data/miso_data/2022_Q2.csv\n",
      "  📦 Found nested ZIP: 2022_Jan-Mar_DA_LMPs.zip\n",
      "    📄 Extracting CSV: 2022_Jan-Mar_DA_LMP.csv\n",
      "✅ Saved quarterly CSV: iso_data/miso_data/2022_Q1.csv\n",
      "  📦 Found nested ZIP: 2022_Jul-Sep_DA_LMPs.zip\n",
      "    📄 Extracting CSV: 2022_Jul-Sep_DA_LMPs.csv\n",
      "✅ Saved quarterly CSV: iso_data/miso_data/2022_Q3.csv\n",
      "  📦 Found nested ZIP: 2022_Oct-Dec_DA_LMPs.zip\n",
      "    📄 Extracting CSV: 2022_Oct-Dec_DA_LMPs.csv\n",
      "✅ Saved quarterly CSV: iso_data/miso_data/2022_Q4.csv\n"
     ]
    }
   ],
   "source": [
    "# for 2023 to present data extraction\n",
    "def download_and_extract_zip(url):\n",
    "    \"\"\"\n",
    "    Downloads and extracts all CSVs from a MISO ZIP archive.\n",
    "    Returns a list of DataFrames.\n",
    "    \"\"\"\n",
    "    print(f\"  Trying: {url}\")\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return []\n",
    "\n",
    "    if response.content[:2] != b'PK':  # ZIP files start with 'PK'\n",
    "        return []\n",
    "\n",
    "    dfs = []\n",
    "    try:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            for filename in z.namelist():\n",
    "                if filename.endswith(\".csv\"):\n",
    "                    print(f\"    Extracting: {filename}\")\n",
    "                    with z.open(filename) as f:\n",
    "                        try:\n",
    "                            df = pd.read_csv(f, low_memory=False)\n",
    "                            df[\"SOURCE_FILE\"] = filename\n",
    "                            dfs.append(df)\n",
    "                        except Exception as e:\n",
    "                            print(f\"    Failed to parse {filename}: {e}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"    Error: File at {url} is not a valid ZIP archive.\")\n",
    "    return dfs\n",
    "\n",
    "def scrape_miso_quarterly_zips(year, quarters, output_dir=\"iso_data/miso_data\"):\n",
    "    \"\"\"\n",
    "    Downloads and processes MISO LMP ZIP files for given year and quarters,\n",
    "    handling naming inconsistencies and saving each quarter separately.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    quarter_map_variants = {\n",
    "        \"Q1\": [\"Jan-Mar\", \"Jan_Mar\"],\n",
    "        \"Q2\": [\"Apr-Jun\", \"Apr_Jun\"],\n",
    "        \"Q3\": [\"Jul-Sep\", \"Jul_Sep\"],\n",
    "        \"Q4\": [\"Oct-Dec\", \"Oct_Dec\"]\n",
    "    }\n",
    "    suffixes = [\"DA_LMPs.zip\", \"DA_LMP.zip\"]\n",
    "    year_sep_variants = [\"_\", \"-\"]\n",
    "\n",
    "    for q in quarters:\n",
    "        print(f\"\\n📦 Processing {year} {q}\")\n",
    "        success = False\n",
    "        for quarter_str in quarter_map_variants[q]:\n",
    "             for sep in year_sep_variants:\n",
    "                for suffix in suffixes:\n",
    "                    filename = f\"{year}{sep}{quarter_str}_{suffix}\"\n",
    "                    url = f\"https://docs.misoenergy.org/marketreports/{filename}\"\n",
    "\n",
    "                    dfs = download_and_extract_zip(url)\n",
    "                    if dfs:\n",
    "                        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "                        out_path = os.path.join(output_dir, f\"{year}_{q}.csv\")\n",
    "                        combined_df.to_csv(out_path, index=False)\n",
    "                        print(f\"✅ Saved to {out_path}\")\n",
    "                        success = True\n",
    "                        break  # stop after the first successful variant\n",
    "                if success:\n",
    "                    break\n",
    "                if not success:\n",
    "                    print(f\"⚠️ No valid file found for {year} {q}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_miso_quarterly_zips(year=2025, quarters=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "\n",
    "# 2022 and earlier\n",
    "def extract_and_save_quarter_from_nested_zip(nested_zip_bytes, nested_filename, year, output_dir):\n",
    "    \"\"\"\n",
    "    Extracts CSVs from a nested ZIP archive and saves them as a quarterly file\n",
    "    based on the name of the nested ZIP.\n",
    "    \"\"\"\n",
    "    # Identify quarter from filename\n",
    "    quarter_hint = \"\"\n",
    "    if any(m in nested_filename for m in [\"Jan\", \"Feb\", \"Mar\"]):\n",
    "        quarter_hint = \"Q1\"\n",
    "    elif any(m in nested_filename for m in [\"Apr\", \"May\", \"Jun\"]):\n",
    "        quarter_hint = \"Q2\"\n",
    "    elif any(m in nested_filename for m in [\"Jul\", \"Aug\", \"Sep\"]):\n",
    "        quarter_hint = \"Q3\"\n",
    "    elif any(m in nested_filename for m in [\"Oct\", \"Nov\", \"Dec\"]):\n",
    "        quarter_hint = \"Q4\"\n",
    "    else:\n",
    "        print(f\"⚠️ Could not identify quarter from: {nested_filename}\")\n",
    "        return\n",
    "\n",
    "    # Extract CSVs from nested ZIP\n",
    "    dfs = []\n",
    "    try:\n",
    "        with zipfile.ZipFile(io.BytesIO(nested_zip_bytes)) as nested_zip:\n",
    "            for filename in nested_zip.namelist():\n",
    "                if filename.endswith(\".csv\"):\n",
    "                    print(f\"    📄 Extracting CSV: {filename}\")\n",
    "                    with nested_zip.open(filename) as f:\n",
    "                        try:\n",
    "                            df = pd.read_csv(f, low_memory=False)\n",
    "                            df[\"SOURCE_FILE\"] = filename\n",
    "                            dfs.append(df)\n",
    "                        except Exception as e:\n",
    "                            print(f\"      ⚠️ Failed to read {filename}: {e}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"⚠️ Invalid nested ZIP: {nested_filename}\")\n",
    "        return\n",
    "\n",
    "    # Save only that quarter’s data\n",
    "    if dfs:\n",
    "        combined = pd.concat(dfs, ignore_index=True)\n",
    "        out_path = os.path.join(output_dir, f\"{year}_{quarter_hint}.csv\")\n",
    "        combined.to_csv(out_path, index=False)\n",
    "        print(f\"✅ Saved quarterly CSV: {out_path}\")\n",
    "    else:\n",
    "        print(f\"⚠️ No CSVs extracted from: {nested_filename}\")\n",
    "\n",
    "def download_and_process_annual_zip(year, output_dir=\"miso_data\"):\n",
    "    \"\"\"\n",
    "    Downloads a yearly MISO ZIP (which contains nested ZIPs by month/quarter),\n",
    "    extracts quarterly data, and saves each quarter separately.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filename = f\"{year}01_DA_LMPs_zip.zip\"\n",
    "    url = f\"https://docs.misoenergy.org/marketreports/{filename}\"\n",
    "    print(f\"\\n📦 Processing annual archive for {year}: {url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200 or response.content[:2] != b'PK':\n",
    "            print(\"❌ Failed to download or invalid ZIP format.\")\n",
    "            return\n",
    "\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as outer_zip:\n",
    "            for nested_name in outer_zip.namelist():\n",
    "                if nested_name.endswith(\".zip\"):\n",
    "                    print(f\"  📦 Found nested ZIP: {nested_name}\")\n",
    "                    with outer_zip.open(nested_name) as nested_file:\n",
    "                        nested_bytes = nested_file.read()\n",
    "                        extract_and_save_quarter_from_nested_zip(\n",
    "                            nested_zip_bytes=nested_bytes,\n",
    "                            nested_filename=nested_name,\n",
    "                            year=year,\n",
    "                            output_dir=output_dir\n",
    "                        )\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing archive: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_and_process_annual_zip(2022, output_dir=\"iso_data/miso_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d814419f-5a69-4993-a47a-e9e74695305a",
   "metadata": {},
   "source": [
    "#### ERCOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74058be9-e792-4fac-87b0-ed06bd0cc1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Token received.\n",
      "{\n",
      "  \"aud\": \"fec253ea-0d06-4272-a5e6-b478baeecd70\",\n",
      "  \"iss\": \"https://ercotb2c.b2clogin.com/6df17afa-1b36-499a-83f7-56779ad0b9a6/v2.0/\",\n",
      "  \"exp\": 1746314067,\n",
      "  \"nbf\": 1746310467,\n",
      "  \"idp\": \"LocalAccount\",\n",
      "  \"oid\": \"8de9998e-1bce-4aa0-91c4-9dd4d264f0cd\",\n",
      "  \"sub\": \"8de9998e-1bce-4aa0-91c4-9dd4d264f0cd\",\n",
      "  \"given_name\": \"Alan\",\n",
      "  \"family_name\": \"Wang\",\n",
      "  \"emails\": [\n",
      "    \"alanwang2025@u.northwestern.edu\"\n",
      "  ],\n",
      "  \"tfp\": \"B2C_1_PUBAPI-ROPC-FLOW\",\n",
      "  \"azp\": \"fec253ea-0d06-4272-a5e6-b478baeecd70\",\n",
      "  \"ver\": \"1.0\",\n",
      "  \"iat\": 1746310467\n",
      "}\n",
      "📦 Fetching ERCOT DAM LMP archive list...\n",
      "✅ Found 1000 archive entries.\n",
      "✅ Found 53 archive entries from 2022 onward.\n",
      "\n",
      "⬇️ Downloading (1): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (2): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (3): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (4): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (5): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (6): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (7): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (8): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (9): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (10): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (11): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (12): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (13): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (14): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (15): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (16): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (17): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (18): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (19): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (20): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (21): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (22): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (23): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (24): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (25): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (26): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (27): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (28): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (29): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (30): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (31): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (32): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (33): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (34): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (35): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (36): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (37): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (38): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (39): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (40): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (41): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (42): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (43): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (44): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (45): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (46): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (47): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (48): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (49): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (50): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (51): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (52): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n",
      "\n",
      "⬇️ Downloading (53): DAMHRLMPNP4183_csv\n",
      "📦 Detected ZIP archive\n",
      "📑 Columns: ['deliverydate', 'hourending', 'busname', 'lmp', 'dstflag']\n",
      "✅ Saved: DAMHRLMPNP4183_csv\n"
     ]
    }
   ],
   "source": [
    "token_url = \"https://ercotb2c.b2clogin.com/ercotb2c.onmicrosoft.com/B2C_1_PUBAPI-ROPC-FLOW/oauth2/v2.0/token\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "}\n",
    "\n",
    "token_data = {\n",
    "    \"grant_type\": \"password\",\n",
    "    \"scope\": \"openid fec253ea-0d06-4272-a5e6-b478baeecd70 offline_access\",\n",
    "    \"client_id\": \"fec253ea-0d06-4272-a5e6-b478baeecd70\",\n",
    "    \"username\": \"alanwang2025@u.northwestern.edu\",\n",
    "    \"password\": \"Fork102$\"\n",
    "}\n",
    "\n",
    "response = requests.post(token_url, data=token_data, headers=headers)\n",
    "response.raise_for_status()\n",
    "access_token = response.json()[\"access_token\"]\n",
    "print(\"✅ Token received.\")\n",
    "\n",
    "token_parts = access_token.split(\".\")\n",
    "payload = token_parts[1] + '=' * (-len(token_parts[1]) % 4)  # fix padding\n",
    "decoded = json.loads(base64.urlsafe_b64decode(payload.encode()).decode())\n",
    "print(json.dumps(decoded, indent=2))\n",
    "\n",
    "access_token\n",
    "\n",
    "# Replace with your real token and subscription key\n",
    "subscription_key = \"7076e411aeeb461e8bb085df1690f0cd\"\n",
    "\n",
    "\n",
    "# Headers for API access\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Ocp-Apim-Subscription-Key\": subscription_key\n",
    "}\n",
    "\n",
    "# Step 1: Fetch archive metadata\n",
    "print(\"📦 Fetching ERCOT DAM LMP archive list...\")\n",
    "endpoint = \"https://api.ercot.com/api/public-reports/archive/np4-183-cd\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "response.raise_for_status()\n",
    "archives = response.json().get(\"archives\", [])\n",
    "print(f\"✅ Found {len(archives)} archive entries.\")\n",
    "\n",
    "# ✅ Filter for only archives from 2022 onwards\n",
    "archives = [\n",
    "    archive for archive in archives\n",
    "    if \"postDatetime\" in archive and\n",
    "       dt.fromisoformat(archive[\"postDatetime\"][:19]) >= dt(2022, 1, 1) and\n",
    "       dt.fromisoformat(archive[\"postDatetime\"][:19]) <= dt(2022, 10, 1)\n",
    "]\n",
    "\n",
    "print(f\"✅ Found {len(archives)} archive entries from 2022 onward.\")\n",
    "\n",
    "# Output dir\n",
    "output_dir = \"iso_data/ercot_dam_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 2: Download and parse\n",
    "for i, archive in enumerate(archives):\n",
    "    download_url = f\"https://api.ercot.com/api/public-reports/archive/np4-183-cd?download={archive['docId']}\"\n",
    "    print(f\"\\n⬇️ Downloading ({i+1}): {archive['friendlyName']}\")\n",
    "    \n",
    "    for _ in range(3):\n",
    "        r = requests.get(download_url, headers=headers)\n",
    "        if r.status_code == 429:\n",
    "            print(\"⏳ Rate limit hit. Waiting...\")\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        print(f\"⚠️ Failed to download. Status: {r.status_code}\")\n",
    "        continue\n",
    "\n",
    "    content_type = r.headers.get(\"Content-Type\", \"\")\n",
    "    raw = r.content\n",
    "\n",
    "    try:\n",
    "        if raw.startswith(b'PK'):  # ZIP magic number\n",
    "            print(\"📦 Detected ZIP archive\")\n",
    "            with ZipFile(BytesIO(raw)) as z:\n",
    "                for file_info in z.infolist():\n",
    "                    if file_info.filename.endswith(\".csv\"):\n",
    "                        with z.open(file_info) as f:\n",
    "                            df = pd.read_csv(f, engine='python', on_bad_lines='skip')\n",
    "        else:\n",
    "            print(\"🧾 Detected raw CSV file\")\n",
    "            decoded = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "            df = pd.read_csv(io.StringIO(decoded), engine='python', on_bad_lines='skip')\n",
    "\n",
    "        # Normalize column names\n",
    "        df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"\")\n",
    "        print(\"📑 Columns:\", df.columns.tolist())\n",
    "\n",
    "        # Accept 'deliverydate' or fallback options\n",
    "        date_col = None\n",
    "        for col in df.columns:\n",
    "            if col in [\"deliverydate\", \"delivery_date\", \"delvdate\"]:\n",
    "                date_col = col\n",
    "                break\n",
    "\n",
    "        if not date_col:\n",
    "            print(f\"❌ No recognized delivery date column in: {archive['friendlyName']}\")\n",
    "            continue\n",
    "\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "        df[\"quarter\"] = df[date_col].dt.to_period(\"Q\").astype(str)\n",
    "\n",
    "        for quarter, group in df.groupby(\"quarter\"):\n",
    "            out_path = os.path.join(output_dir, f\"ERCOT_LMP_{quarter}.csv\")\n",
    "            if os.path.exists(out_path):\n",
    "                group.to_csv(out_path, mode='a', header=False, index=False)\n",
    "            else:\n",
    "                group.to_csv(out_path, index=False)\n",
    "        \n",
    "        print(f\"✅ Saved: {archive['friendlyName']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error parsing {archive['friendlyName']}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05fffa8-173c-4231-bba2-8263839d4615",
   "metadata": {},
   "source": [
    "#### SPP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5f9d4-c916-45f3-88f0-930a71d628e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Chrome driver with download options\n",
    "def setup_driver(download_dir):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_experimental_option(\"prefs\", {\n",
    "        \"download.default_directory\": os.path.abspath(download_dir),\n",
    "        \"download.prompt_for_download\": False,\n",
    "        \"directory_upgrade\": True,\n",
    "        \"safebrowsing.enabled\": True\n",
    "    })\n",
    "    chrome_options.add_argument(\"--start-maximized\")\n",
    "    return webdriver.Chrome(options=chrome_options)# Click any folder or file element\n",
    "def click_folder(driver, folder_name):\n",
    "    try:\n",
    "        print(f\":mag: Searching for: {folder_name}\")\n",
    "        elements = driver.find_elements(By.XPATH, f\"//*[contains(text(), '{folder_name}')]\")\n",
    "        print(f\":receipt: Found {len(elements)} element(s) with text '{folder_name}'\")\n",
    "        for el in elements:\n",
    "            try:\n",
    "                print(f\":mag_right: Trying: <{el.tag_name}> → '{el.text.strip()}'\")\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", el)\n",
    "                time.sleep(1)\n",
    "                el.click()\n",
    "                print(f\":white_check_mark: Clicked on: {folder_name}\")\n",
    "                return True\n",
    "            except Exception as click_error:\n",
    "                print(f\":warning: Failed to click: {click_error}\")\n",
    "        raise Exception(\"None of the matched elements were clickable.\")\n",
    "    except Exception as e:\n",
    "        print(f\":x: Failed to click '{folder_name}': {e}\")\n",
    "        return False# Wait for download to finish\n",
    "def wait_for_download(download_dir, timeout=180):\n",
    "    print(\":hourglass_flowing_sand: Waiting for download to complete...\")\n",
    "    seconds = 0\n",
    "    while seconds < timeout:\n",
    "        files = os.listdir(download_dir)\n",
    "        if any(f.endswith(\".zip\") for f in files) and not any(f.endswith(\".crdownload\") for f in files):\n",
    "            print(\":white_check_mark: Download complete!\")\n",
    "            return True\n",
    "        time.sleep(2)\n",
    "        seconds += 2\n",
    "    print(\":x: Download timed out.\")\n",
    "    return False# Save page source for debugging (optional)\n",
    "def save_page_source(driver, filename=\"spp_page_source.html\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(driver.page_source)\n",
    "    print(f\":floppy_disk: Saved page source to {filename}\")# Rename and unzip downloaded ZIP\n",
    "def rename_and_unzip(download_dir, year):\n",
    "    print(\":package: Preparing to rename and unzip...\")\n",
    "    zip_files = [f for f in os.listdir(download_dir) if f.endswith(\".zip\")]    if not zip_files:\n",
    "        print(\":x: No ZIP file found to rename and unzip.\")\n",
    "        return    original_zip = zip_files[0]\n",
    "    original_path = os.path.join(download_dir, original_zip)\n",
    "    new_zip_name = f\"SPP_LMP_{year}.zip\"\n",
    "    new_zip_path = os.path.join(download_dir, new_zip_name)    # Rename\n",
    "    os.rename(original_path, new_zip_path)\n",
    "    print(f\":white_check_mark: Renamed {original_zip} → {new_zip_name}\")    # Unzip\n",
    "    extract_dir = os.path.join(download_dir, year)\n",
    "    os.makedirs(extract_dir, exist_ok=True)    with zipfile.ZipFile(new_zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\":white_check_mark: Extracted contents into {extract_dir}\")    # Delete ZIP after extraction\n",
    "    os.remove(new_zip_path)\n",
    "    print(f\":wastebasket: Deleted ZIP file: {new_zip_path}\")# --- MAIN SCRIPT ---\n",
    "if __name__ == \"__main__\":\n",
    "    year = \"2023\"\n",
    "    download_dir = os.path.join(os.getcwd(), \"spp_downloads\")\n",
    "    os.makedirs(download_dir, exist_ok=True)    driver = setup_driver(download_dir)    try:\n",
    "        print(\":globe_with_meridians: Opening SPP LMP Portal...\")\n",
    "        driver.get(\"https://portal.spp.org/pages/da-lmp-by-location\")\n",
    "        time.sleep(6)        save_page_source(driver)  # Optional        # Step 1: Click the Year Folder (e.g., \"2023\")\n",
    "        success = click_folder(driver, year)\n",
    "        time.sleep(5)        if not success:\n",
    "            print(\":x: Could not open year folder.\")\n",
    "        else:\n",
    "            # Step 2: Find and Click on the \"2023.zip\" text\n",
    "            print(\":mag: Waiting for '2023.zip' file to appear...\")\n",
    "            wait = WebDriverWait(driver, 20)\n",
    "            try:\n",
    "                zip_element = wait.until(EC.presence_of_element_located((By.XPATH, \"//*[contains(text(), '2023.zip')]\")))\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView({block: 'center'});\", zip_element)\n",
    "                time.sleep(1)\n",
    "                zip_element.click()\n",
    "                print(\":inbox_tray: Clicking to download 2023.zip file...\")                # Step 3: Wait for download\n",
    "                if wait_for_download(download_dir):\n",
    "                    print(f\":dart: Successfully downloaded ZIP for {year}!\")\n",
    "                    rename_and_unzip(download_dir, year)\n",
    "                else:\n",
    "                    print(f\":warning: Download might have failed. Check {download_dir}.\")\n",
    "            except Exception as e:\n",
    "                print(f\":x: Could not find or click 2023.zip: {e}\")\n",
    "    finally:\n",
    "        input(\":mag: Press Enter to quit browser...\")\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747e53d-41a7-4ae0-950e-cd7035195062",
   "metadata": {},
   "source": [
    "#### CAISO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa32bee-f2c2-4e38-ad96-711c3489bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_day(date, nodes=None):\n",
    "    \"\"\"Download and extract DAM_LMP CSV for a single day\"\"\"\n",
    "    base_url = \"http://oasis.caiso.com/oasisapi/SingleZip\"\n",
    "    date_str = date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    params = {\n",
    "        \"resultformat\": \"6\",\n",
    "        \"queryname\": \"PRC_LMP\",\n",
    "        \"version\": \"12\",\n",
    "        \"market_run_id\": \"DAM\",\n",
    "        \"startdatetime\": f\"{date_str}T08:00-0000\",\n",
    "        \"enddatetime\": f\"{date_str}T08:00-0000\",\n",
    "        \"grp_type\": \"ALL\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, timeout=60)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Download error for {date_str}: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            for name in z.namelist():\n",
    "                if \"DAM_LMP\" in name and name.endswith(\".csv\"):\n",
    "                    with z.open(name) as f:\n",
    "                        df = pd.read_csv(f)\n",
    "                        if nodes is not None:\n",
    "                            df = df[df[\"NODE\"].isin(nodes)]\n",
    "                        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ZIP processing error for {date_str}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_caiso_dam_lmp_parallel(start_date, end_date, nodes=None, max_workers=8):\n",
    "    \"\"\"Parallel downloader for CAISO DAM_LMP data\"\"\"\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    date_list = [start_dt + timedelta(days=i) for i in range((end_dt - start_dt).days + 1)]\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(download_day, date, nodes): date for date in date_list}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"CAISO Parallel\"):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                all_data.append(result)\n",
    "\n",
    "    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    save_folder = \"iso_data/caiso_data\"  # ✅ Your desired folder path\n",
    "    os.makedirs(save_folder, exist_ok=True)   # ✅ Create it if it doesn't exist\n",
    "\n",
    "    df = fetch_caiso_dam_lmp_parallel(\n",
    "        start_date=\"2023-01-01\",\n",
    "        end_date=\"2024-01-01\",\n",
    "        max_workers=6\n",
    "    )\n",
    "\n",
    "    if not df.empty:\n",
    "        save_path = os.path.join(save_folder, \"caiso_dam_lmp_parallel.csv\")\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"✅ Saved to {save_path}\")\n",
    "    else:\n",
    "        print(\"🚫 No data downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dedff3d-5ea6-4961-b0e5-135cd76a8e02",
   "metadata": {},
   "source": [
    "#### Interconnection Queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2549e701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s5/3y73n7d966b04_w4__8xvr3c0000gn/T/ipykernel_23586/3175970722.py:13: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n",
      "  opts.headless = True\n",
      "/var/folders/s5/3y73n7d966b04_w4__8xvr3c0000gn/T/ipykernel_23586/3175970722.py:14: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install(), options=opts)\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def scrape(month, year):\n",
    "    # Berkeley Lab website\n",
    "    base_url = 'https://emp.lbl.gov/ '\n",
    "\n",
    "    opts = Options()\n",
    "    opts.headless = True\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install(), options=opts)\n",
    "\n",
    "    driver.get(base_url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # proxy for general file url\n",
    "    file_url = (\n",
    "        'https://emp.lbl.gov/sites/default/files/'\n",
    "        + year + '-' + month +\n",
    "        '/queues_'\n",
    "        + str(int(year)-1) + '_clean_data_r1.xlsx'\n",
    "    )\n",
    "\n",
    "    resp = requests.get(file_url, headers={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "    })\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    output_dir = 'iso_data/interconnection_data/'\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "    filename = 'iso_data/interconnection_data/queues_' + year + '.xlsx'\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(resp.content)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    scrape('04', '2024')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16ca7c-fccd-4e18-9012-ef30a88a2eea",
   "metadata": {},
   "source": [
    "#### EIA Wholesale Electricity Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f65db058-a79a-4d89-8cb2-b9b7e184c171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: ice_electric-2022final.xlsx\n",
      "Downloaded: ice_electric-2023final.xlsx\n",
      "Downloaded: ice_electric-2024final.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Define the base URL and the list of file names\n",
    "base_url = \"https://www.eia.gov/electricity/wholesale/xls/archive/\"\n",
    "file_names = [\n",
    "    \"ice_electric-2022final.xlsx\",\n",
    "    \"ice_electric-2023final.xlsx\",\n",
    "    \"ice_electric-2024final.xlsx\"\n",
    "]\n",
    "\n",
    "# Directory where the files will be saved\n",
    "save_dir = 'iso_data/Wholesale_Pricing_Data'\n",
    "os.makedirs(save_dir)\n",
    "\n",
    "# Loop to download each file\n",
    "for file_name in file_names:\n",
    "    full_url = base_url + file_name\n",
    "    response = requests.get(full_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        file_path = os.path.join(save_dir, file_name)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded: {file_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to download: {file_name} (Status code: {response.status_code})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe8cab-a2f3-44d4-b6df-49480ceea4fc",
   "metadata": {},
   "source": [
    "### 3. Organize Each Data Source Into Yearly Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e9f8084-defe-4ed5-a811-108b0e8e947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_quarters(folder, year=None):\n",
    "    \"\"\"\n",
    "    Combines quarterly CSVs in a folder.\n",
    "    If 'year' is provided, only combines files containing that year in the filename.\n",
    "    \"\"\"\n",
    "    files = sorted([f for f in os.listdir(folder) if f.endswith('.csv')])\n",
    "\n",
    "    if year is not None:\n",
    "        files = [f for f in files if str(year) in f]\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for f in files:\n",
    "        path = os.path.join(folder, f)\n",
    "        try:\n",
    "            df = pd.read_csv(path, low_memory=False)\n",
    "            dfs.append(df)\n",
    "            print(f\"✅ Loaded {f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Failed to load {f}: {e}\")\n",
    "\n",
    "    if not dfs:\n",
    "        print(f\"⚠️ No files found for year {year} in {folder}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"🧩 Combined {len(dfs)} files with {len(combined):,} rows for year {year}.\")\n",
    "    return combined\n",
    "\n",
    "def combine_files(year):\n",
    "    # Combine only MISO yearly files\n",
    "    miso_df = combine_quarters('iso_data/miso_data', year=year)\n",
    "\n",
    "    # Combine only ISO-NE yearly files\n",
    "    isone_df = combine_quarters('iso_data/isone_lmp_data', year=year)\n",
    "\n",
    "    # Combine only NYISO yearly files\n",
    "    nyo_df = combine_quarters('iso_data/nyiso_data/nyiso_combined_quarters', year=year)\n",
    "\n",
    "    # Combine only ERCOT yearly files\n",
    "    ercot_df = combine_quarters('iso_data/ercot_dam_outputs', year=year)\n",
    "\n",
    "    # Combine only SPP yearly files\n",
    "    spp_df = combine_quarters('iso_data/spp_lmp_quarters', year=year)\n",
    "\n",
    "    # Combine only PJM 2023 files\n",
    "    pjm_df = combine_quarters('iso_data/pjm_data', year=year)\n",
    "\n",
    "    # CAISO already downloaded into yearly format\n",
    "\n",
    "    return miso_df, isone_df, nyo_df, ercot_df, spp_df, pjm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2df11dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2023_Q1.csv\n",
      "✅ Loaded 2023_Q2.csv\n",
      "✅ Loaded 2023_Q3.csv\n",
      "✅ Loaded 2023_Q4.csv\n",
      "🧩 Combined 4 files with 2,621,547 rows for year 2023.\n",
      "✅ Loaded isone_lmp_2023Q1.csv\n",
      "✅ Loaded isone_lmp_2023Q2.csv\n",
      "✅ Loaded isone_lmp_2023Q3.csv\n",
      "✅ Loaded isone_lmp_2023Q4.csv\n",
      "🧩 Combined 4 files with 10,641,239 rows for year 2023.\n",
      "✅ Loaded nyiso_combined_2023Q1.csv\n",
      "✅ Loaded nyiso_combined_2023Q2.csv\n",
      "✅ Loaded nyiso_combined_2023Q3.csv\n",
      "✅ Loaded nyiso_combined_2023Q4.csv\n",
      "🧩 Combined 4 files with 6,153,262 rows for year 2023.\n",
      "✅ Loaded ERCOT_LMP_2023Q1.csv\n",
      "✅ Loaded ERCOT_LMP_2023Q2.csv\n",
      "✅ Loaded ERCOT_LMP_2023Q3.csv\n",
      "✅ Loaded ERCOT_LMP_2023Q4.csv\n",
      "🧩 Combined 4 files with 145,255,128 rows for year 2023.\n",
      "✅ Loaded spp_lmp_2023Q1.csv\n",
      "✅ Loaded spp_lmp_2023Q2.csv\n",
      "✅ Loaded spp_lmp_2023Q3.csv\n",
      "✅ Loaded spp_lmp_2023Q4.csv\n",
      "🧩 Combined 4 files with 10,165,309 rows for year 2023.\n",
      "✅ Loaded rt_da_monthly_lmps_april2023.csv\n",
      "✅ Loaded rt_da_monthly_lmps_aug2023.csv\n",
      "✅ Loaded rt_da_monthly_lmps_dec2023.csv\n",
      "✅ Loaded rt_da_monthly_lmps_feb2023.csv\n",
      "✅ Loaded rt_da_monthly_lmps_jan2023.csv\n",
      "✅ Loaded rt_da_monthly_lmps_july2023.csv\n",
      "✅ Loaded rt_da_monthly_lmps_june2023.csv\n",
      "✅ Loaded rt_da_monthly_lmps_march2023.csv\n",
      "✅ Loaded rt_da_monthly_lmps_may2023.csv\n",
      "✅ Loaded rt_da_monthly_lmps_nov2023.csv\n",
      "✅ Loaded rt_da_monthly_lmps_oct2023.csv\n",
      "✅ Loaded rt_da_monthly_lmps_sept2023.csv\n",
      "🧩 Combined 12 files with 2,885,733 rows for year 2023.\n"
     ]
    }
   ],
   "source": [
    "# year is a variable — change accordingly!\n",
    "#miso_df, isone_df, nyo_df, ercot_df, spp_df, pjm_df = combine_files(2022)\n",
    "miso_df, isone_df, nyo_df, ercot_df, spp_df, pjm_df = combine_files(2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d986a72-e3a7-4f7a-8d4e-d24b776e82ef",
   "metadata": {},
   "source": [
    "### 4. Combine Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c68fc1-c49d-40fa-b99e-d040214d0894",
   "metadata": {},
   "source": [
    "#### ISO-NE, NYISO, PJM + Respective Interconnection Queues and Wholesale Electricity Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3442e86d-ccc1-4ac5-84a8-c14e905a397e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Combined raw DataFrame shape: (19680234, 34)\n",
      "                              iso        Location Name Location Type    LMP  \\\n",
      "timestamp_utc                                                                 \n",
      "2023-01-01 00:00:00+00:00  ISO-NE  UN.FRNKLNSQ13.810CC          Node  31.28   \n",
      "2023-01-01 00:00:00+00:00   NYISO                NORTH          Node  19.64   \n",
      "2023-01-01 00:00:00+00:00   NYISO                  NPX          Node  35.37   \n",
      "2023-01-01 00:00:00+00:00   NYISO                  O H          Node  20.90   \n",
      "2023-01-01 00:00:00+00:00   NYISO                  PJM          Node  26.94   \n",
      "...                           ...                  ...           ...    ...   \n",
      "2024-01-01 04:00:00+00:00     PJM           MIDLOTHIAN          Node  20.42   \n",
      "2024-01-01 04:00:00+00:00     PJM           NORTH ANNA          Node  20.52   \n",
      "2024-01-01 04:00:00+00:00     PJM             FENTRESS          Node  20.46   \n",
      "2024-01-01 04:00:00+00:00     PJM             NAGELAEP          Node  19.52   \n",
      "2024-01-01 04:00:00+00:00     PJM         BLAKELY BORO          Node  14.74   \n",
      "\n",
      "                             MCC   MLC  \n",
      "timestamp_utc                           \n",
      "2023-01-01 00:00:00+00:00   0.00  0.00  \n",
      "2023-01-01 00:00:00+00:00   0.00 -0.28  \n",
      "2023-01-01 00:00:00+00:00 -14.25  1.20  \n",
      "2023-01-01 00:00:00+00:00  -1.89 -0.92  \n",
      "2023-01-01 00:00:00+00:00  -6.78  0.24  \n",
      "...                          ...   ...  \n",
      "2024-01-01 04:00:00+00:00   0.30  0.34  \n",
      "2024-01-01 04:00:00+00:00   0.32  0.42  \n",
      "2024-01-01 04:00:00+00:00   0.29  0.39  \n",
      "2024-01-01 04:00:00+00:00   0.17 -0.43  \n",
      "2024-01-01 04:00:00+00:00  -4.32 -0.72  \n",
      "\n",
      "[19679018 rows x 6 columns]\n",
      "                              iso        Location Name Location Type  node  \\\n",
      "timestamp_utc                                                                \n",
      "2023-01-01 00:00:00+00:00  ISO-NE  UN.FRNKLNSQ13.810CC          Node  node   \n",
      "2023-01-01 00:00:00+00:00   NYISO                NORTH          Node  node   \n",
      "2023-01-01 00:00:00+00:00   NYISO                  NPX          Node  node   \n",
      "2023-01-01 00:00:00+00:00   NYISO                  O H          Node  node   \n",
      "2023-01-01 00:00:00+00:00   NYISO                  PJM          Node  node   \n",
      "...                           ...                  ...           ...   ...   \n",
      "2024-01-01 04:00:00+00:00     PJM           MIDLOTHIAN          Node  node   \n",
      "2024-01-01 04:00:00+00:00     PJM           NORTH ANNA          Node  node   \n",
      "2024-01-01 04:00:00+00:00     PJM             FENTRESS          Node  node   \n",
      "2024-01-01 04:00:00+00:00     PJM             NAGELAEP          Node  node   \n",
      "2024-01-01 04:00:00+00:00     PJM         BLAKELY BORO          Node  node   \n",
      "\n",
      "                             LMP    MCC   MLC  \n",
      "timestamp_utc                                  \n",
      "2023-01-01 00:00:00+00:00  31.28   0.00  0.00  \n",
      "2023-01-01 00:00:00+00:00  19.64   0.00 -0.28  \n",
      "2023-01-01 00:00:00+00:00  35.37 -14.25  1.20  \n",
      "2023-01-01 00:00:00+00:00  20.90  -1.89 -0.92  \n",
      "2023-01-01 00:00:00+00:00  26.94  -6.78  0.24  \n",
      "...                          ...    ...   ...  \n",
      "2024-01-01 04:00:00+00:00  20.42   0.30  0.34  \n",
      "2024-01-01 04:00:00+00:00  20.52   0.32  0.42  \n",
      "2024-01-01 04:00:00+00:00  20.46   0.29  0.39  \n",
      "2024-01-01 04:00:00+00:00  19.52   0.17 -0.43  \n",
      "2024-01-01 04:00:00+00:00  14.74  -4.32 -0.72  \n",
      "\n",
      "[19679018 rows x 7 columns]\n",
      "✅ Successfully saved to iso_data/yearly_combined_data/ISO_combined/combined_ne_2023_lmp_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Combine NE/Mid-atlantic ISO dataframes\n",
    "combined_ne_raw_df = pd.concat([\n",
    "    nyo_df,\n",
    "    isone_df,\n",
    "    pjm_df\n",
    "], ignore_index=True)\n",
    "\n",
    "#print(combined_ne_raw_df.head())\n",
    "\n",
    "print(f\"✅ Combined raw DataFrame shape: {combined_ne_raw_df.shape}\")\n",
    "\n",
    "def clean_pjm(pjm_df):\n",
    "    \"\"\"Clean PJM dataframe to standard format.\"\"\"\n",
    "    pjm_df = pjm_df.copy()\n",
    "    pjm_df['timestamp_utc'] = pd.to_datetime(pjm_df['datetime_beginning_utc'], utc = True)\n",
    "    pjm_df['iso'] = 'PJM'\n",
    "    pjm_df['Location Name'] = pjm_df['pnode_name']\n",
    "    pjm_df['Location Type'] = 'Node'\n",
    "    pjm_df['LMP'] = pjm_df['total_lmp_da']  # or 'total_lmp_rt' if you prefer real-time\n",
    "    pjm_df['MCC'] = pjm_df['congestion_price_da']\n",
    "    pjm_df['MLC'] = pjm_df['marginal_loss_price_da']\n",
    "    \n",
    "    return pjm_df[['timestamp_utc', 'iso', 'Location Name', 'Location Type', 'LMP', 'MCC', 'MLC']]\n",
    "\n",
    "def clean_nyiso(nyiso_df):\n",
    "    \"\"\"Clean NYISO dataframe to standard format.\"\"\"\n",
    "    nyiso_df = nyiso_df.copy()\n",
    "    nyiso_df['timestamp_utc'] = pd.to_datetime(nyiso_df['Date'], utc = True)\n",
    "    nyiso_df['iso'] = 'NYISO'\n",
    "    nyiso_df['Location Name'] = nyiso_df['Name']\n",
    "    nyiso_df['Location Type'] = 'Node'\n",
    "    nyiso_df['LMP'] = nyiso_df['LBMP ($/MWHr)'] \n",
    "    nyiso_df['MCC'] = nyiso_df['Marginal Cost Congestion ($/MWHr)']\n",
    "    nyiso_df['MLC'] = nyiso_df['Marginal Cost Losses ($/MWHr)']\n",
    "    \n",
    "    return nyiso_df[['timestamp_utc', 'iso', 'Location Name', 'Location Type', 'LMP', 'MCC', 'MLC']]\n",
    "\n",
    "def clean_isone(isone_df):\n",
    "    \"\"\"Clean ISO-NE dataframe to standard format.\"\"\"\n",
    "    isone_df = isone_df.copy()\n",
    "\n",
    "    # Safe handling of 'Hour Ending'\n",
    "    isone_df['Hour Ending'] = isone_df['Hour Ending'].astype(str)\n",
    "    isone_df = isone_df[isone_df['Hour Ending'].str.isnumeric()]\n",
    "    isone_df['Hour Ending'] = isone_df['Hour Ending'].astype(int)\n",
    "    isone_df['timestamp_utc'] = pd.to_datetime(isone_df['Date']) + pd.to_timedelta(isone_df['Hour Ending'] - 1, unit='h')\n",
    "    isone_df['timestamp_utc'] = pd.to_datetime(isone_df['timestamp_utc'], utc=True)\n",
    "\n",
    "    isone_df['iso'] = 'ISO-NE'\n",
    "    isone_df['Location Name'] = isone_df['Location Name']\n",
    "    isone_df['Location Type'] = 'Node'  # Simplify network node to Node\n",
    "    isone_df['LMP'] = isone_df['Locational Marginal Price']\n",
    "    isone_df['MCC'] = isone_df['Congestion Component']\n",
    "    isone_df['MLC'] = isone_df['Marginal Loss Component']\n",
    "    \n",
    "    return isone_df[['timestamp_utc', 'iso', 'Location Name', 'Location Type', 'LMP', 'MCC', 'MLC']]\n",
    "\n",
    "def combine_isos(pjm_df, nyiso_df, isone_df):\n",
    "    \"\"\"Combine cleaned ISO dataframes into one.\"\"\"\n",
    "    pjm_clean = clean_pjm(pjm_df)\n",
    "    nyiso_clean = clean_nyiso(nyiso_df)\n",
    "    isone_clean = clean_isone(isone_df)\n",
    "    \n",
    "    combined_df = pd.concat([pjm_clean, nyiso_clean, isone_clean], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "combined_ne_df = combine_isos(pjm_df, nyo_df, isone_df)\n",
    "\n",
    "combined_ne_df = combined_ne_df.sort_values('timestamp_utc').reset_index(drop=True)\n",
    "\n",
    "# 🧠 Now set timestamp_utc as index\n",
    "combined_ne_df = combined_ne_df.set_index('timestamp_utc')\n",
    "print(combined_ne_df)\n",
    "\n",
    "combined_ne_df['node'] = 'node'\n",
    "combined_ne_df_2023 = combined_ne_df[['iso', 'Location Name', 'Location Type', 'node', 'LMP', 'MCC', 'MLC']]\n",
    "print(combined_ne_df_2023)\n",
    "\n",
    "output_path = 'iso_data/yearly_combined_data/ISO_combined/combined_ne_2023_lmp_data.csv'\n",
    "combined_ne_df_2023.to_csv(output_path)\n",
    "\n",
    "print(f\"✅ Successfully saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19cf2762-dee2-4504-a123-8289c2e58f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gr/lg07kybx73v06wc5f3xc5w5w0000gn/T/ipykernel_22630/3803284040.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(series,\n",
      "/var/folders/gr/lg07kybx73v06wc5f3xc5w5w0000gn/T/ipykernel_22630/3803284040.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(series,\n",
      "/var/folders/gr/lg07kybx73v06wc5f3xc5w5w0000gn/T/ipykernel_22630/3803284040.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(series,\n",
      "/var/folders/gr/lg07kybx73v06wc5f3xc5w5w0000gn/T/ipykernel_22630/3803284040.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(series,\n"
     ]
    }
   ],
   "source": [
    "# add in interconnection queue data\n",
    "queues = pd.read_excel('iso_data/interconnection_data/queues_2024.xlsx', sheet_name=1)\n",
    "\n",
    "def standardize_dates(series):\n",
    "    return pd.to_datetime(series, \n",
    "                          infer_datetime_format=True,\n",
    "                          errors='coerce')\n",
    "\n",
    "queues['q_date'] = standardize_dates(queues['q_date'])\n",
    "queues['ia_date'] = standardize_dates(queues['ia_date'])\n",
    "queues['wd_date'] = standardize_dates(queues['wd_date'])\n",
    "queues['on_date'] = standardize_dates(queues['on_date'])\n",
    "\n",
    "cutoff = pd.Timestamp('2022-01-01')\n",
    "filtered_queues = queues.loc[(queues['q_date'] >= cutoff) & \n",
    "                             ((queues['ia_date'] >= cutoff)|(queues['ia_date'].isnull())) & \n",
    "                             ((queues['wd_date'] >= cutoff)|(queues['wd_date'].isnull())) &\n",
    "                             ((queues['on_date'] >= cutoff)|(queues['on_date'].isnull()))]\n",
    "\n",
    "# q_date: date when project entered queue\n",
    "# ia_date: date of signed interconnection agreement\n",
    "# wd_date: date project withdrawn from queue\n",
    "# on_date: date project became operational\n",
    "\n",
    "selected_cols = ['q_id', 'region', 'q_date', \n",
    "                 'ia_date', 'wd_date', 'on_date']\n",
    "\n",
    "selected_regions = ['PJM', 'ERCOT', 'CAISO', 'SPP', 'NYISO', 'ISO-NE']\n",
    "\n",
    "filtered_queues = filtered_queues[filtered_queues['region'].isin(selected_regions)].reset_index(drop=True)\n",
    "\n",
    "def queue_duration(df, start, end):\n",
    "    difference = df[end] - df[start]\n",
    "    return difference.dt.days\n",
    "\n",
    "filtered_queues['days_to_ia'] = queue_duration(filtered_queues, 'q_date', 'ia_date')\n",
    "filtered_queues['days_to_wd'] = queue_duration(filtered_queues, 'q_date', 'wd_date')\n",
    "filtered_queues['days_to_on'] = queue_duration(filtered_queues, 'q_date', 'on_date')\n",
    "\n",
    "as_of = pd.to_datetime('2023-12-31')\n",
    "pending = filtered_queues[['ia_date','wd_date','on_date']].isna().all(axis=1)\n",
    "\n",
    "filtered_queues['days_pending'] = np.where(\n",
    "    pending,\n",
    "    (as_of - filtered_queues['q_date']).dt.days,\n",
    "    np.nan)\n",
    "\n",
    "selected_cols = selected_cols + ['days_to_ia', 'days_to_wd', 'days_to_on', 'days_pending']\n",
    "\n",
    "# 1) build a “long” events frame: one row per event occurrence\n",
    "events = []\n",
    "for event, date_col, days_col in [\n",
    "    ('ia', 'ia_date', 'days_to_ia'),\n",
    "    ('wd', 'wd_date', 'days_to_wd'),\n",
    "    ('on', 'on_date', 'days_to_on'),\n",
    "]:\n",
    "    tmp = (\n",
    "        filtered_queues\n",
    "        .loc[filtered_queues[date_col].notna(), ['region', date_col, days_col]]\n",
    "        .rename(columns={date_col: 'date', days_col: 'days_to'})\n",
    "    )\n",
    "    tmp['event'] = event\n",
    "    events.append(tmp)\n",
    "events = pd.concat(events, ignore_index=True)\n",
    "\n",
    "events.sort_values(by=['region','date'])\n",
    "\n",
    "# 1) pivot out daily counts and daily average days_to\n",
    "daily_counts = (\n",
    "    events\n",
    "      .groupby(['region','date','event'])\n",
    "      .size()\n",
    "      .unstack('event', fill_value=0)\n",
    "      .rename(columns={'ia':'count_ia','wd':'count_wd','on':'count_on'}))\n",
    "\n",
    "daily_days = (\n",
    "    events\n",
    "      .groupby(['region','date','event'])['days_to']\n",
    "      .mean()\n",
    "      .unstack('event')\n",
    "      .rename(columns={'ia':'days_to_ia','wd':'days_to_wd','on':'days_to_on'}))\n",
    "\n",
    "daily = (\n",
    "    daily_counts\n",
    "      .join(daily_days, how='outer')\n",
    "      .sort_index())\n",
    "\n",
    "# 2) reindex to every calendar date so cumsum/expanding works\n",
    "all_dates = pd.date_range(\n",
    "    events['date'].min().floor('D'),\n",
    "    events['date'].max().ceil('D'),\n",
    "    freq='D'\n",
    ")\n",
    "idx = pd.MultiIndex.from_product(\n",
    "    [daily.index.levels[0], all_dates],\n",
    "    names=['region','date']\n",
    ")\n",
    "daily = daily.reindex(idx, fill_value=0).sort_index()\n",
    "\n",
    "# 3) cumulative sums of counts\n",
    "daily['cum_ia'] = daily.groupby(level='region')['count_ia'].cumsum()\n",
    "daily['cum_wd'] = daily.groupby(level='region')['count_wd'].cumsum()\n",
    "daily['cum_on'] = daily.groupby(level='region')['count_on'].cumsum()\n",
    "\n",
    "# 4) cumulative sums & counts of days_to, then expanding mean\n",
    "for ev in ['ia','wd','on']:\n",
    "    # running sum of days_to\n",
    "    daily[f'sum_days_to_{ev}'] = (\n",
    "        daily[f'days_to_{ev}']\n",
    "          .groupby(level='region')\n",
    "          .cumsum()\n",
    "    )\n",
    "    # running count of events (same as cum_count)\n",
    "    daily[f'cnt_days_to_{ev}'] = daily[f'count_{ev}'].groupby(level='region').cumsum()\n",
    "    # expanding average = sum / count\n",
    "    daily[f'avg_days_to_{ev}'] = (\n",
    "        daily[f'sum_days_to_{ev}'] / daily[f'cnt_days_to_{ev}'])\n",
    "\n",
    "# 5) clean up: drop intermediate columns\n",
    "final = daily.reset_index().drop(\n",
    "    columns=[f'days_to_{ev}'      for ev in ['ia','wd','on']]\n",
    "           +[f'sum_days_to_{ev}'  for ev in ['ia','wd','on']]\n",
    "           +[f'cnt_days_to_{ev}'  for ev in ['ia','wd','on']])\n",
    "\n",
    "# 0) your cutoff\n",
    "as_of = pd.to_datetime('2023-12-31')\n",
    "\n",
    "# 1) build the calendar of snapshot dates **only** through the cutoff\n",
    "all_dates = pd.date_range(\n",
    "    start=filtered_queues['q_date'].min().floor('D'),\n",
    "    end=as_of,\n",
    "    freq='D')\n",
    "\n",
    "# 2) pull in each project’s dates\n",
    "proj_raw = filtered_queues[['region','q_date','ia_date','wd_date','on_date']].copy()\n",
    "\n",
    "# 3) Cartesian-merge so each project is paired with each snapshot date\n",
    "dates_df = pd.DataFrame({'date': all_dates})\n",
    "projects = (\n",
    "    proj_raw\n",
    "      .assign(key=1)\n",
    "      .merge(dates_df.assign(key=1), on='key')\n",
    "      .drop('key', axis=1))\n",
    "\n",
    "# 4) replace NaT (i.e. “never happened by cutoff”) with far-future\n",
    "for col in ['ia_date','wd_date','on_date']:\n",
    "    projects[col] = projects[col].fillna(pd.Timestamp.max)\n",
    "\n",
    "# 5) filter to only those still pending **at** each snapshot date\n",
    "mask = (\n",
    "    (projects['date'] >= projects['q_date']) &\n",
    "    (projects['date'] <  projects['ia_date']) &\n",
    "    (projects['date'] <  projects['wd_date']) &\n",
    "    (projects['date'] <  projects['on_date']))\n",
    "pending = projects.loc[mask, ['region','date','q_date']]\n",
    "\n",
    "# 6) compute days_pending and then the daily average by region\n",
    "pending['days_pending'] = (pending['date'] - pending['q_date']).dt.days\n",
    "\n",
    "avg_pending = (\n",
    "    pending\n",
    "      .groupby(['region','date'])['days_pending']\n",
    "      .mean()\n",
    "      .reset_index(name='avg_days_pending'))\n",
    "\n",
    "# d) merge with rolled\n",
    "agg = (\n",
    "    final\n",
    "    .merge(avg_pending, on=['region','date'], how='right')\n",
    "    .sort_values(['region','date']))\n",
    "\n",
    "# final now has, for each ISO and each calendar date:\n",
    "#   - count_ia, count_wd, count_on (30-day sums)\n",
    "#   - avg_days_to_ia, avg_days_to_wd, avg_days_to_on (30-day means)\n",
    "#   - avg_days_pending (daily average for all still-pending in that snapshot)\n",
    "\n",
    "lmp = pd.read_csv('iso_data/yearly_combined_data/ISO_combined/combined_ne_2023_lmp_data.csv')\n",
    "# 1) Make sure your queue‐metrics DataFrame has a proper datetime “date” column\n",
    "agg['date'] = pd.to_datetime(agg['date']).dt.normalize()\n",
    "agg['date'] = pd.to_datetime(agg['date']).dt.tz_localize('UTC')\n",
    "\n",
    "agg = agg.rename(columns={'region':'iso'})\n",
    "agg = agg.drop(['count_ia', 'count_on', 'count_wd'], axis=1)\n",
    "\n",
    "# 2) Prepare your LMP DataFrame\n",
    "#    – parse timestamp_utc → datetime\n",
    "#    – extract the date (drop the time component)\n",
    "lmp['date'] = pd.to_datetime(lmp['timestamp_utc']).dt.normalize()\n",
    "\n",
    "# 4) Merge the two tables on region & date\n",
    "merged = (\n",
    "    lmp\n",
    "      .merge(agg, on=['iso','date'], how='left')\n",
    ").drop(['date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "687d2b6f-3777-45ea-9ef0-e33f7526f310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trade date years: [2021, 2022, 2023, 2024]\n",
      "Delivery start date years: [2021, 2022, 2023, 2024]\n",
      "Delivery end date years: [2021, 2022, 2023, 2024]\n"
     ]
    }
   ],
   "source": [
    "# add in eia wholesale data\n",
    "\n",
    "data_2022 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2022final.xlsx\")\n",
    "data_2023 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2023final.xlsx\")\n",
    "data_2024 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2024final.xlsx\")\n",
    "\n",
    "eia_pricing_data = pd.concat([data_2022, data_2023, data_2024], ignore_index=True, axis=0)\n",
    "\n",
    "hub_to_iso = {\n",
    "    'Indiana Hub RT Peak': 'MISO',\n",
    "    'Mid C Peak': 'Non-ISO (Mid-Columbia)',\n",
    "    'NP15 EZ Gen DA LMP Peak': 'CAISO',\n",
    "    'Nepool MH DA LMP Peak': 'ISO-NE',\n",
    "    'PJM WH Real Time Peak': 'PJM',\n",
    "    'Palo Verde Peak': 'CAISO',\n",
    "    'SP15 EZ Gen DA LMP Peak': 'CAISO'\n",
    "}\n",
    "\n",
    "eia_pricing_data['ISO'] = eia_pricing_data['Price hub'].map(hub_to_iso)\n",
    "\n",
    "# Convert dates to datetime\n",
    "trade_dates = pd.to_datetime(eia_pricing_data['Trade date'], format='mixed', errors='coerce')\n",
    "delivery_start_dates = pd.to_datetime(eia_pricing_data['Delivery start date'], format='mixed', errors='coerce')\n",
    "delivery_end_dates = pd.to_datetime(eia_pricing_data['Delivery \\nend date'], format='mixed', errors='coerce')\n",
    "\n",
    "# Check the unique years\n",
    "print(\"Trade date years:\", sorted(trade_dates.dropna().dt.year.unique()))\n",
    "print(\"Delivery start date years:\", sorted(delivery_start_dates.dropna().dt.year.unique()))\n",
    "print(\"Delivery end date years:\", sorted(delivery_end_dates.dropna().dt.year.unique()))\n",
    "\n",
    "eia_daily = eia_pricing_data.copy()\n",
    "\n",
    "# Convert 'Trade date' column to datetime objects \n",
    "eia_daily['Trade date'] = pd.to_datetime(eia_daily['Trade date'], format='mixed').dt.date\n",
    "\n",
    "# Create a new column for weighted price = price × volume\n",
    "eia_daily['weighted_price'] = eia_daily['Wtd avg price $/MWh'] * eia_daily['Daily volume MWh']\n",
    "\n",
    "# Group data by Trade date and ISO, and aggregate:\n",
    "eia_daily_summary = (\n",
    "    eia_daily\n",
    "    .groupby(['Trade date', 'ISO'])\n",
    "    .agg(\n",
    "        weighted_avg_price=('weighted_price', 'sum'),           # sum of (P × V)\n",
    "        total_volume=('Daily volume MWh', 'sum'),                # sum of volume\n",
    "        total_trades=('Number of trades', 'sum'),                # sum of trades\n",
    "        total_counterparties=('Number of counterparties', 'sum') # sum of counterparties\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate the volume-weighted average price\n",
    "eia_daily_summary['Wtd avg price $/MWh'] = eia_daily_summary['weighted_avg_price'] / eia_daily_summary['total_volume']\n",
    "\n",
    "# Select and reorder the final columns\n",
    "eia_daily_summary = eia_daily_summary[\n",
    "    ['Trade date', 'ISO', 'Wtd avg price $/MWh', 'total_volume', 'total_trades', 'total_counterparties']\n",
    "]\n",
    "\n",
    "# Filter to include only Trade dates from 2022, 2023, or 2024\n",
    "eia_daily_summary = eia_daily_summary[\n",
    "    eia_daily_summary['Trade date'].apply(lambda x: x.year).isin([2022, 2023, 2024])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "merged['date'] = pd.to_datetime(merged['timestamp_utc']).dt.date\n",
    "\n",
    "merged_df = merged.merge(\n",
    "    eia_daily_summary,\n",
    "    how='left',\n",
    "    left_on=['date', 'iso'],\n",
    "    right_on=['Trade date', 'ISO']\n",
    ")\n",
    "\n",
    "merged_df = merged_df.drop(columns=['Trade date', 'ISO', 'date'])\n",
    "merged_df = merged_df.set_index('timestamp_utc')\n",
    "\n",
    "merged_df.to_csv('iso_data/yearly_combined_data/final_dfs/final_ne_2023_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf95848-9998-4622-819e-166dd6bebea3",
   "metadata": {},
   "source": [
    "#### MISO, ERCOT, SPP + Respective Interconnection Queues and Wholesale Electricity Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31289ffd-3b0e-47bc-87f3-57c04c7f5159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MARKET_DAY       NODE       TYPE VALUE    HE1    HE2    HE3    HE4    HE5  \\\n",
      "0   1/1/2023       AECI  Interface   LMP  25.35  23.84  23.76  23.39  22.41   \n",
      "1   1/1/2023       AECI  Interface   MCC  -0.76  -1.11  -0.80  -1.33  -0.87   \n",
      "2   1/1/2023       AECI  Interface   MLC  -1.12  -1.20  -1.08  -1.18  -1.78   \n",
      "3   1/1/2023  AECI.ALTW   Loadzone   LMP  27.08  25.70  25.31  25.63  24.41   \n",
      "4   1/1/2023  AECI.ALTW   Loadzone   MCC  -0.41  -0.17  -0.03  -0.01   0.00   \n",
      "\n",
      "     HE6  ...  GMTIntervalEnd  Settlement Location  Pnode  LMP MLC MCC MEC  \\\n",
      "0  23.53  ...             NaN                  NaN    NaN  NaN NaN NaN NaN   \n",
      "1  -0.93  ...             NaN                  NaN    NaN  NaN NaN NaN NaN   \n",
      "2  -1.18  ...             NaN                  NaN    NaN  NaN NaN NaN NaN   \n",
      "3  25.37  ...             NaN                  NaN    NaN  NaN NaN NaN NaN   \n",
      "4   0.00  ...             NaN                  NaN    NaN  NaN NaN NaN NaN   \n",
      "\n",
      "  date source_file year_quarter  \n",
      "0  NaN         NaN          NaN  \n",
      "1  NaN         NaN          NaN  \n",
      "2  NaN         NaN          NaN  \n",
      "3  NaN         NaN          NaN  \n",
      "4  NaN         NaN          NaN  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "✅ Combined raw DataFrame shape: (158041984, 46)\n",
      "✅ Successfully saved to iso_data/yearly_combined_data/ISO_combined/combined_mw_south_2023_lmp_data.csv\n"
     ]
    }
   ],
   "source": [
    "combined_mw_south_raw_df = pd.concat([\n",
    "    miso_df,\n",
    "    ercot_df,\n",
    "    spp_df\n",
    "], ignore_index=True)\n",
    "\n",
    "print(combined_mw_south_raw_df.head())\n",
    "\n",
    "print(f\"✅ Combined raw DataFrame shape: {combined_mw_south_raw_df.shape}\")\n",
    "\n",
    "def clean_spp(df_spp):\n",
    "    df = df_spp.rename(columns={\n",
    "        'GMTIntervalEnd': 'timestamp_utc',\n",
    "        'Pnode': 'Location Name',\n",
    "    })\n",
    "    df['timestamp_utc'] = pd.to_datetime(df['timestamp_utc'])\n",
    "    df['timestamp_utc'] = df['timestamp_utc'].dt.tz_localize('UTC')\n",
    "    df['Location Type'] = 'Node'\n",
    "    df['iso'] = 'SPP'\n",
    "    return df[['timestamp_utc', 'Location Name', 'Location Type', 'iso', 'LMP', 'MCC', 'MLC']]\n",
    "\n",
    "def clean_ercot(df_ercot):\n",
    "    df = df_ercot.copy()\n",
    "\n",
    "    # Parse deliverydate to datetime\n",
    "    df['deliverydate'] = pd.to_datetime(df['deliverydate'])\n",
    "\n",
    "    # Fix 24:00 by shifting the date and setting hour to 0\n",
    "    mask_24 = df['hourending'] == '24:00'\n",
    "    df.loc[mask_24, 'deliverydate'] += pd.Timedelta(days=1)\n",
    "    df.loc[mask_24, 'hourending'] = '00:00'\n",
    "\n",
    "    # Now safely extract the hour as an integer\n",
    "    df['hour'] = df['hourending'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "    \n",
    "    # Step 1: Combine deliverydate and hourending as strings\n",
    "    datetime_str = df['deliverydate'].astype(str) + ' ' + df['hourending'].astype(str)\n",
    "\n",
    "    # Step 2: Parse the combined string into a real datetime\n",
    "    df['timestamp_utc'] = pd.to_datetime(datetime_str, format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "\n",
    "    # Step 3: Localize to UTC\n",
    "    df['timestamp_utc'] = df['timestamp_utc'].dt.tz_localize('UTC')\n",
    "\n",
    "\n",
    "    # Standardize columns\n",
    "    df['Location Name'] = df['busname']\n",
    "    df['Location Type'] = 'Node'\n",
    "    df['iso'] = 'ERCOT'\n",
    "    df['MCC'] = None\n",
    "    df['MLC'] = None\n",
    "    df = df.rename(columns={'lmp': 'LMP'})\n",
    "    \n",
    "    return df[['timestamp_utc', 'Location Name', 'Location Type', 'iso', 'LMP', 'MCC', 'MLC']]\n",
    "\n",
    "\n",
    "def reshape_miso(df_miso):\n",
    "    # Identify the hour columns (HE1, HE2, ..., HE24)\n",
    "    hour_columns = [col for col in df_miso.columns if str(col).startswith('HE')]\n",
    "\n",
    "    # Melt from wide to long\n",
    "    df_long = df_miso.melt(\n",
    "        id_vars=['MARKET_DAY', 'NODE', 'TYPE', 'VALUE'],\n",
    "        value_vars=hour_columns,\n",
    "        var_name='Hour Ending',\n",
    "        value_name='Price'\n",
    "    )\n",
    "\n",
    "    # ✅ Safe handling of 'Hour Ending'\n",
    "    df_long['Hour Ending'] = df_long['Hour Ending'].astype(str)  # Ensure it's string\n",
    "    df_long['Hour Ending'] = df_long['Hour Ending'].str.replace('HE', '', regex=False)\n",
    "    df_long = df_long[df_long['Hour Ending'].str.isnumeric()]\n",
    "    df_long['Hour Ending'] = df_long['Hour Ending'].astype(int)\n",
    "\n",
    "    # Create local timestamp (interval beginning, so subtract 1 hour)\n",
    "    df_long['Date'] = pd.to_datetime(df_long['MARKET_DAY'], format='%m/%d/%Y', errors='coerce')\n",
    "    df_long['timestamp_local'] = df_long['Date'] + pd.to_timedelta(df_long['Hour Ending'] - 1, unit='h')\n",
    "\n",
    "    # ✅ Handle DST properly\n",
    "    df_long['timestamp_utc'] = (\n",
    "        df_long['timestamp_local']\n",
    "        .dt.tz_localize('US/Central', ambiguous='NaT', nonexistent='shift_forward')\n",
    "        .dt.tz_convert('UTC')\n",
    "    )\n",
    "\n",
    "    # Rename columns to match final schema\n",
    "    df_long = df_long.rename(columns={\n",
    "        'NODE': 'Location Name',\n",
    "        'TYPE': 'Location Type',\n",
    "        'VALUE': 'Component'\n",
    "    })\n",
    "\n",
    "    df_long['iso'] = 'MISO'\n",
    "\n",
    "    return df_long[['timestamp_utc', 'Location Name', 'Location Type', 'Component', 'Price', 'iso']]\n",
    "\n",
    "def pivot_components(df_long):\n",
    "    \"\"\"\n",
    "    Pivots 'Component' rows (LMP, MCC, MLC) into separate columns,\n",
    "    keeping timestamp, location info, and ISO.\n",
    "    \"\"\"\n",
    "    df_wide = df_long.pivot_table(\n",
    "        index=['timestamp_utc', 'Location Name', 'Location Type', 'iso'],\n",
    "        columns='Component',\n",
    "        values='Price',\n",
    "        aggfunc='first'   # ✅ Critical: Avoid aggregation crash on objects\n",
    "    ).reset_index()\n",
    "\n",
    "    df_wide.columns.name = None  # Remove pivot artifacts\n",
    "    return df_wide\n",
    "\n",
    "\n",
    "\n",
    "def combine_all(df_spp, df_ercot, df_miso):\n",
    "    spp_clean = clean_spp(df_spp)\n",
    "    ercot_clean = clean_ercot(df_ercot)\n",
    "    df_miso_long = reshape_miso(df_miso)\n",
    "    miso_clean = pivot_components(df_miso_long)\n",
    "    combined = pd.concat([spp_clean, ercot_clean, miso_clean], ignore_index=True)\n",
    "    return combined\n",
    "\n",
    "combined_df = combine_all(spp_df, ercot_df, miso_df)\n",
    "\n",
    "combined_df = combined_df.sort_values('timestamp_utc').reset_index(drop=True)\n",
    "\n",
    "# 🧠 Now set timestamp_utc as index\n",
    "combined_df = combined_df.set_index('timestamp_utc')\n",
    "\n",
    "combined_df['node'] = 'node'\n",
    "combined_mw_south_df_2023 = combined_df[['iso', 'Location Name', 'Location Type', 'node', 'LMP', 'MCC', 'MLC']]\n",
    "# add column to call everything a node (but keep location type as what ISO calls it\n",
    "combined_mw_south_df_2023\n",
    "\n",
    "output_path = 'iso_data/yearly_combined_data/ISO_combined/combined_mw_south_2023_lmp_data.csv'\n",
    "combined_mw_south_df_2023.to_csv(output_path)\n",
    "\n",
    "print(f\"✅ Successfully saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12af35-1289-48be-b8f1-b4c1bf4642fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gr/lg07kybx73v06wc5f3xc5w5w0000gn/T/ipykernel_63838/359046606.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(series,\n",
      "/var/folders/gr/lg07kybx73v06wc5f3xc5w5w0000gn/T/ipykernel_63838/359046606.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(series,\n",
      "/var/folders/gr/lg07kybx73v06wc5f3xc5w5w0000gn/T/ipykernel_63838/359046606.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(series,\n",
      "/var/folders/gr/lg07kybx73v06wc5f3xc5w5w0000gn/T/ipykernel_63838/359046606.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(series,\n",
      "/var/folders/gr/lg07kybx73v06wc5f3xc5w5w0000gn/T/ipykernel_63838/359046606.py:177: DtypeWarning: Columns (5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  lmp = pd.read_csv('iso_data/yearly_combined_data/ISO_combined/combined_mw_south_2023_lmp_data.csv')\n"
     ]
    }
   ],
   "source": [
    "# add in interconnection queue data\n",
    "queues = pd.read_excel('iso_data/interconnection_data/queues_2024.xlsx', sheet_name=1)\n",
    "\n",
    "def standardize_dates(series):\n",
    "    return pd.to_datetime(series, \n",
    "                          infer_datetime_format=True,\n",
    "                          errors='coerce')\n",
    "\n",
    "queues['q_date'] = standardize_dates(queues['q_date'])\n",
    "queues['ia_date'] = standardize_dates(queues['ia_date'])\n",
    "queues['wd_date'] = standardize_dates(queues['wd_date'])\n",
    "queues['on_date'] = standardize_dates(queues['on_date'])\n",
    "\n",
    "cutoff = pd.Timestamp('2022-01-01')\n",
    "filtered_queues = queues.loc[(queues['q_date'] >= cutoff) & \n",
    "                             ((queues['ia_date'] >= cutoff)|(queues['ia_date'].isnull())) & \n",
    "                             ((queues['wd_date'] >= cutoff)|(queues['wd_date'].isnull())) &\n",
    "                             ((queues['on_date'] >= cutoff)|(queues['on_date'].isnull()))]\n",
    "\n",
    "# q_date: date when project entered queue\n",
    "# ia_date: date of signed interconnection agreement\n",
    "# wd_date: date project withdrawn from queue\n",
    "# on_date: date project became operational\n",
    "\n",
    "selected_cols = ['q_id', 'region', 'q_date', \n",
    "                 'ia_date', 'wd_date', 'on_date']\n",
    "\n",
    "selected_regions = ['PJM', 'ERCOT', 'CAISO', 'SPP', 'NYISO', 'ISO-NE']\n",
    "\n",
    "filtered_queues = filtered_queues[filtered_queues['region'].isin(selected_regions)].reset_index(drop=True)\n",
    "\n",
    "def queue_duration(df, start, end):\n",
    "    difference = df[end] - df[start]\n",
    "    return difference.dt.days\n",
    "\n",
    "filtered_queues['days_to_ia'] = queue_duration(filtered_queues, 'q_date', 'ia_date')\n",
    "filtered_queues['days_to_wd'] = queue_duration(filtered_queues, 'q_date', 'wd_date')\n",
    "filtered_queues['days_to_on'] = queue_duration(filtered_queues, 'q_date', 'on_date')\n",
    "\n",
    "as_of = pd.to_datetime('2023-12-31')\n",
    "pending = filtered_queues[['ia_date','wd_date','on_date']].isna().all(axis=1)\n",
    "\n",
    "filtered_queues['days_pending'] = np.where(\n",
    "    pending,\n",
    "    (as_of - filtered_queues['q_date']).dt.days,\n",
    "    np.nan)\n",
    "\n",
    "selected_cols = selected_cols + ['days_to_ia', 'days_to_wd', 'days_to_on', 'days_pending']\n",
    "\n",
    "# 1) build a “long” events frame: one row per event occurrence\n",
    "events = []\n",
    "for event, date_col, days_col in [\n",
    "    ('ia', 'ia_date', 'days_to_ia'),\n",
    "    ('wd', 'wd_date', 'days_to_wd'),\n",
    "    ('on', 'on_date', 'days_to_on'),\n",
    "]:\n",
    "    tmp = (\n",
    "        filtered_queues\n",
    "        .loc[filtered_queues[date_col].notna(), ['region', date_col, days_col]]\n",
    "        .rename(columns={date_col: 'date', days_col: 'days_to'})\n",
    "    )\n",
    "    tmp['event'] = event\n",
    "    events.append(tmp)\n",
    "events = pd.concat(events, ignore_index=True)\n",
    "\n",
    "events.sort_values(by=['region','date'])\n",
    "\n",
    "# 1) pivot out daily counts and daily average days_to\n",
    "daily_counts = (\n",
    "    events\n",
    "      .groupby(['region','date','event'])\n",
    "      .size()\n",
    "      .unstack('event', fill_value=0)\n",
    "      .rename(columns={'ia':'count_ia','wd':'count_wd','on':'count_on'}))\n",
    "\n",
    "daily_days = (\n",
    "    events\n",
    "      .groupby(['region','date','event'])['days_to']\n",
    "      .mean()\n",
    "      .unstack('event')\n",
    "      .rename(columns={'ia':'days_to_ia','wd':'days_to_wd','on':'days_to_on'}))\n",
    "\n",
    "daily = (\n",
    "    daily_counts\n",
    "      .join(daily_days, how='outer')\n",
    "      .sort_index())\n",
    "\n",
    "# 2) reindex to every calendar date so cumsum/expanding works\n",
    "all_dates = pd.date_range(\n",
    "    events['date'].min().floor('D'),\n",
    "    events['date'].max().ceil('D'),\n",
    "    freq='D'\n",
    ")\n",
    "idx = pd.MultiIndex.from_product(\n",
    "    [daily.index.levels[0], all_dates],\n",
    "    names=['region','date']\n",
    ")\n",
    "daily = daily.reindex(idx, fill_value=0).sort_index()\n",
    "\n",
    "# 3) cumulative sums of counts\n",
    "daily['cum_ia'] = daily.groupby(level='region')['count_ia'].cumsum()\n",
    "daily['cum_wd'] = daily.groupby(level='region')['count_wd'].cumsum()\n",
    "daily['cum_on'] = daily.groupby(level='region')['count_on'].cumsum()\n",
    "\n",
    "# 4) cumulative sums & counts of days_to, then expanding mean\n",
    "for ev in ['ia','wd','on']:\n",
    "    # running sum of days_to\n",
    "    daily[f'sum_days_to_{ev}'] = (\n",
    "        daily[f'days_to_{ev}']\n",
    "          .groupby(level='region')\n",
    "          .cumsum()\n",
    "    )\n",
    "    # running count of events (same as cum_count)\n",
    "    daily[f'cnt_days_to_{ev}'] = daily[f'count_{ev}'].groupby(level='region').cumsum()\n",
    "    # expanding average = sum / count\n",
    "    daily[f'avg_days_to_{ev}'] = (\n",
    "        daily[f'sum_days_to_{ev}'] / daily[f'cnt_days_to_{ev}'])\n",
    "\n",
    "# 5) clean up: drop intermediate columns\n",
    "final = daily.reset_index().drop(\n",
    "    columns=[f'days_to_{ev}'      for ev in ['ia','wd','on']]\n",
    "           +[f'sum_days_to_{ev}'  for ev in ['ia','wd','on']]\n",
    "           +[f'cnt_days_to_{ev}'  for ev in ['ia','wd','on']])\n",
    "\n",
    "# 0) your cutoff\n",
    "as_of = pd.to_datetime('2023-12-31')\n",
    "\n",
    "# 1) build the calendar of snapshot dates **only** through the cutoff\n",
    "all_dates = pd.date_range(\n",
    "    start=filtered_queues['q_date'].min().floor('D'),\n",
    "    end=as_of,\n",
    "    freq='D')\n",
    "\n",
    "# 2) pull in each project’s dates\n",
    "proj_raw = filtered_queues[['region','q_date','ia_date','wd_date','on_date']].copy()\n",
    "\n",
    "# 3) Cartesian-merge so each project is paired with each snapshot date\n",
    "dates_df = pd.DataFrame({'date': all_dates})\n",
    "projects = (\n",
    "    proj_raw\n",
    "      .assign(key=1)\n",
    "      .merge(dates_df.assign(key=1), on='key')\n",
    "      .drop('key', axis=1))\n",
    "\n",
    "# 4) replace NaT (i.e. “never happened by cutoff”) with far-future\n",
    "for col in ['ia_date','wd_date','on_date']:\n",
    "    projects[col] = projects[col].fillna(pd.Timestamp.max)\n",
    "\n",
    "# 5) filter to only those still pending **at** each snapshot date\n",
    "mask = (\n",
    "    (projects['date'] >= projects['q_date']) &\n",
    "    (projects['date'] <  projects['ia_date']) &\n",
    "    (projects['date'] <  projects['wd_date']) &\n",
    "    (projects['date'] <  projects['on_date']))\n",
    "pending = projects.loc[mask, ['region','date','q_date']]\n",
    "\n",
    "# 6) compute days_pending and then the daily average by region\n",
    "pending['days_pending'] = (pending['date'] - pending['q_date']).dt.days\n",
    "\n",
    "avg_pending = (\n",
    "    pending\n",
    "      .groupby(['region','date'])['days_pending']\n",
    "      .mean()\n",
    "      .reset_index(name='avg_days_pending'))\n",
    "\n",
    "# d) merge with rolled\n",
    "agg = (\n",
    "    final\n",
    "    .merge(avg_pending, on=['region','date'], how='right')\n",
    "    .sort_values(['region','date']))\n",
    "\n",
    "# final now has, for each ISO and each calendar date:\n",
    "#   - count_ia, count_wd, count_on (30-day sums)\n",
    "#   - avg_days_to_ia, avg_days_to_wd, avg_days_to_on (30-day means)\n",
    "#   - avg_days_pending (daily average for all still-pending in that snapshot)\n",
    "\n",
    "lmp = pd.read_csv('iso_data/yearly_combined_data/ISO_combined/combined_mw_south_2023_lmp_data.csv')\n",
    "\n",
    "# 1) Make sure your queue‐metrics DataFrame has a proper datetime “date” column\n",
    "agg['date'] = pd.to_datetime(agg['date']).dt.normalize()\n",
    "agg['date'] = pd.to_datetime(agg['date']).dt.tz_localize('UTC')\n",
    "\n",
    "agg = agg.rename(columns={'region':'iso'})\n",
    "agg = agg.drop(['count_ia', 'count_on', 'count_wd'], axis=1)\n",
    "\n",
    "agg[['cum_ia', 'cum_wd', 'cum_on']] = agg[['cum_ia', 'cum_wd', 'cum_on']].fillna(value=0)\n",
    "\n",
    "# 2) Prepare your LMP DataFrame\n",
    "#    – parse timestamp_utc → datetime\n",
    "#    – extract the date (drop the time component)\n",
    "lmp['date'] = pd.to_datetime(lmp['timestamp_utc']).dt.normalize()\n",
    "\n",
    "# 4) Merge the two tables on region & date\n",
    "merged = (\n",
    "    lmp\n",
    "      .merge(agg, on=['iso','date'], how='left')\n",
    ").drop(['date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d8dcaa1-81c3-4925-8e8e-c9530660445f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trade date years: [2021, 2022, 2023, 2024]\n",
      "Delivery start date years: [2021, 2022, 2023, 2024]\n",
      "Delivery end date years: [2021, 2022, 2023, 2024]\n"
     ]
    }
   ],
   "source": [
    "# add in eia wholesale data\n",
    "\n",
    "data_2022 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2022final.xlsx\")\n",
    "data_2023 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2023final.xlsx\")\n",
    "data_2024 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2024final.xlsx\")\n",
    "\n",
    "eia_pricing_data = pd.concat([data_2022, data_2023, data_2024], ignore_index=True, axis=0)\n",
    "\n",
    "hub_to_iso = {\n",
    "    'Indiana Hub RT Peak': 'MISO',\n",
    "    'Mid C Peak': 'Non-ISO (Mid-Columbia)',\n",
    "    'NP15 EZ Gen DA LMP Peak': 'CAISO',\n",
    "    'Nepool MH DA LMP Peak': 'ISO-NE',\n",
    "    'PJM WH Real Time Peak': 'PJM',\n",
    "    'Palo Verde Peak': 'CAISO',\n",
    "    'SP15 EZ Gen DA LMP Peak': 'CAISO'\n",
    "}\n",
    "\n",
    "eia_pricing_data['ISO'] = eia_pricing_data['Price hub'].map(hub_to_iso)\n",
    "\n",
    "# Convert dates to datetime\n",
    "trade_dates = pd.to_datetime(eia_pricing_data['Trade date'], format='mixed', errors='coerce')\n",
    "delivery_start_dates = pd.to_datetime(eia_pricing_data['Delivery start date'], format='mixed', errors='coerce')\n",
    "delivery_end_dates = pd.to_datetime(eia_pricing_data['Delivery \\nend date'], format='mixed', errors='coerce')\n",
    "\n",
    "# Check the unique years\n",
    "print(\"Trade date years:\", sorted(trade_dates.dropna().dt.year.unique()))\n",
    "print(\"Delivery start date years:\", sorted(delivery_start_dates.dropna().dt.year.unique()))\n",
    "print(\"Delivery end date years:\", sorted(delivery_end_dates.dropna().dt.year.unique()))\n",
    "\n",
    "eia_daily = eia_pricing_data.copy()\n",
    "\n",
    "# Convert 'Trade date' column to datetime objects \n",
    "eia_daily['Trade date'] = pd.to_datetime(eia_daily['Trade date'], format='mixed').dt.date\n",
    "\n",
    "# Create a new column for weighted price = price × volume\n",
    "eia_daily['weighted_price'] = eia_daily['Wtd avg price $/MWh'] * eia_daily['Daily volume MWh']\n",
    "\n",
    "# Group data by Trade date and ISO, and aggregate:\n",
    "eia_daily_summary = (\n",
    "    eia_daily\n",
    "    .groupby(['Trade date', 'ISO'])\n",
    "    .agg(\n",
    "        weighted_avg_price=('weighted_price', 'sum'),           # sum of (P × V)\n",
    "        total_volume=('Daily volume MWh', 'sum'),                # sum of volume\n",
    "        total_trades=('Number of trades', 'sum'),                # sum of trades\n",
    "        total_counterparties=('Number of counterparties', 'sum') # sum of counterparties\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate the volume-weighted average price\n",
    "eia_daily_summary['Wtd avg price $/MWh'] = eia_daily_summary['weighted_avg_price'] / eia_daily_summary['total_volume']\n",
    "\n",
    "# Select and reorder the final columns\n",
    "eia_daily_summary = eia_daily_summary[\n",
    "    ['Trade date', 'ISO', 'Wtd avg price $/MWh', 'total_volume', 'total_trades', 'total_counterparties']\n",
    "]\n",
    "\n",
    "# Filter to include only Trade dates from 2022, 2023, or 2024\n",
    "eia_daily_summary = eia_daily_summary[\n",
    "    eia_daily_summary['Trade date'].apply(lambda x: x.year).isin([2022, 2023, 2024])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "merged['date'] = pd.to_datetime(merged['timestamp_utc']).dt.date\n",
    "\n",
    "merged_df = merged.merge(\n",
    "    eia_daily_summary,\n",
    "    how='left',\n",
    "    left_on=['date', 'iso'],\n",
    "    right_on=['Trade date', 'ISO']\n",
    ")\n",
    "\n",
    "merged_df = merged_df.drop(columns=['Trade date', 'ISO', 'date'])\n",
    "\n",
    "merged_df.to_csv('iso_data/yearly_combined_data/final_dfs/final_mw_south_2023_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc138b-7a9c-4989-a0f7-1788a8563660",
   "metadata": {},
   "source": [
    "#### CAISO Respective Interconnection Queues and Wholesale Electricity Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "514e11ec-5788-4192-b948-2d1784ed38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "caiso = pd.read_csv('iso_data/caiso_data/caiso_dam_lmp_2022.csv')\n",
    "caiso\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "\n",
    "df_new['timestamp_utc'] = pd.to_datetime(caiso['INTERVALENDTIME_GMT'], errors='coerce')\n",
    "df_new['iso'] = 'CAISO'\n",
    "df_new['Location Name'] = caiso['NODE']\n",
    "df_new['Location Type'] = 'Node'\n",
    "df_new['node'] = 'node'\n",
    "df_new['LMP'] = caiso['MW']\n",
    "df_new['MCC'] = pd.NA  \n",
    "df_new['MLC'] = pd.NA\n",
    "\n",
    "df_new = df_new.sort_values('timestamp_utc').reset_index(drop=True)\n",
    "\n",
    "df_new = df_new.set_index('timestamp_utc')\n",
    "df_new\n",
    "\n",
    "output_path = 'iso_data/yearly_combined_data/ISO_combined/caiso_2022_lmp_data.csv'\n",
    "df_new.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9345718-53e2-44b0-a7a3-14be8ed8048a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gr/lg07kybx73v06wc5f3xc5w5w0000gn/T/ipykernel_22630/2754709886.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(series,\n",
      "/var/folders/gr/lg07kybx73v06wc5f3xc5w5w0000gn/T/ipykernel_22630/2754709886.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(series,\n",
      "/var/folders/gr/lg07kybx73v06wc5f3xc5w5w0000gn/T/ipykernel_22630/2754709886.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(series,\n",
      "/var/folders/gr/lg07kybx73v06wc5f3xc5w5w0000gn/T/ipykernel_22630/2754709886.py:5: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(series,\n"
     ]
    }
   ],
   "source": [
    "# add in interconnection queue data\n",
    "queues = pd.read_excel('iso_data/interconnection_data/queues_2024.xlsx', sheet_name=1)\n",
    "\n",
    "def standardize_dates(series):\n",
    "    return pd.to_datetime(series, \n",
    "                          infer_datetime_format=True,\n",
    "                          errors='coerce')\n",
    "\n",
    "queues['q_date'] = standardize_dates(queues['q_date'])\n",
    "queues['ia_date'] = standardize_dates(queues['ia_date'])\n",
    "queues['wd_date'] = standardize_dates(queues['wd_date'])\n",
    "queues['on_date'] = standardize_dates(queues['on_date'])\n",
    "\n",
    "cutoff = pd.Timestamp('2022-01-01')\n",
    "filtered_queues = queues.loc[(queues['q_date'] >= cutoff) & \n",
    "                             ((queues['ia_date'] >= cutoff)|(queues['ia_date'].isnull())) & \n",
    "                             ((queues['wd_date'] >= cutoff)|(queues['wd_date'].isnull())) &\n",
    "                             ((queues['on_date'] >= cutoff)|(queues['on_date'].isnull()))]\n",
    "\n",
    "# q_date: date when project entered queue\n",
    "# ia_date: date of signed interconnection agreement\n",
    "# wd_date: date project withdrawn from queue\n",
    "# on_date: date project became operational\n",
    "\n",
    "selected_cols = ['q_id', 'region', 'q_date', \n",
    "                 'ia_date', 'wd_date', 'on_date']\n",
    "\n",
    "selected_regions = ['PJM', 'ERCOT', 'CAISO', 'SPP', 'NYISO', 'ISO-NE']\n",
    "\n",
    "filtered_queues = filtered_queues[filtered_queues['region'].isin(selected_regions)].reset_index(drop=True)\n",
    "\n",
    "def queue_duration(df, start, end):\n",
    "    difference = df[end] - df[start]\n",
    "    return difference.dt.days\n",
    "\n",
    "filtered_queues['days_to_ia'] = queue_duration(filtered_queues, 'q_date', 'ia_date')\n",
    "filtered_queues['days_to_wd'] = queue_duration(filtered_queues, 'q_date', 'wd_date')\n",
    "filtered_queues['days_to_on'] = queue_duration(filtered_queues, 'q_date', 'on_date')\n",
    "\n",
    "as_of = pd.to_datetime('2023-12-31')\n",
    "pending = filtered_queues[['ia_date','wd_date','on_date']].isna().all(axis=1)\n",
    "\n",
    "filtered_queues['days_pending'] = np.where(\n",
    "    pending,\n",
    "    (as_of - filtered_queues['q_date']).dt.days,\n",
    "    np.nan)\n",
    "\n",
    "selected_cols = selected_cols + ['days_to_ia', 'days_to_wd', 'days_to_on', 'days_pending']\n",
    "\n",
    "# 1) build a “long” events frame: one row per event occurrence\n",
    "events = []\n",
    "for event, date_col, days_col in [\n",
    "    ('ia', 'ia_date', 'days_to_ia'),\n",
    "    ('wd', 'wd_date', 'days_to_wd'),\n",
    "    ('on', 'on_date', 'days_to_on'),\n",
    "]:\n",
    "    tmp = (\n",
    "        filtered_queues\n",
    "        .loc[filtered_queues[date_col].notna(), ['region', date_col, days_col]]\n",
    "        .rename(columns={date_col: 'date', days_col: 'days_to'})\n",
    "    )\n",
    "    tmp['event'] = event\n",
    "    events.append(tmp)\n",
    "events = pd.concat(events, ignore_index=True)\n",
    "\n",
    "events.sort_values(by=['region','date'])\n",
    "\n",
    "# 1) pivot out daily counts and daily average days_to\n",
    "daily_counts = (\n",
    "    events\n",
    "      .groupby(['region','date','event'])\n",
    "      .size()\n",
    "      .unstack('event', fill_value=0)\n",
    "      .rename(columns={'ia':'count_ia','wd':'count_wd','on':'count_on'}))\n",
    "\n",
    "daily_days = (\n",
    "    events\n",
    "      .groupby(['region','date','event'])['days_to']\n",
    "      .mean()\n",
    "      .unstack('event')\n",
    "      .rename(columns={'ia':'days_to_ia','wd':'days_to_wd','on':'days_to_on'}))\n",
    "\n",
    "daily = (\n",
    "    daily_counts\n",
    "      .join(daily_days, how='outer')\n",
    "      .sort_index())\n",
    "\n",
    "# 2) reindex to every calendar date so cumsum/expanding works\n",
    "all_dates = pd.date_range(\n",
    "    events['date'].min().floor('D'),\n",
    "    events['date'].max().ceil('D'),\n",
    "    freq='D'\n",
    ")\n",
    "idx = pd.MultiIndex.from_product(\n",
    "    [daily.index.levels[0], all_dates],\n",
    "    names=['region','date']\n",
    ")\n",
    "daily = daily.reindex(idx, fill_value=0).sort_index()\n",
    "\n",
    "# 3) cumulative sums of counts\n",
    "daily['cum_ia'] = daily.groupby(level='region')['count_ia'].cumsum()\n",
    "daily['cum_wd'] = daily.groupby(level='region')['count_wd'].cumsum()\n",
    "daily['cum_on'] = daily.groupby(level='region')['count_on'].cumsum()\n",
    "\n",
    "# 4) cumulative sums & counts of days_to, then expanding mean\n",
    "for ev in ['ia','wd','on']:\n",
    "    # running sum of days_to\n",
    "    daily[f'sum_days_to_{ev}'] = (\n",
    "        daily[f'days_to_{ev}']\n",
    "          .groupby(level='region')\n",
    "          .cumsum()\n",
    "    )\n",
    "    # running count of events (same as cum_count)\n",
    "    daily[f'cnt_days_to_{ev}'] = daily[f'count_{ev}'].groupby(level='region').cumsum()\n",
    "    # expanding average = sum / count\n",
    "    daily[f'avg_days_to_{ev}'] = (\n",
    "        daily[f'sum_days_to_{ev}'] / daily[f'cnt_days_to_{ev}'])\n",
    "\n",
    "# 5) clean up: drop intermediate columns\n",
    "final = daily.reset_index().drop(\n",
    "    columns=[f'days_to_{ev}'      for ev in ['ia','wd','on']]\n",
    "           +[f'sum_days_to_{ev}'  for ev in ['ia','wd','on']]\n",
    "           +[f'cnt_days_to_{ev}'  for ev in ['ia','wd','on']])\n",
    "\n",
    "# 0) your cutoff\n",
    "as_of = pd.to_datetime('2023-12-31')\n",
    "\n",
    "# 1) build the calendar of snapshot dates **only** through the cutoff\n",
    "all_dates = pd.date_range(\n",
    "    start=filtered_queues['q_date'].min().floor('D'),\n",
    "    end=as_of,\n",
    "    freq='D')\n",
    "\n",
    "# 2) pull in each project’s dates\n",
    "proj_raw = filtered_queues[['region','q_date','ia_date','wd_date','on_date']].copy()\n",
    "\n",
    "# 3) Cartesian-merge so each project is paired with each snapshot date\n",
    "dates_df = pd.DataFrame({'date': all_dates})\n",
    "projects = (\n",
    "    proj_raw\n",
    "      .assign(key=1)\n",
    "      .merge(dates_df.assign(key=1), on='key')\n",
    "      .drop('key', axis=1))\n",
    "\n",
    "# 4) replace NaT (i.e. “never happened by cutoff”) with far-future\n",
    "for col in ['ia_date','wd_date','on_date']:\n",
    "    projects[col] = projects[col].fillna(pd.Timestamp.max)\n",
    "\n",
    "# 5) filter to only those still pending **at** each snapshot date\n",
    "mask = (\n",
    "    (projects['date'] >= projects['q_date']) &\n",
    "    (projects['date'] <  projects['ia_date']) &\n",
    "    (projects['date'] <  projects['wd_date']) &\n",
    "    (projects['date'] <  projects['on_date']))\n",
    "pending = projects.loc[mask, ['region','date','q_date']]\n",
    "\n",
    "# 6) compute days_pending and then the daily average by region\n",
    "pending['days_pending'] = (pending['date'] - pending['q_date']).dt.days\n",
    "\n",
    "avg_pending = (\n",
    "    pending\n",
    "      .groupby(['region','date'])['days_pending']\n",
    "      .mean()\n",
    "      .reset_index(name='avg_days_pending'))\n",
    "\n",
    "# d) merge with rolled\n",
    "agg = (\n",
    "    final\n",
    "    .merge(avg_pending, on=['region','date'], how='right')\n",
    "    .sort_values(['region','date']))\n",
    "\n",
    "# final now has, for each ISO and each calendar date:\n",
    "#   - count_ia, count_wd, count_on (30-day sums)\n",
    "#   - avg_days_to_ia, avg_days_to_wd, avg_days_to_on (30-day means)\n",
    "#   - avg_days_pending (daily average for all still-pending in that snapshot)\n",
    "\n",
    "lmp = pd.read_csv('iso_data/yearly_combined_data/ISO_combined/caiso_2022_lmp_data.csv')\n",
    "\n",
    "# 1) Make sure your queue‐metrics DataFrame has a proper datetime “date” column\n",
    "agg['date'] = pd.to_datetime(agg['date']).dt.normalize()\n",
    "agg['date'] = pd.to_datetime(agg['date']).dt.tz_localize('UTC')\n",
    "\n",
    "agg = agg.rename(columns={'region':'iso'})\n",
    "agg = agg.drop(['count_ia', 'count_on', 'count_wd'], axis=1)\n",
    "\n",
    "# 2) Prepare your LMP DataFrame\n",
    "#    – parse timestamp_utc → datetime\n",
    "#    – extract the date (drop the time component)\n",
    "lmp['date'] = pd.to_datetime(lmp['timestamp_utc']).dt.normalize()\n",
    "\n",
    "# 4) Merge the two tables on region & date\n",
    "merged = (\n",
    "    lmp\n",
    "      .merge(agg, on=['iso','date'], how='left')\n",
    ").drop(['date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5bea8aa-af35-486f-83a0-707c35f9d6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trade date years: [2021, 2022, 2023, 2024]\n",
      "Delivery start date years: [2021, 2022, 2023, 2024]\n",
      "Delivery end date years: [2021, 2022, 2023, 2024]\n",
      "                             iso    Location Name Location Type  node  \\\n",
      "timestamp_utc                                                           \n",
      "2022-01-01 09:00:00+00:00  CAISO    0096WD_7_N001          Node  node   \n",
      "2022-01-01 09:00:00+00:00  CAISO   HURLEYS_2_N013          Node  node   \n",
      "2022-01-01 09:00:00+00:00  CAISO  ROSSMOOR_2_N101          Node  node   \n",
      "2022-01-01 09:00:00+00:00  CAISO   HURLEYS_2_N014          Node  node   \n",
      "2022-01-01 09:00:00+00:00  CAISO  ROSSMOOR_2_N001          Node  node   \n",
      "\n",
      "                                LMP  MCC  MLC  cum_ia  cum_wd  cum_on  \\\n",
      "timestamp_utc                                                           \n",
      "2022-01-01 09:00:00+00:00  58.71010  NaN  NaN     NaN     NaN     NaN   \n",
      "2022-01-01 09:00:00+00:00  60.01757  NaN  NaN     NaN     NaN     NaN   \n",
      "2022-01-01 09:00:00+00:00  60.84523  NaN  NaN     NaN     NaN     NaN   \n",
      "2022-01-01 09:00:00+00:00  60.01757  NaN  NaN     NaN     NaN     NaN   \n",
      "2022-01-01 09:00:00+00:00  60.84523  NaN  NaN     NaN     NaN     NaN   \n",
      "\n",
      "                           avg_days_to_ia  avg_days_to_wd  avg_days_to_on  \\\n",
      "timestamp_utc                                                               \n",
      "2022-01-01 09:00:00+00:00             NaN             NaN             NaN   \n",
      "2022-01-01 09:00:00+00:00             NaN             NaN             NaN   \n",
      "2022-01-01 09:00:00+00:00             NaN             NaN             NaN   \n",
      "2022-01-01 09:00:00+00:00             NaN             NaN             NaN   \n",
      "2022-01-01 09:00:00+00:00             NaN             NaN             NaN   \n",
      "\n",
      "                           avg_days_pending  Wtd avg price $/MWh  \\\n",
      "timestamp_utc                                                      \n",
      "2022-01-01 09:00:00+00:00               NaN                  NaN   \n",
      "2022-01-01 09:00:00+00:00               NaN                  NaN   \n",
      "2022-01-01 09:00:00+00:00               NaN                  NaN   \n",
      "2022-01-01 09:00:00+00:00               NaN                  NaN   \n",
      "2022-01-01 09:00:00+00:00               NaN                  NaN   \n",
      "\n",
      "                           total_volume  total_trades  total_counterparties  \n",
      "timestamp_utc                                                                \n",
      "2022-01-01 09:00:00+00:00           NaN           NaN                   NaN  \n",
      "2022-01-01 09:00:00+00:00           NaN           NaN                   NaN  \n",
      "2022-01-01 09:00:00+00:00           NaN           NaN                   NaN  \n",
      "2022-01-01 09:00:00+00:00           NaN           NaN                   NaN  \n",
      "2022-01-01 09:00:00+00:00           NaN           NaN                   NaN  \n"
     ]
    }
   ],
   "source": [
    "# add in eia wholesale data\n",
    "\n",
    "data_2022 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2022final.xlsx\")\n",
    "data_2023 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2023final.xlsx\")\n",
    "data_2024 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2024final.xlsx\")\n",
    "\n",
    "eia_pricing_data = pd.concat([data_2022, data_2023, data_2024], ignore_index=True, axis=0)\n",
    "\n",
    "hub_to_iso = {\n",
    "    'Indiana Hub RT Peak': 'MISO',\n",
    "    'Mid C Peak': 'Non-ISO (Mid-Columbia)',\n",
    "    'NP15 EZ Gen DA LMP Peak': 'CAISO',\n",
    "    'Nepool MH DA LMP Peak': 'ISO-NE',\n",
    "    'PJM WH Real Time Peak': 'PJM',\n",
    "    'Palo Verde Peak': 'CAISO',\n",
    "    'SP15 EZ Gen DA LMP Peak': 'CAISO'\n",
    "}\n",
    "\n",
    "eia_pricing_data['ISO'] = eia_pricing_data['Price hub'].map(hub_to_iso)\n",
    "\n",
    "# Convert dates to datetime\n",
    "trade_dates = pd.to_datetime(eia_pricing_data['Trade date'], format='mixed', errors='coerce')\n",
    "delivery_start_dates = pd.to_datetime(eia_pricing_data['Delivery start date'], format='mixed', errors='coerce')\n",
    "delivery_end_dates = pd.to_datetime(eia_pricing_data['Delivery \\nend date'], format='mixed', errors='coerce')\n",
    "\n",
    "# Check the unique years\n",
    "print(\"Trade date years:\", sorted(trade_dates.dropna().dt.year.unique()))\n",
    "print(\"Delivery start date years:\", sorted(delivery_start_dates.dropna().dt.year.unique()))\n",
    "print(\"Delivery end date years:\", sorted(delivery_end_dates.dropna().dt.year.unique()))\n",
    "\n",
    "eia_daily = eia_pricing_data.copy()\n",
    "\n",
    "# Convert 'Trade date' column to datetime objects \n",
    "eia_daily['Trade date'] = pd.to_datetime(eia_daily['Trade date'], format='mixed').dt.date\n",
    "\n",
    "# Create a new column for weighted price = price × volume\n",
    "eia_daily['weighted_price'] = eia_daily['Wtd avg price $/MWh'] * eia_daily['Daily volume MWh']\n",
    "\n",
    "# Group data by Trade date and ISO, and aggregate:\n",
    "eia_daily_summary = (\n",
    "    eia_daily\n",
    "    .groupby(['Trade date', 'ISO'])\n",
    "    .agg(\n",
    "        weighted_avg_price=('weighted_price', 'sum'),           # sum of (P × V)\n",
    "        total_volume=('Daily volume MWh', 'sum'),                # sum of volume\n",
    "        total_trades=('Number of trades', 'sum'),                # sum of trades\n",
    "        total_counterparties=('Number of counterparties', 'sum') # sum of counterparties\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate the volume-weighted average price\n",
    "eia_daily_summary['Wtd avg price $/MWh'] = eia_daily_summary['weighted_avg_price'] / eia_daily_summary['total_volume']\n",
    "\n",
    "# Select and reorder the final columns\n",
    "eia_daily_summary = eia_daily_summary[\n",
    "    ['Trade date', 'ISO', 'Wtd avg price $/MWh', 'total_volume', 'total_trades', 'total_counterparties']\n",
    "]\n",
    "\n",
    "# Filter to include only Trade dates from 2022, 2023, or 2024\n",
    "eia_daily_summary = eia_daily_summary[\n",
    "    eia_daily_summary['Trade date'].apply(lambda x: x.year).isin([2022, 2023, 2024])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "merged['date'] = pd.to_datetime(merged['timestamp_utc']).dt.date\n",
    "\n",
    "merged_df = merged.merge(\n",
    "    eia_daily_summary,\n",
    "    how='left',\n",
    "    left_on=['date', 'iso'],\n",
    "    right_on=['Trade date', 'ISO']\n",
    ")\n",
    "\n",
    "merged_df = merged_df.drop(columns=['Trade date', 'ISO', 'date'])\n",
    "merged_df = merged_df.set_index('timestamp_utc')\n",
    "print(merged_df.head())\n",
    "\n",
    "merged_df.to_csv('iso_data/yearly_combined_data/final_dfs/final_caiso_2022_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
