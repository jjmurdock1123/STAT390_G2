{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8124606-a4b4-4802-84e1-316f9c142c24",
   "metadata": {},
   "source": [
    "## Pipeline for Data Frame Creation and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a1a72-0448-43bd-99c5-d0461b33c375",
   "metadata": {},
   "source": [
    "### 1. Load Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9bdb223-4324-46d2-8980-165719ae3501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from io import StringIO\n",
    "import zipfile\n",
    "import shutil\n",
    "from io import BytesIO\n",
    "import base64\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "from datetime import datetime as dt\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35aa5bc7-98d5-4c03-9670-3c8264dd0551",
   "metadata": {},
   "source": [
    "### 2. Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb50d4cd-b312-4723-8e98-1678b447ebd5",
   "metadata": {},
   "source": [
    "#### ISO-NE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126e01b-1d58-4bfa-8c2a-236404141be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_isone_consolidated_lmp_by_quarter(start_date: str, end_date: str, save_path: str = 'iso_data/isone_lmp_data') -> None:\n",
    "    \"\"\"\n",
    "    Downloads ISO-NE Day-Ahead LMP CSVs for each day in the date range,\n",
    "    cleans and combines the data, and saves one CSV per quarter.\n",
    "\n",
    "    No individual daily files are written to disk.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    all_data = []\n",
    "\n",
    "    # Parse date range\n",
    "    start_dt = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_dt = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    current_dt = start_dt\n",
    "    while current_dt <= end_dt:\n",
    "        date_str = current_dt.strftime('%Y%m%d')\n",
    "        filename = f\"WW_DALMP_ISO_{date_str}.csv\"\n",
    "        url = f\"https://www.iso-ne.com/static-transform/csv/histRpts/da-lmp/{filename}\"\n",
    "\n",
    "        print(f\"ðŸ“¥ Downloading {filename}...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "\n",
    "            # Read directly from the response content\n",
    "            df = pd.read_csv(StringIO(response.text), skiprows=4, header=0)\n",
    "            df = df.iloc[1:].reset_index(drop=True)  # Remove type row\n",
    "\n",
    "            df['Fetched_Date'] = current_dt.strftime('%Y-%m-%d')\n",
    "            all_data.append(df)\n",
    "\n",
    "        except requests.exceptions.HTTPError:\n",
    "            print(f\"âŒ File not found for {date_str} â€” skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing {date_str}: {e}\")\n",
    "\n",
    "        current_dt += timedelta(days=1)\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"ðŸš« No data was downloaded.\")\n",
    "        return\n",
    "\n",
    "    # Combine all data\n",
    "    full_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Parse the actual 'Date' column\n",
    "    full_df['Date'] = pd.to_datetime(full_df['Date'], errors='coerce')\n",
    "    full_df = full_df.dropna(subset=['Date'])\n",
    "\n",
    "    # Assign each row to a calendar quarter\n",
    "    full_df['Quarter'] = full_df['Date'].dt.to_period('Q')\n",
    "\n",
    "    # Save one CSV per quarter\n",
    "    for quarter, group in full_df.groupby('Quarter'):\n",
    "        quarter_str = str(quarter).replace('/', '')  # e.g., '2024Q1'\n",
    "        output_filename = f\"isone_lmp_{quarter_str}.csv\"\n",
    "        output_path = os.path.join(save_path, output_filename)\n",
    "\n",
    "        group.drop(columns='Quarter').to_csv(output_path, index=False)\n",
    "        print(f\"âœ… Saved {output_filename} with {len(group)} records.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_isone_consolidated_lmp_by_quarter(\"2022-01-01\", \"2024-12-31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6eba8-e46e-49be-a3ab-ec34f610f792",
   "metadata": {},
   "source": [
    "#### NYISO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15edae1-ad85-4d77-bb17-6aaa4c39e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYISO Zonal Data\n",
    "def download_nyiso_zone_lmp_monthly_by_quarter(start_year: int, end_year: int, save_path='iso_data/nyiso_data/nyiso_zonal_lmp_data'):\n",
    "    \"\"\"\n",
    "    Downloads NYISO monthly ZONAL LBMP ZIPs, extracts daily CSVs, aggregates by quarter,\n",
    "    saves quarterly CSVs, and deletes the extracted folder afterward.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    temp_extract_path = os.path.join(save_path, 'extracted')\n",
    "    os.makedirs(temp_extract_path, exist_ok=True)\n",
    "    all_data = []\n",
    "\n",
    "    # Generate list of months\n",
    "    months = pd.date_range(start=f'{start_year}-01-01', end=f'{end_year}-12-31', freq='MS')\n",
    "\n",
    "    for dt in months:\n",
    "        yyyymmdd = dt.strftime('%Y%m01')\n",
    "        zip_filename = f\"{yyyymmdd}damlbmp_zone_csv.zip\"  # â¬…ï¸ _zone_ here!\n",
    "        url = f\"https://mis.nyiso.com/public/csv/damlbmp/{zip_filename}\"\n",
    "\n",
    "        print(f\"â¬‡ï¸ Downloading {zip_filename}...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"âŒ File not found for {dt.strftime('%B %Y')} â€” skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Extract ZIP in memory\n",
    "            with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "                z.extractall(temp_extract_path)\n",
    "            print(f\"âœ… Extracted {zip_filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing {zip_filename}: {e}\")\n",
    "\n",
    "    # Collect all extracted CSVs\n",
    "    all_files = [os.path.join(temp_extract_path, f) for f in os.listdir(temp_extract_path) if f.endswith('.csv')]\n",
    "    print(f\"ðŸ“‚ Found {len(all_files)} daily CSVs to process.\")\n",
    "\n",
    "    for file in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Skipping {file}: {e}\")\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No data was processed.\")\n",
    "        return\n",
    "\n",
    "    # Combine\n",
    "    full_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Detect timestamp column\n",
    "    timestamp_col = next((col for col in full_df.columns if 'Time' in col or 'time' in col.lower()), None)\n",
    "    if not timestamp_col:\n",
    "        raise ValueError(\"Couldn't find a timestamp column in the combined CSVs.\")\n",
    "\n",
    "    full_df['Date'] = pd.to_datetime(full_df[timestamp_col], errors='coerce')\n",
    "    full_df = full_df.dropna(subset=['Date'])\n",
    "\n",
    "    # Assign calendar quarter\n",
    "    full_df['Quarter'] = full_df['Date'].dt.to_period('Q')\n",
    "\n",
    "    # Save one CSV per quarter\n",
    "    for quarter, group in full_df.groupby('Quarter'):\n",
    "        quarter_str = str(quarter).replace('/', '')\n",
    "        output_path = os.path.join(save_path, f'nyiso_zonal_lmp_{quarter_str}.csv')\n",
    "        group.drop(columns='Quarter').to_csv(output_path, index=False)\n",
    "        print(f\"âœ… Saved {output_path} with {len(group)} rows.\")\n",
    "\n",
    "    # âœ… Clean up extracted folder\n",
    "    print(f\"ðŸ§¹ Removing extracted folder: {temp_extract_path}\")\n",
    "    shutil.rmtree(temp_extract_path)\n",
    "    print(\"âœ… Cleanup complete.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    download_nyiso_zone_lmp_monthly_by_quarter(start_year=2022, end_year=2024)\n",
    "\n",
    "# NYISO Generator Data\n",
    "def download_nyiso_gen_lmp_monthly_by_quarter(start_year: int, end_year: int, save_path='iso_data/nyiso_data/nyiso_gen_lmp_data'):\n",
    "    \"\"\"\n",
    "    Downloads NYISO monthly generator LBMP ZIPs, extracts daily CSVs, aggregates by quarter,\n",
    "    saves quarterly CSVs, and deletes the extracted folder afterward.\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    temp_extract_path = os.path.join(save_path, 'extracted')\n",
    "    os.makedirs(temp_extract_path, exist_ok=True)\n",
    "    all_data = []\n",
    "\n",
    "    # Generate list of months\n",
    "    months = pd.date_range(start=f'{start_year}-01-01', end=f'{end_year}-12-31', freq='MS')\n",
    "\n",
    "    for dt in months:\n",
    "        yyyymmdd = dt.strftime('%Y%m01')\n",
    "        zip_filename = f\"{yyyymmdd}damlbmp_gen_csv.zip\"\n",
    "        url = f\"https://mis.nyiso.com/public/csv/damlbmp/{zip_filename}\"\n",
    "\n",
    "        print(f\"â¬‡ï¸ Downloading {zip_filename}...\")\n",
    "\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"âŒ File not found for {dt.strftime('%B %Y')} â€” skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Extract ZIP in memory\n",
    "            with zipfile.ZipFile(BytesIO(response.content)) as z:\n",
    "                z.extractall(temp_extract_path)\n",
    "            print(f\"âœ… Extracted {zip_filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Error processing {zip_filename}: {e}\")\n",
    "\n",
    "    # Collect all extracted CSVs\n",
    "    all_files = [os.path.join(temp_extract_path, f) for f in os.listdir(temp_extract_path) if f.endswith('.csv')]\n",
    "    print(f\"ðŸ“‚ Found {len(all_files)} daily CSVs to process.\")\n",
    "\n",
    "    for file in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            all_data.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Skipping {file}: {e}\")\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No data was processed.\")\n",
    "        return\n",
    "\n",
    "    # Combine\n",
    "    full_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Detect timestamp column\n",
    "    timestamp_col = next((col for col in full_df.columns if 'Time' in col or 'time' in col.lower()), None)\n",
    "    if not timestamp_col:\n",
    "        raise ValueError(\"Couldn't find a timestamp column in the combined CSVs.\")\n",
    "\n",
    "    full_df['Date'] = pd.to_datetime(full_df[timestamp_col], errors='coerce')\n",
    "    full_df = full_df.dropna(subset=['Date'])\n",
    "\n",
    "    # Assign calendar quarter\n",
    "    full_df['Quarter'] = full_df['Date'].dt.to_period('Q')\n",
    "\n",
    "    # Save one CSV per quarter\n",
    "    for quarter, group in full_df.groupby('Quarter'):\n",
    "        quarter_str = str(quarter).replace('/', '')\n",
    "        output_path = os.path.join(save_path, f'nyiso_lmp_{quarter_str}.csv')\n",
    "        group.drop(columns='Quarter').to_csv(output_path, index=False)\n",
    "        print(f\"âœ… Saved {output_path} with {len(group)} rows.\")\n",
    "\n",
    "    # âœ… Clean up extracted folder\n",
    "    print(f\"ðŸ§¹ Removing extracted folder: {temp_extract_path}\")\n",
    "    shutil.rmtree(temp_extract_path)\n",
    "    print(\"âœ… Cleanup complete.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    download_nyiso_gen_lmp_monthly_by_quarter(start_year=2022, end_year=2023)\n",
    "\n",
    "# combine zonal and generator data\n",
    "\n",
    "# Paths to your folders\n",
    "nodal_dir = 'iso_data/nyiso_data/nyiso_gen_lmp_data'\n",
    "zonal_dir = 'iso_data/nyiso_data/nyiso_zonal_lmp_data'\n",
    "output_dir = 'iso_data/nyiso_data/nyiso_combined_quarters'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Get all quarter identifiers from filenames (e.g., '2022Q1')\n",
    "nodal_quarters = [f.replace('nyiso_lmp_', '').replace('.csv', '') for f in os.listdir(nodal_dir) if f.endswith('.csv')]\n",
    "zonal_quarters = [f.replace('nyiso_zonal_lmp_', '').replace('.csv', '') for f in os.listdir(zonal_dir) if f.endswith('.csv')]\n",
    "\n",
    "# Get common quarters that exist in both folders\n",
    "all_quarters = sorted(set(nodal_quarters) & set(zonal_quarters))\n",
    "\n",
    "# Process and merge each quarter\n",
    "for quarter in all_quarters:\n",
    "    nodal_file = os.path.join(nodal_dir, f'nyiso_lmp_{quarter}.csv')\n",
    "    zonal_file = os.path.join(zonal_dir, f'nyiso_zonal_lmp_{quarter}.csv')\n",
    "    \n",
    "    # Read files and tag node_type\n",
    "    nodal_df = pd.read_csv(nodal_file)\n",
    "    nodal_df['node_type'] = 'nodal'\n",
    "    \n",
    "    zonal_df = pd.read_csv(zonal_file)\n",
    "    zonal_df['node_type'] = 'zonal'\n",
    "    \n",
    "    # Combine and save\n",
    "    combined_df = pd.concat([nodal_df, zonal_df], ignore_index=True)\n",
    "    output_path = os.path.join(output_dir, f'nyiso_combined_{quarter}.csv')\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    \n",
    "    print(f\"âœ… Combined and saved {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52640f50-77a2-48a2-9ba3-1c815d482d69",
   "metadata": {},
   "source": [
    "#### PJM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a258c-90cd-4256-9522-19e8eac165c4",
   "metadata": {},
   "source": [
    "#### MISO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9491b1-6919-4359-adec-cd8f6cd9529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for 2023 to present data extraction\n",
    "def download_and_extract_zip(url):\n",
    "    \"\"\"\n",
    "    Downloads and extracts all CSVs from a MISO ZIP archive.\n",
    "    Returns a list of DataFrames.\n",
    "    \"\"\"\n",
    "    print(f\"  Trying: {url}\")\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        return []\n",
    "\n",
    "    if response.content[:2] != b'PK':  # ZIP files start with 'PK'\n",
    "        return []\n",
    "\n",
    "    dfs = []\n",
    "    try:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            for filename in z.namelist():\n",
    "                if filename.endswith(\".csv\"):\n",
    "                    print(f\"    Extracting: {filename}\")\n",
    "                    with z.open(filename) as f:\n",
    "                        try:\n",
    "                            df = pd.read_csv(f, low_memory=False)\n",
    "                            df[\"SOURCE_FILE\"] = filename\n",
    "                            dfs.append(df)\n",
    "                        except Exception as e:\n",
    "                            print(f\"    Failed to parse {filename}: {e}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"    Error: File at {url} is not a valid ZIP archive.\")\n",
    "    return dfs\n",
    "\n",
    "def scrape_miso_quarterly_zips(year, quarters, output_dir=\"iso_data/miso_data\"):\n",
    "    \"\"\"\n",
    "    Downloads and processes MISO LMP ZIP files for given year and quarters,\n",
    "    handling naming inconsistencies and saving each quarter separately.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    quarter_map_variants = {\n",
    "        \"Q1\": [\"Jan-Mar\", \"Jan_Mar\"],\n",
    "        \"Q2\": [\"Apr-Jun\", \"Apr_Jun\"],\n",
    "        \"Q3\": [\"Jul-Sep\", \"Jul_Sep\"],\n",
    "        \"Q4\": [\"Oct-Dec\", \"Oct_Dec\"]\n",
    "    }\n",
    "    suffixes = [\"DA_LMPs.zip\", \"DA_LMP.zip\"]\n",
    "    year_sep_variants = [\"_\", \"-\"]\n",
    "\n",
    "    for q in quarters:\n",
    "        print(f\"\\nðŸ“¦ Processing {year} {q}\")\n",
    "        success = False\n",
    "        for quarter_str in quarter_map_variants[q]:\n",
    "             for sep in year_sep_variants:\n",
    "                for suffix in suffixes:\n",
    "                    filename = f\"{year}{sep}{quarter_str}_{suffix}\"\n",
    "                    url = f\"https://docs.misoenergy.org/marketreports/{filename}\"\n",
    "\n",
    "                    dfs = download_and_extract_zip(url)\n",
    "                    if dfs:\n",
    "                        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "                        out_path = os.path.join(output_dir, f\"{year}_{q}.csv\")\n",
    "                        combined_df.to_csv(out_path, index=False)\n",
    "                        print(f\"âœ… Saved to {out_path}\")\n",
    "                        success = True\n",
    "                        break  # stop after the first successful variant\n",
    "                if success:\n",
    "                    break\n",
    "                if not success:\n",
    "                    print(f\"âš ï¸ No valid file found for {year} {q}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    scrape_miso_quarterly_zips(year=2025, quarters=[\"Q1\", \"Q2\", \"Q3\", \"Q4\"])\n",
    "\n",
    "# 2022 and earlier\n",
    "def extract_and_save_quarter_from_nested_zip(nested_zip_bytes, nested_filename, year, output_dir):\n",
    "    \"\"\"\n",
    "    Extracts CSVs from a nested ZIP archive and saves them as a quarterly file\n",
    "    based on the name of the nested ZIP.\n",
    "    \"\"\"\n",
    "    # Identify quarter from filename\n",
    "    quarter_hint = \"\"\n",
    "    if any(m in nested_filename for m in [\"Jan\", \"Feb\", \"Mar\"]):\n",
    "        quarter_hint = \"Q1\"\n",
    "    elif any(m in nested_filename for m in [\"Apr\", \"May\", \"Jun\"]):\n",
    "        quarter_hint = \"Q2\"\n",
    "    elif any(m in nested_filename for m in [\"Jul\", \"Aug\", \"Sep\"]):\n",
    "        quarter_hint = \"Q3\"\n",
    "    elif any(m in nested_filename for m in [\"Oct\", \"Nov\", \"Dec\"]):\n",
    "        quarter_hint = \"Q4\"\n",
    "    else:\n",
    "        print(f\"âš ï¸ Could not identify quarter from: {nested_filename}\")\n",
    "        return\n",
    "\n",
    "    # Extract CSVs from nested ZIP\n",
    "    dfs = []\n",
    "    try:\n",
    "        with zipfile.ZipFile(io.BytesIO(nested_zip_bytes)) as nested_zip:\n",
    "            for filename in nested_zip.namelist():\n",
    "                if filename.endswith(\".csv\"):\n",
    "                    print(f\"    ðŸ“„ Extracting CSV: {filename}\")\n",
    "                    with nested_zip.open(filename) as f:\n",
    "                        try:\n",
    "                            df = pd.read_csv(f, low_memory=False)\n",
    "                            df[\"SOURCE_FILE\"] = filename\n",
    "                            dfs.append(df)\n",
    "                        except Exception as e:\n",
    "                            print(f\"      âš ï¸ Failed to read {filename}: {e}\")\n",
    "    except zipfile.BadZipFile:\n",
    "        print(f\"âš ï¸ Invalid nested ZIP: {nested_filename}\")\n",
    "        return\n",
    "\n",
    "    # Save only that quarterâ€™s data\n",
    "    if dfs:\n",
    "        combined = pd.concat(dfs, ignore_index=True)\n",
    "        out_path = os.path.join(output_dir, f\"{year}_{quarter_hint}.csv\")\n",
    "        combined.to_csv(out_path, index=False)\n",
    "        print(f\"âœ… Saved quarterly CSV: {out_path}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ No CSVs extracted from: {nested_filename}\")\n",
    "\n",
    "def download_and_process_annual_zip(year, output_dir=\"miso_data\"):\n",
    "    \"\"\"\n",
    "    Downloads a yearly MISO ZIP (which contains nested ZIPs by month/quarter),\n",
    "    extracts quarterly data, and saves each quarter separately.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    filename = f\"{year}01_DA_LMPs_zip.zip\"\n",
    "    url = f\"https://docs.misoenergy.org/marketreports/{filename}\"\n",
    "    print(f\"\\nðŸ“¦ Processing annual archive for {year}: {url}\")\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200 or response.content[:2] != b'PK':\n",
    "            print(\"âŒ Failed to download or invalid ZIP format.\")\n",
    "            return\n",
    "\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as outer_zip:\n",
    "            for nested_name in outer_zip.namelist():\n",
    "                if nested_name.endswith(\".zip\"):\n",
    "                    print(f\"  ðŸ“¦ Found nested ZIP: {nested_name}\")\n",
    "                    with outer_zip.open(nested_name) as nested_file:\n",
    "                        nested_bytes = nested_file.read()\n",
    "                        extract_and_save_quarter_from_nested_zip(\n",
    "                            nested_zip_bytes=nested_bytes,\n",
    "                            nested_filename=nested_name,\n",
    "                            year=year,\n",
    "                            output_dir=output_dir\n",
    "                        )\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Error processing archive: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    download_and_process_annual_zip(2022, output_dir=\"iso_data/miso_data\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d814419f-5a69-4993-a47a-e9e74695305a",
   "metadata": {},
   "source": [
    "#### ERCOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74058be9-e792-4fac-87b0-ed06bd0cc1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_url = \"https://ercotb2c.b2clogin.com/ercotb2c.onmicrosoft.com/B2C_1_PUBAPI-ROPC-FLOW/oauth2/v2.0/token\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "}\n",
    "\n",
    "token_data = {\n",
    "    \"grant_type\": \"password\",\n",
    "    \"scope\": \"openid fec253ea-0d06-4272-a5e6-b478baeecd70 offline_access\",\n",
    "    \"client_id\": \"fec253ea-0d06-4272-a5e6-b478baeecd70\",\n",
    "    \"username\": \"alanwang2025@u.northwestern.edu\",\n",
    "    \"password\": \"Fork102$\"\n",
    "}\n",
    "\n",
    "response = requests.post(token_url, data=token_data, headers=headers)\n",
    "response.raise_for_status()\n",
    "access_token = response.json()[\"access_token\"]\n",
    "print(\"âœ… Token received.\")\n",
    "\n",
    "token_parts = access_token.split(\".\")\n",
    "payload = token_parts[1] + '=' * (-len(token_parts[1]) % 4)  # fix padding\n",
    "decoded = json.loads(base64.urlsafe_b64decode(payload.encode()).decode())\n",
    "print(json.dumps(decoded, indent=2))\n",
    "\n",
    "access_token\n",
    "\n",
    "# Replace with your real token and subscription key\n",
    "subscription_key = \"7076e411aeeb461e8bb085df1690f0cd\"\n",
    "\n",
    "\n",
    "# Headers for API access\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {access_token}\",\n",
    "    \"Ocp-Apim-Subscription-Key\": subscription_key\n",
    "}\n",
    "\n",
    "# Step 1: Fetch archive metadata\n",
    "print(\"ðŸ“¦ Fetching ERCOT DAM LMP archive list...\")\n",
    "endpoint = \"https://api.ercot.com/api/public-reports/archive/np4-183-cd\"\n",
    "response = requests.get(endpoint, headers=headers)\n",
    "response.raise_for_status()\n",
    "archives = response.json().get(\"archives\", [])\n",
    "print(f\"âœ… Found {len(archives)} archive entries.\")\n",
    "\n",
    "# âœ… Filter for only archives from 2022 onwards\n",
    "archives = [\n",
    "    archive for archive in archives\n",
    "    if \"postDatetime\" in archive and\n",
    "       dt.fromisoformat(archive[\"postDatetime\"][:19]) >= dt(2022, 1, 1) and\n",
    "       dt.fromisoformat(archive[\"postDatetime\"][:19]) <= dt(2022, 10, 1)\n",
    "]\n",
    "\n",
    "print(f\"âœ… Found {len(archives)} archive entries from 2022 onward.\")\n",
    "\n",
    "# Output dir\n",
    "output_dir = \"ercot_dam_outputs\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Step 2: Download and parse\n",
    "for i, archive in enumerate(archives):\n",
    "    download_url = f\"https://api.ercot.com/api/public-reports/archive/np4-183-cd?download={archive['docId']}\"\n",
    "    print(f\"\\nâ¬‡ï¸ Downloading ({i+1}): {archive['friendlyName']}\")\n",
    "    \n",
    "    for _ in range(3):\n",
    "        r = requests.get(download_url, headers=headers)\n",
    "        if r.status_code == 429:\n",
    "            print(\"â³ Rate limit hit. Waiting...\")\n",
    "            time.sleep(2)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        print(f\"âš ï¸ Failed to download. Status: {r.status_code}\")\n",
    "        continue\n",
    "\n",
    "    content_type = r.headers.get(\"Content-Type\", \"\")\n",
    "    raw = r.content\n",
    "\n",
    "    try:\n",
    "        if raw.startswith(b'PK'):  # ZIP magic number\n",
    "            print(\"ðŸ“¦ Detected ZIP archive\")\n",
    "            with ZipFile(BytesIO(raw)) as z:\n",
    "                for file_info in z.infolist():\n",
    "                    if file_info.filename.endswith(\".csv\"):\n",
    "                        with z.open(file_info) as f:\n",
    "                            df = pd.read_csv(f, engine='python', on_bad_lines='skip')\n",
    "        else:\n",
    "            print(\"ðŸ§¾ Detected raw CSV file\")\n",
    "            decoded = raw.decode(\"utf-8\", errors=\"ignore\")\n",
    "            df = pd.read_csv(StringIO(decoded), engine='python', on_bad_lines='skip')\n",
    "\n",
    "        # Normalize column names\n",
    "        df.columns = df.columns.str.strip().str.lower().str.replace(\" \", \"\")\n",
    "        print(\"ðŸ“‘ Columns:\", df.columns.tolist())\n",
    "\n",
    "        # Accept 'deliverydate' or fallback options\n",
    "        date_col = None\n",
    "        for col in df.columns:\n",
    "            if col in [\"deliverydate\", \"delivery_date\", \"delvdate\"]:\n",
    "                date_col = col\n",
    "                break\n",
    "\n",
    "        if not date_col:\n",
    "            print(f\"âŒ No recognized delivery date column in: {archive['friendlyName']}\")\n",
    "            continue\n",
    "\n",
    "        df[date_col] = pd.to_datetime(df[date_col], errors=\"coerce\")\n",
    "        df[\"quarter\"] = df[date_col].dt.to_period(\"Q\").astype(str)\n",
    "\n",
    "        for quarter, group in df.groupby(\"quarter\"):\n",
    "            out_path = os.path.join(output_dir, f\"ERCOT_LMP_{quarter}.csv\")\n",
    "            if os.path.exists(out_path):\n",
    "                group.to_csv(out_path, mode='a', header=False, index=False)\n",
    "            else:\n",
    "                group.to_csv(out_path, index=False)\n",
    "        \n",
    "        print(f\"âœ… Saved: {archive['friendlyName']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error parsing {archive['friendlyName']}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05fffa8-173c-4231-bba2-8263839d4615",
   "metadata": {},
   "source": [
    "#### SPP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747e53d-41a7-4ae0-950e-cd7035195062",
   "metadata": {},
   "source": [
    "#### CAISO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa32bee-f2c2-4e38-ad96-711c3489bdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_day(date, nodes=None):\n",
    "    \"\"\"Download and extract DAM_LMP CSV for a single day\"\"\"\n",
    "    base_url = \"http://oasis.caiso.com/oasisapi/SingleZip\"\n",
    "    date_str = date.strftime(\"%Y%m%d\")\n",
    "    \n",
    "    params = {\n",
    "        \"resultformat\": \"6\",\n",
    "        \"queryname\": \"PRC_LMP\",\n",
    "        \"version\": \"12\",\n",
    "        \"market_run_id\": \"DAM\",\n",
    "        \"startdatetime\": f\"{date_str}T08:00-0000\",\n",
    "        \"enddatetime\": f\"{date_str}T08:00-0000\",\n",
    "        \"grp_type\": \"ALL\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, timeout=60)\n",
    "        response.raise_for_status()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Download error for {date_str}: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "            for name in z.namelist():\n",
    "                if \"DAM_LMP\" in name and name.endswith(\".csv\"):\n",
    "                    with z.open(name) as f:\n",
    "                        df = pd.read_csv(f)\n",
    "                        if nodes is not None:\n",
    "                            df = df[df[\"NODE\"].isin(nodes)]\n",
    "                        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ZIP processing error for {date_str}: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_caiso_dam_lmp_parallel(start_date, end_date, nodes=None, max_workers=8):\n",
    "    \"\"\"Parallel downloader for CAISO DAM_LMP data\"\"\"\n",
    "    start_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "    date_list = [start_dt + timedelta(days=i) for i in range((end_dt - start_dt).days + 1)]\n",
    "\n",
    "    all_data = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(download_day, date, nodes): date for date in date_list}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"CAISO Parallel\"):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                all_data.append(result)\n",
    "\n",
    "    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    save_folder = \"iso_data/caiso_data\"  # âœ… Your desired folder path\n",
    "    os.makedirs(save_folder, exist_ok=True)   # âœ… Create it if it doesn't exist\n",
    "\n",
    "    df = fetch_caiso_dam_lmp_parallel(\n",
    "        start_date=\"2023-01-01\",\n",
    "        end_date=\"2024-01-01\",\n",
    "        max_workers=6\n",
    "    )\n",
    "\n",
    "    if not df.empty:\n",
    "        save_path = os.path.join(save_folder, \"caiso_dam_lmp_parallel.csv\")\n",
    "        df.to_csv(save_path, index=False)\n",
    "        print(f\"âœ… Saved to {save_path}\")\n",
    "    else:\n",
    "        print(\"ðŸš« No data downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dedff3d-5ea6-4961-b0e5-135cd76a8e02",
   "metadata": {},
   "source": [
    "#### Interconnection Queues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16ca7c-fccd-4e18-9012-ef30a88a2eea",
   "metadata": {},
   "source": [
    "#### EIA Wholesale Electricity Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65db058-a79a-4d89-8cb2-b9b7e184c171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base URL and the list of file names\n",
    "base_url = \"https://www.eia.gov/electricity/wholesale/xls/archive/\"\n",
    "file_names = [\n",
    "    \"ice_electric-2022final.xlsx\",\n",
    "    \"ice_electric-2023final.xlsx\",\n",
    "    \"ice_electric-2024final.xlsx\"\n",
    "]\n",
    "\n",
    "# Directory where the files will be saved\n",
    "save_dir = 'iso_data/Wholesale_Pricing_Data'\n",
    "\n",
    "# Loop to download each file\n",
    "for file_name in file_names:\n",
    "    full_url = base_url + file_name\n",
    "    response = requests.get(full_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        file_path = os.path.join(save_dir, file_name)\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded: {file_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to download: {file_name} (Status code: {response.status_code})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fe8cab-a2f3-44d4-b6df-49480ceea4fc",
   "metadata": {},
   "source": [
    "### 3. Organize Each Data Source Into Yearly Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9f8084-defe-4ed5-a811-108b0e8e947b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_quarters(folder, year=None):\n",
    "    \"\"\"\n",
    "    Combines quarterly CSVs in a folder.\n",
    "    If 'year' is provided, only combines files containing that year in the filename.\n",
    "    \"\"\"\n",
    "    files = sorted([f for f in os.listdir(folder) if f.endswith('.csv')])\n",
    "\n",
    "    if year is not None:\n",
    "        files = [f for f in files if str(year) in f]\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for f in files:\n",
    "        path = os.path.join(folder, f)\n",
    "        try:\n",
    "            df = pd.read_csv(path, low_memory=False)\n",
    "            dfs.append(df)\n",
    "            print(f\"âœ… Loaded {f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to load {f}: {e}\")\n",
    "\n",
    "    if not dfs:\n",
    "        print(f\"âš ï¸ No files found for year {year} in {folder}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"ðŸ§© Combined {len(dfs)} files with {len(combined):,} rows for year {year}.\")\n",
    "    return combined\n",
    "\n",
    "# Combine only MISO yearly files\n",
    "miso_2023_df = combine_quarters('iso_data/miso_data', year=2023)\n",
    "\n",
    "# Combine only ISO-NE yearly files\n",
    "isone_2023_df = combine_quarters('iso_data/isone_lmp_data', year=2023)\n",
    "\n",
    "# Combine only NYISO yearly files\n",
    "nyo_2023_df = combine_quarters('iso_data/nyiso_data/nyiso_combined_quarters', year=2023)\n",
    "\n",
    "# Combine only ERCOT yearly files\n",
    "ercot_2023_df = combine_quarters('iso_data/ercot_dam_outputs', year=2023)\n",
    "\n",
    "# Combine only SPP yearly files\n",
    "spp_2023_df = combine_quarters('iso_data/spp_lmp_quarters', year=2023)\n",
    "\n",
    "# Combine only PJM 2023 files\n",
    "pjm_2023_df = combine_quarters('iso_data/pjm_data', year=2023)\n",
    "\n",
    "# CAISO already downloaded into yearly format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d986a72-e3a7-4f7a-8d4e-d24b776e82ef",
   "metadata": {},
   "source": [
    "### 4. Combine Data Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c68fc1-c49d-40fa-b99e-d040214d0894",
   "metadata": {},
   "source": [
    "#### ISO-NE, NYISO, PJM + Respective Interconnection Queues and Wholesale Electricity Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3442e86d-ccc1-4ac5-84a8-c14e905a397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine NE/Mid-atlantic ISO dataframes\n",
    "combined_ne_raw_df = pd.concat([\n",
    "    nyo_2023_df,\n",
    "    isone_2023_df, \n",
    "    pjm_2023_df\n",
    "], ignore_index=True)\n",
    "\n",
    "print(combined_ne_raw_df.head())\n",
    "\n",
    "print(f\"âœ… Combined raw DataFrame shape: {combined_ne_raw_df.shape}\")\n",
    "\n",
    "def clean_pjm(pjm_df):\n",
    "    \"\"\"Clean PJM dataframe to standard format.\"\"\"\n",
    "    pjm_df = pjm_df.copy()\n",
    "    pjm_df['timestamp_utc'] = pd.to_datetime(pjm_df['datetime_beginning_utc'], utc = True)\n",
    "    pjm_df['iso'] = 'PJM'\n",
    "    pjm_df['Location Name'] = pjm_df['pnode_name']\n",
    "    pjm_df['Location Type'] = 'Node'\n",
    "    pjm_df['LMP'] = pjm_df['total_lmp_da']  # or 'total_lmp_rt' if you prefer real-time\n",
    "    pjm_df['MCC'] = pjm_df['congestion_price_da']\n",
    "    pjm_df['MLC'] = pjm_df['marginal_loss_price_da']\n",
    "    \n",
    "    return pjm_df[['timestamp_utc', 'iso', 'Location Name', 'Location Type', 'LMP', 'MCC', 'MLC']]\n",
    "\n",
    "def clean_nyiso(nyiso_df):\n",
    "    \"\"\"Clean NYISO dataframe to standard format.\"\"\"\n",
    "    nyiso_df = nyiso_df.copy()\n",
    "    nyiso_df['timestamp_utc'] = pd.to_datetime(nyiso_df['Date'], utc = True)\n",
    "    nyiso_df['iso'] = 'NYISO'\n",
    "    nyiso_df['Location Name'] = nyiso_df['Name']\n",
    "    nyiso_df['Location Type'] = 'Node'\n",
    "    nyiso_df['LMP'] = nyiso_df['LBMP ($/MWHr)'] \n",
    "    nyiso_df['MCC'] = nyiso_df['Marginal Cost Congestion ($/MWHr)']\n",
    "    nyiso_df['MLC'] = nyiso_df['Marginal Cost Losses ($/MWHr)']\n",
    "    \n",
    "    return nyiso_df[['timestamp_utc', 'iso', 'Location Name', 'Location Type', 'LMP', 'MCC', 'MLC']]\n",
    "\n",
    "def clean_isone(isone_df):\n",
    "    \"\"\"Clean ISO-NE dataframe to standard format.\"\"\"\n",
    "    isone_df = isone_df.copy()\n",
    "\n",
    "    # Safe handling of 'Hour Ending'\n",
    "    isone_df['Hour Ending'] = isone_df['Hour Ending'].astype(str)\n",
    "    isone_df = isone_df[isone_df['Hour Ending'].str.isnumeric()]\n",
    "    isone_df['Hour Ending'] = isone_df['Hour Ending'].astype(int)\n",
    "    isone_df['timestamp_utc'] = pd.to_datetime(isone_df['Date']) + pd.to_timedelta(isone_df['Hour Ending'] - 1, unit='h')\n",
    "    isone_df['timestamp_utc'] = pd.to_datetime(isone_df['timestamp_utc'], utc=True)\n",
    "\n",
    "    isone_df['iso'] = 'ISO-NE'\n",
    "    isone_df['Location Name'] = isone_df['Location Name']\n",
    "    isone_df['Location Type'] = 'Node'  # Simplify network node to Node\n",
    "    isone_df['LMP'] = isone_df['Locational Marginal Price']\n",
    "    isone_df['MCC'] = isone_df['Congestion Component']\n",
    "    isone_df['MLC'] = isone_df['Marginal Loss Component']\n",
    "    \n",
    "    return isone_df[['timestamp_utc', 'iso', 'Location Name', 'Location Type', 'LMP', 'MCC', 'MLC']]\n",
    "\n",
    "def combine_isos(pjm_df, nyiso_df, isone_df):\n",
    "    \"\"\"Combine cleaned ISO dataframes into one.\"\"\"\n",
    "    pjm_clean = clean_pjm(pjm_df)\n",
    "    nyiso_clean = clean_nyiso(nyiso_df)\n",
    "    isone_clean = clean_isone(isone_df)\n",
    "    \n",
    "    combined_df = pd.concat([pjm_clean, nyiso_clean, isone_clean], ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "combined_ne_df = combine_isos(pjm_2023_df, nyo_2023_df, isone_2023_df)\n",
    "\n",
    "combined_ne_df = combined_ne_df.sort_values('timestamp_utc').reset_index(drop=True)\n",
    "\n",
    "# ðŸ§  Now set timestamp_utc as index\n",
    "combined_ne_df = combined_ne_df.set_index('timestamp_utc')\n",
    "combined_ne_df\n",
    "\n",
    "combined_ne_df_2023 = combined_ne_df[['iso', 'Location Name', 'Location Type', 'LMP', 'MCC', 'MLC']]\n",
    "combined_ne_df_2023['node'] = 'node'\n",
    "combined_ne_df_2023\n",
    "\n",
    "output_path = 'iso_data/yearly_combined_data/combined_ne_2023_lmp_data.csv'\n",
    "combined_ne_df_2023.to_csv(output_path)\n",
    "\n",
    "print(f\"âœ… Successfully saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cf2762-dee2-4504-a123-8289c2e58f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in interconnection queue data\n",
    "queues = pd.read_excel('iso_data/queues_2024.xlsx', sheet_name=1)\n",
    "\n",
    "def standardize_dates(series):\n",
    "    return pd.to_datetime(series, \n",
    "                          infer_datetime_format=True,\n",
    "                          errors='coerce')\n",
    "\n",
    "queues['q_date'] = standardize_dates(queues['q_date'])\n",
    "queues['ia_date'] = standardize_dates(queues['ia_date'])\n",
    "queues['wd_date'] = standardize_dates(queues['wd_date'])\n",
    "queues['on_date'] = standardize_dates(queues['on_date'])\n",
    "\n",
    "cutoff = pd.Timestamp('2022-01-01')\n",
    "filtered_queues = queues.loc[(queues['q_date'] >= cutoff) & \n",
    "                             ((queues['ia_date'] >= cutoff)|(queues['ia_date'].isnull())) & \n",
    "                             ((queues['wd_date'] >= cutoff)|(queues['wd_date'].isnull())) &\n",
    "                             ((queues['on_date'] >= cutoff)|(queues['on_date'].isnull()))]\n",
    "\n",
    "# q_date: date when project entered queue\n",
    "# ia_date: date of signed interconnection agreement\n",
    "# wd_date: date project withdrawn from queue\n",
    "# on_date: date project became operational\n",
    "\n",
    "selected_cols = ['q_id', 'region', 'q_date', \n",
    "                 'ia_date', 'wd_date', 'on_date']\n",
    "\n",
    "selected_regions = ['PJM', 'ERCOT', 'CAISO', 'SPP', 'NYISO', 'ISO-NE']\n",
    "\n",
    "filtered_queues = filtered_queues[filtered_queues['region'].isin(selected_regions)].reset_index(drop=True)\n",
    "\n",
    "def queue_duration(df, start, end):\n",
    "    difference = df[end] - df[start]\n",
    "    return difference.dt.days\n",
    "\n",
    "filtered_queues['days_to_ia'] = queue_duration(filtered_queues, 'q_date', 'ia_date')\n",
    "filtered_queues['days_to_wd'] = queue_duration(filtered_queues, 'q_date', 'wd_date')\n",
    "filtered_queues['days_to_on'] = queue_duration(filtered_queues, 'q_date', 'on_date')\n",
    "\n",
    "as_of = pd.to_datetime('2023-12-31')\n",
    "pending = filtered_queues[['ia_date','wd_date','on_date']].isna().all(axis=1)\n",
    "\n",
    "filtered_queues['days_pending'] = np.where(\n",
    "    pending,\n",
    "    (as_of - filtered_queues['q_date']).dt.days,\n",
    "    np.nan)\n",
    "\n",
    "selected_cols = selected_cols + ['days_to_ia', 'days_to_wd', 'days_to_on', 'days_pending']\n",
    "\n",
    "# 1) build a â€œlongâ€ events frame: one row per event occurrence\n",
    "events = []\n",
    "for event, date_col, days_col in [\n",
    "    ('ia', 'ia_date', 'days_to_ia'),\n",
    "    ('wd', 'wd_date', 'days_to_wd'),\n",
    "    ('on', 'on_date', 'days_to_on'),\n",
    "]:\n",
    "    tmp = (\n",
    "        filtered_queues\n",
    "        .loc[filtered_queues[date_col].notna(), ['region', date_col, days_col]]\n",
    "        .rename(columns={date_col: 'date', days_col: 'days_to'})\n",
    "    )\n",
    "    tmp['event'] = event\n",
    "    events.append(tmp)\n",
    "events = pd.concat(events, ignore_index=True)\n",
    "\n",
    "events.sort_values(by=['region','date'])\n",
    "\n",
    "# 1) pivot out daily counts and daily average days_to\n",
    "daily_counts = (\n",
    "    events\n",
    "      .groupby(['region','date','event'])\n",
    "      .size()\n",
    "      .unstack('event', fill_value=0)\n",
    "      .rename(columns={'ia':'count_ia','wd':'count_wd','on':'count_on'}))\n",
    "\n",
    "daily_days = (\n",
    "    events\n",
    "      .groupby(['region','date','event'])['days_to']\n",
    "      .mean()\n",
    "      .unstack('event')\n",
    "      .rename(columns={'ia':'days_to_ia','wd':'days_to_wd','on':'days_to_on'}))\n",
    "\n",
    "daily = (\n",
    "    daily_counts\n",
    "      .join(daily_days, how='outer')\n",
    "      .sort_index())\n",
    "\n",
    "# 2) reindex to every calendar date so cumsum/expanding works\n",
    "all_dates = pd.date_range(\n",
    "    events['date'].min().floor('D'),\n",
    "    events['date'].max().ceil('D'),\n",
    "    freq='D'\n",
    ")\n",
    "idx = pd.MultiIndex.from_product(\n",
    "    [daily.index.levels[0], all_dates],\n",
    "    names=['region','date']\n",
    ")\n",
    "daily = daily.reindex(idx, fill_value=0).sort_index()\n",
    "\n",
    "# 3) cumulative sums of counts\n",
    "daily['cum_ia'] = daily.groupby(level='region')['count_ia'].cumsum()\n",
    "daily['cum_wd'] = daily.groupby(level='region')['count_wd'].cumsum()\n",
    "daily['cum_on'] = daily.groupby(level='region')['count_on'].cumsum()\n",
    "\n",
    "# 4) cumulative sums & counts of days_to, then expanding mean\n",
    "for ev in ['ia','wd','on']:\n",
    "    # running sum of days_to\n",
    "    daily[f'sum_days_to_{ev}'] = (\n",
    "        daily[f'days_to_{ev}']\n",
    "          .groupby(level='region')\n",
    "          .cumsum()\n",
    "    )\n",
    "    # running count of events (same as cum_count)\n",
    "    daily[f'cnt_days_to_{ev}'] = daily[f'count_{ev}'].groupby(level='region').cumsum()\n",
    "    # expanding average = sum / count\n",
    "    daily[f'avg_days_to_{ev}'] = (\n",
    "        daily[f'sum_days_to_{ev}'] / daily[f'cnt_days_to_{ev}'])\n",
    "\n",
    "# 5) clean up: drop intermediate columns\n",
    "final = daily.reset_index().drop(\n",
    "    columns=[f'days_to_{ev}'      for ev in ['ia','wd','on']]\n",
    "           +[f'sum_days_to_{ev}'  for ev in ['ia','wd','on']]\n",
    "           +[f'cnt_days_to_{ev}'  for ev in ['ia','wd','on']])\n",
    "\n",
    "# 0) your cutoff\n",
    "as_of = pd.to_datetime('2023-12-31')\n",
    "\n",
    "# 1) build the calendar of snapshot dates **only** through the cutoff\n",
    "all_dates = pd.date_range(\n",
    "    start=filtered_queues['q_date'].min().floor('D'),\n",
    "    end=as_of,\n",
    "    freq='D')\n",
    "\n",
    "# 2) pull in each projectâ€™s dates\n",
    "proj_raw = filtered_queues[['region','q_date','ia_date','wd_date','on_date']].copy()\n",
    "\n",
    "# 3) Cartesian-merge so each project is paired with each snapshot date\n",
    "dates_df = pd.DataFrame({'date': all_dates})\n",
    "projects = (\n",
    "    proj_raw\n",
    "      .assign(key=1)\n",
    "      .merge(dates_df.assign(key=1), on='key')\n",
    "      .drop('key', axis=1))\n",
    "\n",
    "# 4) replace NaT (i.e. â€œnever happened by cutoffâ€) with far-future\n",
    "for col in ['ia_date','wd_date','on_date']:\n",
    "    projects[col] = projects[col].fillna(pd.Timestamp.max)\n",
    "\n",
    "# 5) filter to only those still pending **at** each snapshot date\n",
    "mask = (\n",
    "    (projects['date'] >= projects['q_date']) &\n",
    "    (projects['date'] <  projects['ia_date']) &\n",
    "    (projects['date'] <  projects['wd_date']) &\n",
    "    (projects['date'] <  projects['on_date']))\n",
    "pending = projects.loc[mask, ['region','date','q_date']]\n",
    "\n",
    "# 6) compute days_pending and then the daily average by region\n",
    "pending['days_pending'] = (pending['date'] - pending['q_date']).dt.days\n",
    "\n",
    "avg_pending = (\n",
    "    pending\n",
    "      .groupby(['region','date'])['days_pending']\n",
    "      .mean()\n",
    "      .reset_index(name='avg_days_pending'))\n",
    "\n",
    "# d) merge with rolled\n",
    "agg = (\n",
    "    final\n",
    "    .merge(avg_pending, on=['region','date'], how='right')\n",
    "    .sort_values(['region','date']))\n",
    "\n",
    "# final now has, for each ISO and each calendar date:\n",
    "#   - count_ia, count_wd, count_on (30-day sums)\n",
    "#   - avg_days_to_ia, avg_days_to_wd, avg_days_to_on (30-day means)\n",
    "#   - avg_days_pending (daily average for all still-pending in that snapshot)\n",
    "\n",
    "lmp = pd.read_csv('iso_data/yearly_combined_data/combined_ne_2023_lmp_data.csv')\n",
    "\n",
    "# 1) Make sure your queueâ€metrics DataFrame has a proper datetime â€œdateâ€ column\n",
    "agg['date'] = pd.to_datetime(agg['date']).dt.normalize()\n",
    "agg['date'] = pd.to_datetime(agg['date']).dt.tz_localize('UTC')\n",
    "\n",
    "agg = agg.rename(columns={'region':'iso'})\n",
    "agg = agg.drop(['count_ia', 'count_on', 'count_wd'], axis=1)\n",
    "\n",
    "# 2) Prepare your LMP DataFrame\n",
    "#    â€“ parse timestamp_utc â†’ datetime\n",
    "#    â€“ extract the date (drop the time component)\n",
    "lmp['date'] = pd.to_datetime(lmp['timestamp_utc']).dt.normalize()\n",
    "\n",
    "# 4) Merge the two tables on region & date\n",
    "merged = (\n",
    "    lmp\n",
    "      .merge(agg, on=['iso','date'], how='left')\n",
    ").drop(['date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d2b6f-3777-45ea-9ef0-e33f7526f310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in eia wholesale data\n",
    "\n",
    "data_2022 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2022final.xlsx\")\n",
    "data_2023 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2023final.xlsx\")\n",
    "data_2024 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2024final.xlsx\")\n",
    "\n",
    "eia_pricing_data = pd.concat([data_2022, data_2023, data_2024], ignore_index=True, axis=0)\n",
    "\n",
    "hub_to_iso = {\n",
    "    'Indiana Hub RT Peak': 'MISO',\n",
    "    'Mid C Peak': 'Non-ISO (Mid-Columbia)',\n",
    "    'NP15 EZ Gen DA LMP Peak': 'CAISO',\n",
    "    'Nepool MH DA LMP Peak': 'ISO-NE',\n",
    "    'PJM WH Real Time Peak': 'PJM',\n",
    "    'Palo Verde Peak': 'CAISO',\n",
    "    'SP15 EZ Gen DA LMP Peak': 'CAISO'\n",
    "}\n",
    "\n",
    "eia_pricing_data['ISO'] = eia_pricing_data['Price hub'].map(hub_to_iso)\n",
    "\n",
    "# Convert dates to datetime\n",
    "trade_dates = pd.to_datetime(eia_pricing_data['Trade date'], format='mixed', errors='coerce')\n",
    "delivery_start_dates = pd.to_datetime(eia_pricing_data['Delivery start date'], format='mixed', errors='coerce')\n",
    "delivery_end_dates = pd.to_datetime(eia_pricing_data['Delivery \\nend date'], format='mixed', errors='coerce')\n",
    "\n",
    "# Check the unique years\n",
    "print(\"Trade date years:\", sorted(trade_dates.dropna().dt.year.unique()))\n",
    "print(\"Delivery start date years:\", sorted(delivery_start_dates.dropna().dt.year.unique()))\n",
    "print(\"Delivery end date years:\", sorted(delivery_end_dates.dropna().dt.year.unique()))\n",
    "\n",
    "eia_daily = eia_pricing_data.copy()\n",
    "\n",
    "# Convert 'Trade date' column to datetime objects \n",
    "eia_daily['Trade date'] = pd.to_datetime(eia_daily['Trade date'], format='mixed').dt.date\n",
    "\n",
    "# Create a new column for weighted price = price Ã— volume\n",
    "eia_daily['weighted_price'] = eia_daily['Wtd avg price $/MWh'] * eia_daily['Daily volume MWh']\n",
    "\n",
    "# Group data by Trade date and ISO, and aggregate:\n",
    "eia_daily_summary = (\n",
    "    eia_daily\n",
    "    .groupby(['Trade date', 'ISO'])\n",
    "    .agg(\n",
    "        weighted_avg_price=('weighted_price', 'sum'),           # sum of (P Ã— V)\n",
    "        total_volume=('Daily volume MWh', 'sum'),                # sum of volume\n",
    "        total_trades=('Number of trades', 'sum'),                # sum of trades\n",
    "        total_counterparties=('Number of counterparties', 'sum') # sum of counterparties\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate the volume-weighted average price\n",
    "eia_daily_summary['Wtd avg price $/MWh'] = eia_daily_summary['weighted_avg_price'] / eia_daily_summary['total_volume']\n",
    "\n",
    "# Select and reorder the final columns\n",
    "eia_daily_summary = eia_daily_summary[\n",
    "    ['Trade date', 'ISO', 'Wtd avg price $/MWh', 'total_volume', 'total_trades', 'total_counterparties']\n",
    "]\n",
    "\n",
    "# Filter to include only Trade dates from 2022, 2023, or 2024\n",
    "eia_daily_summary = eia_daily_summary[\n",
    "    eia_daily_summary['Trade date'].apply(lambda x: x.year).isin([2022, 2023, 2024])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "merged['date'] = pd.to_datetime(temp_df['timestamp_utc']).dt.date\n",
    "\n",
    "merged_df = merged.merge(\n",
    "    eia_daily_summary,\n",
    "    how='left',\n",
    "    left_on=['date', 'iso'],\n",
    "    right_on=['Trade date', 'ISO']\n",
    ")\n",
    "\n",
    "merged_df = merged_df.drop(columns=['Trade date', 'ISO', 'date', 'Unnamed: 0'])\n",
    "\n",
    "merged_df.to_csv('iso_data/yearly_combined_data/final_ne_2023_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf95848-9998-4622-819e-166dd6bebea3",
   "metadata": {},
   "source": [
    "#### MISO, ERCOT, SPP + Respective Interconnection Queues and Wholesale Electricity Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31289ffd-3b0e-47bc-87f3-57c04c7f5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_mw_south_raw_df = pd.concat([\n",
    "    miso_2023_df,\n",
    "    ercot_2023_df,\n",
    "    spp_2023_df\n",
    "], ignore_index=True)\n",
    "\n",
    "print(combined_mw_south_raw_df.head())\n",
    "\n",
    "print(f\"âœ… Combined raw DataFrame shape: {combined_mw_south_raw_df.shape}\")\n",
    "\n",
    "def clean_spp(df_spp):\n",
    "    df = df_spp.rename(columns={\n",
    "        'GMTIntervalEnd': 'timestamp_utc',\n",
    "        'Pnode': 'Location Name',\n",
    "    })\n",
    "    df['timestamp_utc'] = pd.to_datetime(df['timestamp_utc'])\n",
    "    df['timestamp_utc'] = df['timestamp_utc'].dt.tz_localize('UTC')\n",
    "    df['Location Type'] = 'Node'\n",
    "    df['iso'] = 'SPP'\n",
    "    return df[['timestamp_utc', 'Location Name', 'Location Type', 'iso', 'LMP', 'MCC', 'MLC']]\n",
    "\n",
    "def clean_ercot(df_ercot):\n",
    "    df = df_ercot.copy()\n",
    "\n",
    "    # Parse deliverydate to datetime\n",
    "    df['deliverydate'] = pd.to_datetime(df['deliverydate'])\n",
    "\n",
    "    # Fix 24:00 by shifting the date and setting hour to 0\n",
    "    mask_24 = df['hourending'] == '24:00'\n",
    "    df.loc[mask_24, 'deliverydate'] += pd.Timedelta(days=1)\n",
    "    df.loc[mask_24, 'hourending'] = '00:00'\n",
    "\n",
    "    # Now safely extract the hour as an integer\n",
    "    df['hour'] = df['hourending'].str.extract('(\\d+)').astype(int)\n",
    "\n",
    "    \n",
    "    # Step 1: Combine deliverydate and hourending as strings\n",
    "    datetime_str = df['deliverydate'].astype(str) + ' ' + df['hourending'].astype(str)\n",
    "\n",
    "    # Step 2: Parse the combined string into a real datetime\n",
    "    df['timestamp_utc'] = pd.to_datetime(datetime_str, format='%Y-%m-%d %H:%M', errors='coerce')\n",
    "\n",
    "    # Step 3: Localize to UTC\n",
    "    df['timestamp_utc'] = df['timestamp_utc'].dt.tz_localize('UTC')\n",
    "\n",
    "\n",
    "    # Standardize columns\n",
    "    df['Location Name'] = df['busname']\n",
    "    df['Location Type'] = 'Node'\n",
    "    df['iso'] = 'ERCOT'\n",
    "    df['MCC'] = None\n",
    "    df['MLC'] = None\n",
    "    df = df.rename(columns={'lmp': 'LMP'})\n",
    "    \n",
    "    return df[['timestamp_utc', 'Location Name', 'Location Type', 'iso', 'LMP', 'MCC', 'MLC']]\n",
    "\n",
    "\n",
    "def reshape_miso(df_miso):\n",
    "    # Identify the hour columns (HE1, HE2, ..., HE24)\n",
    "    hour_columns = [col for col in df_miso.columns if str(col).startswith('HE')]\n",
    "\n",
    "    # Melt from wide to long\n",
    "    df_long = df_miso.melt(\n",
    "        id_vars=['MARKET_DAY', 'NODE', 'TYPE', 'VALUE'],\n",
    "        value_vars=hour_columns,\n",
    "        var_name='Hour Ending',\n",
    "        value_name='Price'\n",
    "    )\n",
    "\n",
    "    # âœ… Safe handling of 'Hour Ending'\n",
    "    df_long['Hour Ending'] = df_long['Hour Ending'].astype(str)  # Ensure it's string\n",
    "    df_long['Hour Ending'] = df_long['Hour Ending'].str.replace('HE', '', regex=False)\n",
    "    df_long = df_long[df_long['Hour Ending'].str.isnumeric()]\n",
    "    df_long['Hour Ending'] = df_long['Hour Ending'].astype(int)\n",
    "\n",
    "    # Create local timestamp (interval beginning, so subtract 1 hour)\n",
    "    df_long['Date'] = pd.to_datetime(df_long['MARKET_DAY'], format='%m/%d/%Y', errors='coerce')\n",
    "    df_long['timestamp_local'] = df_long['Date'] + pd.to_timedelta(df_long['Hour Ending'] - 1, unit='h')\n",
    "\n",
    "    # âœ… Handle DST properly\n",
    "    df_long['timestamp_utc'] = (\n",
    "        df_long['timestamp_local']\n",
    "        .dt.tz_localize('US/Central', ambiguous='NaT', nonexistent='shift_forward')\n",
    "        .dt.tz_convert('UTC')\n",
    "    )\n",
    "\n",
    "    # Rename columns to match final schema\n",
    "    df_long = df_long.rename(columns={\n",
    "        'NODE': 'Location Name',\n",
    "        'TYPE': 'Location Type',\n",
    "        'VALUE': 'Component'\n",
    "    })\n",
    "\n",
    "    df_long['iso'] = 'MISO'\n",
    "\n",
    "    return df_long[['timestamp_utc', 'Location Name', 'Location Type', 'Component', 'Price', 'iso']]\n",
    "\n",
    "def pivot_components(df_long):\n",
    "    \"\"\"\n",
    "    Pivots 'Component' rows (LMP, MCC, MLC) into separate columns,\n",
    "    keeping timestamp, location info, and ISO.\n",
    "    \"\"\"\n",
    "    df_wide = df_long.pivot_table(\n",
    "        index=['timestamp_utc', 'Location Name', 'Location Type', 'iso'],\n",
    "        columns='Component',\n",
    "        values='Price',\n",
    "        aggfunc='first'   # âœ… Critical: Avoid aggregation crash on objects\n",
    "    ).reset_index()\n",
    "\n",
    "    df_wide.columns.name = None  # Remove pivot artifacts\n",
    "    return df_wide\n",
    "\n",
    "\n",
    "\n",
    "def combine_all(df_spp, df_ercot, df_miso):\n",
    "    spp_clean = clean_spp(df_spp)\n",
    "    ercot_clean = clean_ercot(df_ercot)\n",
    "    df_miso_long = reshape_miso(df_miso)\n",
    "    miso_clean = pivot_components(df_miso_long)\n",
    "    combined = pd.concat([spp_clean, ercot_clean, miso_clean], ignore_index=True)\n",
    "    return combined\n",
    "\n",
    "combined_df = combine_all(spp_2023_df, ercot_2023_df, miso_2023_df)\n",
    "\n",
    "combined_df = combined_df.sort_values('timestamp_utc').reset_index(drop=True)\n",
    "\n",
    "# ðŸ§  Now set timestamp_utc as index\n",
    "combined_df = combined_df.set_index('timestamp_utc')\n",
    "\n",
    "combined_mw_south_df_2023 = combined_df[['iso', 'Location Name', 'Location Type', 'LMP', 'MCC', 'MLC']]\n",
    "# add column to call everything a node (but keep location type as what ISO calls it\n",
    "combined_mw_south_df_2023['node'] = 'node'\n",
    "combined_mw_south_df_2023\n",
    "\n",
    "output_path = 'iso_data/yearly_combined_data/combined_mw_south_2023_lmp_data.csv'\n",
    "combined_mw_south_df_2023.to_csv(output_path)\n",
    "\n",
    "print(f\"âœ… Successfully saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a12af35-1289-48be-b8f1-b4c1bf4642fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in interconnection queue data\n",
    "queues = pd.read_excel('iso_data/queues_2024.xlsx', sheet_name=1)\n",
    "\n",
    "def standardize_dates(series):\n",
    "    return pd.to_datetime(series, \n",
    "                          infer_datetime_format=True,\n",
    "                          errors='coerce')\n",
    "\n",
    "queues['q_date'] = standardize_dates(queues['q_date'])\n",
    "queues['ia_date'] = standardize_dates(queues['ia_date'])\n",
    "queues['wd_date'] = standardize_dates(queues['wd_date'])\n",
    "queues['on_date'] = standardize_dates(queues['on_date'])\n",
    "\n",
    "cutoff = pd.Timestamp('2022-01-01')\n",
    "filtered_queues = queues.loc[(queues['q_date'] >= cutoff) & \n",
    "                             ((queues['ia_date'] >= cutoff)|(queues['ia_date'].isnull())) & \n",
    "                             ((queues['wd_date'] >= cutoff)|(queues['wd_date'].isnull())) &\n",
    "                             ((queues['on_date'] >= cutoff)|(queues['on_date'].isnull()))]\n",
    "\n",
    "# q_date: date when project entered queue\n",
    "# ia_date: date of signed interconnection agreement\n",
    "# wd_date: date project withdrawn from queue\n",
    "# on_date: date project became operational\n",
    "\n",
    "selected_cols = ['q_id', 'region', 'q_date', \n",
    "                 'ia_date', 'wd_date', 'on_date']\n",
    "\n",
    "selected_regions = ['PJM', 'ERCOT', 'CAISO', 'SPP', 'NYISO', 'ISO-NE']\n",
    "\n",
    "filtered_queues = filtered_queues[filtered_queues['region'].isin(selected_regions)].reset_index(drop=True)\n",
    "\n",
    "def queue_duration(df, start, end):\n",
    "    difference = df[end] - df[start]\n",
    "    return difference.dt.days\n",
    "\n",
    "filtered_queues['days_to_ia'] = queue_duration(filtered_queues, 'q_date', 'ia_date')\n",
    "filtered_queues['days_to_wd'] = queue_duration(filtered_queues, 'q_date', 'wd_date')\n",
    "filtered_queues['days_to_on'] = queue_duration(filtered_queues, 'q_date', 'on_date')\n",
    "\n",
    "as_of = pd.to_datetime('2023-12-31')\n",
    "pending = filtered_queues[['ia_date','wd_date','on_date']].isna().all(axis=1)\n",
    "\n",
    "filtered_queues['days_pending'] = np.where(\n",
    "    pending,\n",
    "    (as_of - filtered_queues['q_date']).dt.days,\n",
    "    np.nan)\n",
    "\n",
    "selected_cols = selected_cols + ['days_to_ia', 'days_to_wd', 'days_to_on', 'days_pending']\n",
    "\n",
    "# 1) build a â€œlongâ€ events frame: one row per event occurrence\n",
    "events = []\n",
    "for event, date_col, days_col in [\n",
    "    ('ia', 'ia_date', 'days_to_ia'),\n",
    "    ('wd', 'wd_date', 'days_to_wd'),\n",
    "    ('on', 'on_date', 'days_to_on'),\n",
    "]:\n",
    "    tmp = (\n",
    "        filtered_queues\n",
    "        .loc[filtered_queues[date_col].notna(), ['region', date_col, days_col]]\n",
    "        .rename(columns={date_col: 'date', days_col: 'days_to'})\n",
    "    )\n",
    "    tmp['event'] = event\n",
    "    events.append(tmp)\n",
    "events = pd.concat(events, ignore_index=True)\n",
    "\n",
    "events.sort_values(by=['region','date'])\n",
    "\n",
    "# 1) pivot out daily counts and daily average days_to\n",
    "daily_counts = (\n",
    "    events\n",
    "      .groupby(['region','date','event'])\n",
    "      .size()\n",
    "      .unstack('event', fill_value=0)\n",
    "      .rename(columns={'ia':'count_ia','wd':'count_wd','on':'count_on'}))\n",
    "\n",
    "daily_days = (\n",
    "    events\n",
    "      .groupby(['region','date','event'])['days_to']\n",
    "      .mean()\n",
    "      .unstack('event')\n",
    "      .rename(columns={'ia':'days_to_ia','wd':'days_to_wd','on':'days_to_on'}))\n",
    "\n",
    "daily = (\n",
    "    daily_counts\n",
    "      .join(daily_days, how='outer')\n",
    "      .sort_index())\n",
    "\n",
    "# 2) reindex to every calendar date so cumsum/expanding works\n",
    "all_dates = pd.date_range(\n",
    "    events['date'].min().floor('D'),\n",
    "    events['date'].max().ceil('D'),\n",
    "    freq='D'\n",
    ")\n",
    "idx = pd.MultiIndex.from_product(\n",
    "    [daily.index.levels[0], all_dates],\n",
    "    names=['region','date']\n",
    ")\n",
    "daily = daily.reindex(idx, fill_value=0).sort_index()\n",
    "\n",
    "# 3) cumulative sums of counts\n",
    "daily['cum_ia'] = daily.groupby(level='region')['count_ia'].cumsum()\n",
    "daily['cum_wd'] = daily.groupby(level='region')['count_wd'].cumsum()\n",
    "daily['cum_on'] = daily.groupby(level='region')['count_on'].cumsum()\n",
    "\n",
    "# 4) cumulative sums & counts of days_to, then expanding mean\n",
    "for ev in ['ia','wd','on']:\n",
    "    # running sum of days_to\n",
    "    daily[f'sum_days_to_{ev}'] = (\n",
    "        daily[f'days_to_{ev}']\n",
    "          .groupby(level='region')\n",
    "          .cumsum()\n",
    "    )\n",
    "    # running count of events (same as cum_count)\n",
    "    daily[f'cnt_days_to_{ev}'] = daily[f'count_{ev}'].groupby(level='region').cumsum()\n",
    "    # expanding average = sum / count\n",
    "    daily[f'avg_days_to_{ev}'] = (\n",
    "        daily[f'sum_days_to_{ev}'] / daily[f'cnt_days_to_{ev}'])\n",
    "\n",
    "# 5) clean up: drop intermediate columns\n",
    "final = daily.reset_index().drop(\n",
    "    columns=[f'days_to_{ev}'      for ev in ['ia','wd','on']]\n",
    "           +[f'sum_days_to_{ev}'  for ev in ['ia','wd','on']]\n",
    "           +[f'cnt_days_to_{ev}'  for ev in ['ia','wd','on']])\n",
    "\n",
    "# 0) your cutoff\n",
    "as_of = pd.to_datetime('2023-12-31')\n",
    "\n",
    "# 1) build the calendar of snapshot dates **only** through the cutoff\n",
    "all_dates = pd.date_range(\n",
    "    start=filtered_queues['q_date'].min().floor('D'),\n",
    "    end=as_of,\n",
    "    freq='D')\n",
    "\n",
    "# 2) pull in each projectâ€™s dates\n",
    "proj_raw = filtered_queues[['region','q_date','ia_date','wd_date','on_date']].copy()\n",
    "\n",
    "# 3) Cartesian-merge so each project is paired with each snapshot date\n",
    "dates_df = pd.DataFrame({'date': all_dates})\n",
    "projects = (\n",
    "    proj_raw\n",
    "      .assign(key=1)\n",
    "      .merge(dates_df.assign(key=1), on='key')\n",
    "      .drop('key', axis=1))\n",
    "\n",
    "# 4) replace NaT (i.e. â€œnever happened by cutoffâ€) with far-future\n",
    "for col in ['ia_date','wd_date','on_date']:\n",
    "    projects[col] = projects[col].fillna(pd.Timestamp.max)\n",
    "\n",
    "# 5) filter to only those still pending **at** each snapshot date\n",
    "mask = (\n",
    "    (projects['date'] >= projects['q_date']) &\n",
    "    (projects['date'] <  projects['ia_date']) &\n",
    "    (projects['date'] <  projects['wd_date']) &\n",
    "    (projects['date'] <  projects['on_date']))\n",
    "pending = projects.loc[mask, ['region','date','q_date']]\n",
    "\n",
    "# 6) compute days_pending and then the daily average by region\n",
    "pending['days_pending'] = (pending['date'] - pending['q_date']).dt.days\n",
    "\n",
    "avg_pending = (\n",
    "    pending\n",
    "      .groupby(['region','date'])['days_pending']\n",
    "      .mean()\n",
    "      .reset_index(name='avg_days_pending'))\n",
    "\n",
    "# d) merge with rolled\n",
    "agg = (\n",
    "    final\n",
    "    .merge(avg_pending, on=['region','date'], how='right')\n",
    "    .sort_values(['region','date']))\n",
    "\n",
    "# final now has, for each ISO and each calendar date:\n",
    "#   - count_ia, count_wd, count_on (30-day sums)\n",
    "#   - avg_days_to_ia, avg_days_to_wd, avg_days_to_on (30-day means)\n",
    "#   - avg_days_pending (daily average for all still-pending in that snapshot)\n",
    "\n",
    "lmp = pd.read_csv('iso_data/yearly_combined_data/combined_mw_south_2023_lmp_data.csv')\n",
    "\n",
    "# 1) Make sure your queueâ€metrics DataFrame has a proper datetime â€œdateâ€ column\n",
    "agg['date'] = pd.to_datetime(agg['date']).dt.normalize()\n",
    "agg['date'] = pd.to_datetime(agg['date']).dt.tz_localize('UTC')\n",
    "\n",
    "agg = agg.rename(columns={'region':'iso'})\n",
    "agg = agg.drop(['count_ia', 'count_on', 'count_wd'], axis=1)\n",
    "\n",
    "# 2) Prepare your LMP DataFrame\n",
    "#    â€“ parse timestamp_utc â†’ datetime\n",
    "#    â€“ extract the date (drop the time component)\n",
    "lmp['date'] = pd.to_datetime(lmp['timestamp_utc']).dt.normalize()\n",
    "\n",
    "# 4) Merge the two tables on region & date\n",
    "merged = (\n",
    "    lmp\n",
    "      .merge(agg, on=['iso','date'], how='left')\n",
    ").drop(['date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8dcaa1-81c3-4925-8e8e-c9530660445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in eia wholesale data\n",
    "\n",
    "data_2022 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2022final.xlsx\")\n",
    "data_2023 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2023final.xlsx\")\n",
    "data_2024 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2024final.xlsx\")\n",
    "\n",
    "eia_pricing_data = pd.concat([data_2022, data_2023, data_2024], ignore_index=True, axis=0)\n",
    "\n",
    "hub_to_iso = {\n",
    "    'Indiana Hub RT Peak': 'MISO',\n",
    "    'Mid C Peak': 'Non-ISO (Mid-Columbia)',\n",
    "    'NP15 EZ Gen DA LMP Peak': 'CAISO',\n",
    "    'Nepool MH DA LMP Peak': 'ISO-NE',\n",
    "    'PJM WH Real Time Peak': 'PJM',\n",
    "    'Palo Verde Peak': 'CAISO',\n",
    "    'SP15 EZ Gen DA LMP Peak': 'CAISO'\n",
    "}\n",
    "\n",
    "eia_pricing_data['ISO'] = eia_pricing_data['Price hub'].map(hub_to_iso)\n",
    "\n",
    "# Convert dates to datetime\n",
    "trade_dates = pd.to_datetime(eia_pricing_data['Trade date'], format='mixed', errors='coerce')\n",
    "delivery_start_dates = pd.to_datetime(eia_pricing_data['Delivery start date'], format='mixed', errors='coerce')\n",
    "delivery_end_dates = pd.to_datetime(eia_pricing_data['Delivery \\nend date'], format='mixed', errors='coerce')\n",
    "\n",
    "# Check the unique years\n",
    "print(\"Trade date years:\", sorted(trade_dates.dropna().dt.year.unique()))\n",
    "print(\"Delivery start date years:\", sorted(delivery_start_dates.dropna().dt.year.unique()))\n",
    "print(\"Delivery end date years:\", sorted(delivery_end_dates.dropna().dt.year.unique()))\n",
    "\n",
    "eia_daily = eia_pricing_data.copy()\n",
    "\n",
    "# Convert 'Trade date' column to datetime objects \n",
    "eia_daily['Trade date'] = pd.to_datetime(eia_daily['Trade date'], format='mixed').dt.date\n",
    "\n",
    "# Create a new column for weighted price = price Ã— volume\n",
    "eia_daily['weighted_price'] = eia_daily['Wtd avg price $/MWh'] * eia_daily['Daily volume MWh']\n",
    "\n",
    "# Group data by Trade date and ISO, and aggregate:\n",
    "eia_daily_summary = (\n",
    "    eia_daily\n",
    "    .groupby(['Trade date', 'ISO'])\n",
    "    .agg(\n",
    "        weighted_avg_price=('weighted_price', 'sum'),           # sum of (P Ã— V)\n",
    "        total_volume=('Daily volume MWh', 'sum'),                # sum of volume\n",
    "        total_trades=('Number of trades', 'sum'),                # sum of trades\n",
    "        total_counterparties=('Number of counterparties', 'sum') # sum of counterparties\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate the volume-weighted average price\n",
    "eia_daily_summary['Wtd avg price $/MWh'] = eia_daily_summary['weighted_avg_price'] / eia_daily_summary['total_volume']\n",
    "\n",
    "# Select and reorder the final columns\n",
    "eia_daily_summary = eia_daily_summary[\n",
    "    ['Trade date', 'ISO', 'Wtd avg price $/MWh', 'total_volume', 'total_trades', 'total_counterparties']\n",
    "]\n",
    "\n",
    "# Filter to include only Trade dates from 2022, 2023, or 2024\n",
    "eia_daily_summary = eia_daily_summary[\n",
    "    eia_daily_summary['Trade date'].apply(lambda x: x.year).isin([2022, 2023, 2024])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "merged['date'] = pd.to_datetime(temp_df['timestamp_utc']).dt.date\n",
    "\n",
    "merged_df = merged.merge(\n",
    "    eia_daily_summary,\n",
    "    how='left',\n",
    "    left_on=['date', 'iso'],\n",
    "    right_on=['Trade date', 'ISO']\n",
    ")\n",
    "\n",
    "merged_df = merged_df.drop(columns=['Trade date', 'ISO', 'date', 'Unnamed: 0'])\n",
    "\n",
    "merged_df.to_csv('iso_data/yearly_combined_data/final_mf_south_2023_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc138b-7a9c-4989-a0f7-1788a8563660",
   "metadata": {},
   "source": [
    "#### CAISO Respective Interconnection Queues and Wholesale Electricity Trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514e11ec-5788-4192-b948-2d1784ed38d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "caiso = pd.read_csv('iso_data/caiso_data/caiso_dam_lmp_parallel.csv')\n",
    "caiso\n",
    "\n",
    "df_new = pd.DataFrame()\n",
    "\n",
    "df_new['timestamp_utc'] = pd.to_datetime(caiso['INTERVALENDTIME_GMT'], errors='coerce')\n",
    "df_new['iso'] = 'CAISO'\n",
    "df_new['Location Name'] = caiso['NODE']\n",
    "df_new['Location Type'] = 'Node'\n",
    "df_new['LMP'] = caiso['MW']\n",
    "df_new['MCC'] = pd.NA  \n",
    "df_new['MLC'] = pd.NA\n",
    "df_new['node'] = 'node'\n",
    "\n",
    "df_new = df_new.sort_values('timestamp_utc').reset_index(drop=True)\n",
    "\n",
    "df_new = df_new.set_index('timestamp_utc')\n",
    "df_new\n",
    "\n",
    "output_path = 'iso_data/yearly_combined_data/caiso_2023_lmp_data.csv'\n",
    "df_new.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9345718-53e2-44b0-a7a3-14be8ed8048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in interconnection queue data\n",
    "queues = pd.read_excel('iso_data/queues_2024.xlsx', sheet_name=1)\n",
    "\n",
    "def standardize_dates(series):\n",
    "    return pd.to_datetime(series, \n",
    "                          infer_datetime_format=True,\n",
    "                          errors='coerce')\n",
    "\n",
    "queues['q_date'] = standardize_dates(queues['q_date'])\n",
    "queues['ia_date'] = standardize_dates(queues['ia_date'])\n",
    "queues['wd_date'] = standardize_dates(queues['wd_date'])\n",
    "queues['on_date'] = standardize_dates(queues['on_date'])\n",
    "\n",
    "cutoff = pd.Timestamp('2022-01-01')\n",
    "filtered_queues = queues.loc[(queues['q_date'] >= cutoff) & \n",
    "                             ((queues['ia_date'] >= cutoff)|(queues['ia_date'].isnull())) & \n",
    "                             ((queues['wd_date'] >= cutoff)|(queues['wd_date'].isnull())) &\n",
    "                             ((queues['on_date'] >= cutoff)|(queues['on_date'].isnull()))]\n",
    "\n",
    "# q_date: date when project entered queue\n",
    "# ia_date: date of signed interconnection agreement\n",
    "# wd_date: date project withdrawn from queue\n",
    "# on_date: date project became operational\n",
    "\n",
    "selected_cols = ['q_id', 'region', 'q_date', \n",
    "                 'ia_date', 'wd_date', 'on_date']\n",
    "\n",
    "selected_regions = ['PJM', 'ERCOT', 'CAISO', 'SPP', 'NYISO', 'ISO-NE']\n",
    "\n",
    "filtered_queues = filtered_queues[filtered_queues['region'].isin(selected_regions)].reset_index(drop=True)\n",
    "\n",
    "def queue_duration(df, start, end):\n",
    "    difference = df[end] - df[start]\n",
    "    return difference.dt.days\n",
    "\n",
    "filtered_queues['days_to_ia'] = queue_duration(filtered_queues, 'q_date', 'ia_date')\n",
    "filtered_queues['days_to_wd'] = queue_duration(filtered_queues, 'q_date', 'wd_date')\n",
    "filtered_queues['days_to_on'] = queue_duration(filtered_queues, 'q_date', 'on_date')\n",
    "\n",
    "as_of = pd.to_datetime('2023-12-31')\n",
    "pending = filtered_queues[['ia_date','wd_date','on_date']].isna().all(axis=1)\n",
    "\n",
    "filtered_queues['days_pending'] = np.where(\n",
    "    pending,\n",
    "    (as_of - filtered_queues['q_date']).dt.days,\n",
    "    np.nan)\n",
    "\n",
    "selected_cols = selected_cols + ['days_to_ia', 'days_to_wd', 'days_to_on', 'days_pending']\n",
    "\n",
    "# 1) build a â€œlongâ€ events frame: one row per event occurrence\n",
    "events = []\n",
    "for event, date_col, days_col in [\n",
    "    ('ia', 'ia_date', 'days_to_ia'),\n",
    "    ('wd', 'wd_date', 'days_to_wd'),\n",
    "    ('on', 'on_date', 'days_to_on'),\n",
    "]:\n",
    "    tmp = (\n",
    "        filtered_queues\n",
    "        .loc[filtered_queues[date_col].notna(), ['region', date_col, days_col]]\n",
    "        .rename(columns={date_col: 'date', days_col: 'days_to'})\n",
    "    )\n",
    "    tmp['event'] = event\n",
    "    events.append(tmp)\n",
    "events = pd.concat(events, ignore_index=True)\n",
    "\n",
    "events.sort_values(by=['region','date'])\n",
    "\n",
    "# 1) pivot out daily counts and daily average days_to\n",
    "daily_counts = (\n",
    "    events\n",
    "      .groupby(['region','date','event'])\n",
    "      .size()\n",
    "      .unstack('event', fill_value=0)\n",
    "      .rename(columns={'ia':'count_ia','wd':'count_wd','on':'count_on'}))\n",
    "\n",
    "daily_days = (\n",
    "    events\n",
    "      .groupby(['region','date','event'])['days_to']\n",
    "      .mean()\n",
    "      .unstack('event')\n",
    "      .rename(columns={'ia':'days_to_ia','wd':'days_to_wd','on':'days_to_on'}))\n",
    "\n",
    "daily = (\n",
    "    daily_counts\n",
    "      .join(daily_days, how='outer')\n",
    "      .sort_index())\n",
    "\n",
    "# 2) reindex to every calendar date so cumsum/expanding works\n",
    "all_dates = pd.date_range(\n",
    "    events['date'].min().floor('D'),\n",
    "    events['date'].max().ceil('D'),\n",
    "    freq='D'\n",
    ")\n",
    "idx = pd.MultiIndex.from_product(\n",
    "    [daily.index.levels[0], all_dates],\n",
    "    names=['region','date']\n",
    ")\n",
    "daily = daily.reindex(idx, fill_value=0).sort_index()\n",
    "\n",
    "# 3) cumulative sums of counts\n",
    "daily['cum_ia'] = daily.groupby(level='region')['count_ia'].cumsum()\n",
    "daily['cum_wd'] = daily.groupby(level='region')['count_wd'].cumsum()\n",
    "daily['cum_on'] = daily.groupby(level='region')['count_on'].cumsum()\n",
    "\n",
    "# 4) cumulative sums & counts of days_to, then expanding mean\n",
    "for ev in ['ia','wd','on']:\n",
    "    # running sum of days_to\n",
    "    daily[f'sum_days_to_{ev}'] = (\n",
    "        daily[f'days_to_{ev}']\n",
    "          .groupby(level='region')\n",
    "          .cumsum()\n",
    "    )\n",
    "    # running count of events (same as cum_count)\n",
    "    daily[f'cnt_days_to_{ev}'] = daily[f'count_{ev}'].groupby(level='region').cumsum()\n",
    "    # expanding average = sum / count\n",
    "    daily[f'avg_days_to_{ev}'] = (\n",
    "        daily[f'sum_days_to_{ev}'] / daily[f'cnt_days_to_{ev}'])\n",
    "\n",
    "# 5) clean up: drop intermediate columns\n",
    "final = daily.reset_index().drop(\n",
    "    columns=[f'days_to_{ev}'      for ev in ['ia','wd','on']]\n",
    "           +[f'sum_days_to_{ev}'  for ev in ['ia','wd','on']]\n",
    "           +[f'cnt_days_to_{ev}'  for ev in ['ia','wd','on']])\n",
    "\n",
    "# 0) your cutoff\n",
    "as_of = pd.to_datetime('2023-12-31')\n",
    "\n",
    "# 1) build the calendar of snapshot dates **only** through the cutoff\n",
    "all_dates = pd.date_range(\n",
    "    start=filtered_queues['q_date'].min().floor('D'),\n",
    "    end=as_of,\n",
    "    freq='D')\n",
    "\n",
    "# 2) pull in each projectâ€™s dates\n",
    "proj_raw = filtered_queues[['region','q_date','ia_date','wd_date','on_date']].copy()\n",
    "\n",
    "# 3) Cartesian-merge so each project is paired with each snapshot date\n",
    "dates_df = pd.DataFrame({'date': all_dates})\n",
    "projects = (\n",
    "    proj_raw\n",
    "      .assign(key=1)\n",
    "      .merge(dates_df.assign(key=1), on='key')\n",
    "      .drop('key', axis=1))\n",
    "\n",
    "# 4) replace NaT (i.e. â€œnever happened by cutoffâ€) with far-future\n",
    "for col in ['ia_date','wd_date','on_date']:\n",
    "    projects[col] = projects[col].fillna(pd.Timestamp.max)\n",
    "\n",
    "# 5) filter to only those still pending **at** each snapshot date\n",
    "mask = (\n",
    "    (projects['date'] >= projects['q_date']) &\n",
    "    (projects['date'] <  projects['ia_date']) &\n",
    "    (projects['date'] <  projects['wd_date']) &\n",
    "    (projects['date'] <  projects['on_date']))\n",
    "pending = projects.loc[mask, ['region','date','q_date']]\n",
    "\n",
    "# 6) compute days_pending and then the daily average by region\n",
    "pending['days_pending'] = (pending['date'] - pending['q_date']).dt.days\n",
    "\n",
    "avg_pending = (\n",
    "    pending\n",
    "      .groupby(['region','date'])['days_pending']\n",
    "      .mean()\n",
    "      .reset_index(name='avg_days_pending'))\n",
    "\n",
    "# d) merge with rolled\n",
    "agg = (\n",
    "    final\n",
    "    .merge(avg_pending, on=['region','date'], how='right')\n",
    "    .sort_values(['region','date']))\n",
    "\n",
    "# final now has, for each ISO and each calendar date:\n",
    "#   - count_ia, count_wd, count_on (30-day sums)\n",
    "#   - avg_days_to_ia, avg_days_to_wd, avg_days_to_on (30-day means)\n",
    "#   - avg_days_pending (daily average for all still-pending in that snapshot)\n",
    "\n",
    "lmp = pd.read_csv('iso_data/yearly_combined_data/caiso_2023_lmp_data.csv')\n",
    "\n",
    "# 1) Make sure your queueâ€metrics DataFrame has a proper datetime â€œdateâ€ column\n",
    "agg['date'] = pd.to_datetime(agg['date']).dt.normalize()\n",
    "agg['date'] = pd.to_datetime(agg['date']).dt.tz_localize('UTC')\n",
    "\n",
    "agg = agg.rename(columns={'region':'iso'})\n",
    "agg = agg.drop(['count_ia', 'count_on', 'count_wd'], axis=1)\n",
    "\n",
    "# 2) Prepare your LMP DataFrame\n",
    "#    â€“ parse timestamp_utc â†’ datetime\n",
    "#    â€“ extract the date (drop the time component)\n",
    "lmp['date'] = pd.to_datetime(lmp['timestamp_utc']).dt.normalize()\n",
    "\n",
    "# 4) Merge the two tables on region & date\n",
    "merged = (\n",
    "    lmp\n",
    "      .merge(agg, on=['iso','date'], how='left')\n",
    ").drop(['date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bea8aa-af35-486f-83a0-707c35f9d6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add in eia wholesale data\n",
    "\n",
    "data_2022 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2022final.xlsx\")\n",
    "data_2023 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2023final.xlsx\")\n",
    "data_2024 = pd.read_excel(\"iso_data/Wholesale_Pricing_Data/ice_electric-2024final.xlsx\")\n",
    "\n",
    "eia_pricing_data = pd.concat([data_2022, data_2023, data_2024], ignore_index=True, axis=0)\n",
    "\n",
    "hub_to_iso = {\n",
    "    'Indiana Hub RT Peak': 'MISO',\n",
    "    'Mid C Peak': 'Non-ISO (Mid-Columbia)',\n",
    "    'NP15 EZ Gen DA LMP Peak': 'CAISO',\n",
    "    'Nepool MH DA LMP Peak': 'ISO-NE',\n",
    "    'PJM WH Real Time Peak': 'PJM',\n",
    "    'Palo Verde Peak': 'CAISO',\n",
    "    'SP15 EZ Gen DA LMP Peak': 'CAISO'\n",
    "}\n",
    "\n",
    "eia_pricing_data['ISO'] = eia_pricing_data['Price hub'].map(hub_to_iso)\n",
    "\n",
    "# Convert dates to datetime\n",
    "trade_dates = pd.to_datetime(eia_pricing_data['Trade date'], format='mixed', errors='coerce')\n",
    "delivery_start_dates = pd.to_datetime(eia_pricing_data['Delivery start date'], format='mixed', errors='coerce')\n",
    "delivery_end_dates = pd.to_datetime(eia_pricing_data['Delivery \\nend date'], format='mixed', errors='coerce')\n",
    "\n",
    "# Check the unique years\n",
    "print(\"Trade date years:\", sorted(trade_dates.dropna().dt.year.unique()))\n",
    "print(\"Delivery start date years:\", sorted(delivery_start_dates.dropna().dt.year.unique()))\n",
    "print(\"Delivery end date years:\", sorted(delivery_end_dates.dropna().dt.year.unique()))\n",
    "\n",
    "eia_daily = eia_pricing_data.copy()\n",
    "\n",
    "# Convert 'Trade date' column to datetime objects \n",
    "eia_daily['Trade date'] = pd.to_datetime(eia_daily['Trade date'], format='mixed').dt.date\n",
    "\n",
    "# Create a new column for weighted price = price Ã— volume\n",
    "eia_daily['weighted_price'] = eia_daily['Wtd avg price $/MWh'] * eia_daily['Daily volume MWh']\n",
    "\n",
    "# Group data by Trade date and ISO, and aggregate:\n",
    "eia_daily_summary = (\n",
    "    eia_daily\n",
    "    .groupby(['Trade date', 'ISO'])\n",
    "    .agg(\n",
    "        weighted_avg_price=('weighted_price', 'sum'),           # sum of (P Ã— V)\n",
    "        total_volume=('Daily volume MWh', 'sum'),                # sum of volume\n",
    "        total_trades=('Number of trades', 'sum'),                # sum of trades\n",
    "        total_counterparties=('Number of counterparties', 'sum') # sum of counterparties\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Calculate the volume-weighted average price\n",
    "eia_daily_summary['Wtd avg price $/MWh'] = eia_daily_summary['weighted_avg_price'] / eia_daily_summary['total_volume']\n",
    "\n",
    "# Select and reorder the final columns\n",
    "eia_daily_summary = eia_daily_summary[\n",
    "    ['Trade date', 'ISO', 'Wtd avg price $/MWh', 'total_volume', 'total_trades', 'total_counterparties']\n",
    "]\n",
    "\n",
    "# Filter to include only Trade dates from 2022, 2023, or 2024\n",
    "eia_daily_summary = eia_daily_summary[\n",
    "    eia_daily_summary['Trade date'].apply(lambda x: x.year).isin([2022, 2023, 2024])\n",
    "].reset_index(drop=True)\n",
    "\n",
    "merged['date'] = pd.to_datetime(temp_df['timestamp_utc']).dt.date\n",
    "\n",
    "merged_df = merged.merge(\n",
    "    eia_daily_summary,\n",
    "    how='left',\n",
    "    left_on=['date', 'iso'],\n",
    "    right_on=['Trade date', 'ISO']\n",
    ")\n",
    "\n",
    "merged_df = merged_df.drop(columns=['Trade date', 'ISO', 'date', 'Unnamed: 0'])\n",
    "\n",
    "merged_df.to_csv('iso_data/yearly_combined_data/final_caiso_2023_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
